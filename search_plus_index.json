{"./":{"url":"./","title":"简介","keywords":"","body":"简介 本书主要介绍了运维相关的知识，包括了云计算、容器、linux知识的学习记录，已经在使用的过程中遇到的一些问题及解决方案，后续会逐步完善文档笔记，希望这些笔记在大家学习的过程能帮到大家。 © vishon all right reserved，powered by GitbookUpdated at 2021-08-01 11:16:59 "},"gitbook/install-gitbook.html":{"url":"gitbook/install-gitbook.html","title":"【gitbook】本地机器安装gitbook","keywords":"","body":"gitbook简介 gitbook网站是一个简单的个人在线书籍网站，在这里可以把自己的文档整理成书籍发布出来，便于阅读。 gitbook网站：https://legacy.gitbook.com/ 本文主要讲解在gitbook网站上发布一个书籍文档和使用gitbook提供的工具在本地开发一个书籍文档部署到自己的服务上 在此之前你需要会如下准备： 账号： github有账号，gitbook使用github账号注册 git：代码管理工具 Markdown：gitbook主要使用MD语法来编写书籍的 gitbook工具：如果你在本地开发需要安装此插件，下面有介绍 nodejs环境：gitbook插件需要的运行环境 一款Markdown编辑器：方便本地开发，推荐Typora或gitbook自己的编辑器gitbook editor 安装nodejs环境 本次操作都在windows系统上进行操作，nodejs的安装具体可以参考文档https://www.runoob.com/nodejs/nodejs-install-setup.html 安装gitbook npm install gitbook-cli -g 初始化gitbook 创建一个文件夹gitbook，然后初始化gitbook，最后运行gitbook # mkdir gitbook # cd gitbook # gitbook init # gitbook serve 运行后会在启动一个服务，可以在浏览器输入localhost:4000，这样就可以访问gitbook了 © vishon all right reserved，powered by GitbookUpdated at 2021-05-19 20:00:58 "},"gitbook/gitbook-add-note.html":{"url":"gitbook/gitbook-add-note.html","title":"【gitbook】gitbook添加笔记","keywords":"","body":"我们讲下如何在gitbook中添加笔记，gitbook初始化之后默认会创建SUMMARY.md和README.md这2个文件 README.md里面的内容是你这本书的简介部分 # 简介 本书主要介绍了一些k8s的相关操作,运维知识的学习和讲解，test SUMMARY.md文件主要是用来添加文章目录 # Summary - [简介](README.md) ### 如何搭建属于自己的gitbook ### linux运维笔记 - [腾讯云cvm上搭建openvpn](linux/2021-03-25-Build-openvpn-intranet-access-vpc-on-cvm.md) ### docker运维笔记 - [1](docker/1.md) ### kubernetes运维笔记 - [强制删除Terminating状态ns](k8s/强制删除Terminating状态ns.md) ### TKE运维笔记 - [1](tke/1.md) - [2](tke/2.md) 如果你需要添加笔记，可以先创建文件夹，存放你编写的markdown笔记，然后再SUMMARY.md配置上目录即可。 © vishon all right reserved，powered by GitbookUpdated at 2021-05-19 20:07:36 "},"gitbook/gitbook-upload-github.html":{"url":"gitbook/gitbook-upload-github.html","title":"【gitbook】gitbook上传到github托管","keywords":"","body":"github创建代码分支 这里我们创建了一个mybook的代码仓库用来存放gitbook 编译gitbook为静态文件 # cd gitbook # mkdir content 然后将所有的md文件拷贝到content目录下，然后我们运行gitbook gitbook serve ./content ./gh-pages 这样会自动创建 gh-pages 文件夹，文件夹中的内容，就是编译后的输出。 编写自动化脚本部署gitbook 自动化创建和更新 gh-pages 所以，我们采用一个 npm 包，来帮助我们完成上面的操作 cd gitbook/ npm i gh-pages 然后创建 gitbook/scripts/deploy-gh-pages.js 里面的内容是： 'use strict'; var ghpages = require('gh-pages'); main(); function main() { ghpages.publish('./gh-pages', console.error.bind(console)); } 上面的脚本的作用，就是把当前文件夹下的 gh-pages 文件夹中的所有内容，push 到本仓库的 gh-pages 分支。 然后在根目录下package.json文件添加几个npm脚本 deploy （ deploy 就是部署的意思），还有 build （意思是编译），还有 publish（意思是发布），如下： { \"name\": \"gitbook\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"start\": \"gitbook serve ./content ./gh-pages\", \"build\": \"gitbook build ./content ./gh-pages\", \"deploy\": \"node ./scripts/deploy-gh-pages.js\", \"publish\": \"npm run build&&npm run deploy\" }, \"repository\": { \"type\": \"git\", \"url\": \"git+://github.com/nieweixing/mybook.git\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\", \"bugs\": { \"url\": \"https://github.com/nieweixing/mybook/issues\" }, \"homepage\": \"https://github.com/nieweixing/mybook#readme\", \"dependencies\": { \"cheerio\": \"^1.0.0-rc.9\", \"gh-pages\": \"^3.1.0\", \"gitbook-plugin-code\": \"^0.1.0\", \"gitbook-plugin-mygitalk\": \"^0.2.6\", \"gitbook-plugin-prism\": \"^2.4.0\", \"gitbook-plugin-prism-themes\": \"0.0.2\", \"prismjs\": \"^1.23.0\" } } 这样，以后我修改了书稿，只需运行 npm run publish 如果命令返回 undefined 字样，表示没有出现错误，部署成功。 就可以把最新的书稿 push 到远端仓库的 gh-pages 分支了。 配置github page 进入仓库，选择settings选项，找到page选项，配置下GitHub Pages，这里我之前配置了我自己的域名到github，所以我这边显示的自定义域名，而不是github.io这个 通过域名访问gitbook 浏览器输入https://www.niewx.cn/mybook，就可以查看gitbook了 © vishon all right reserved，powered by GitbookUpdated at 2021-08-14 10:40:55 "},"gitbook/gitbook-plugin-usage.html":{"url":"gitbook/gitbook-plugin-usage.html","title":"【gitbook】gitbook插件使用","keywords":"","body":"配置gitbook插件 gitbook默认只有一些插件，其他插件需要自己安装，安装插件很简单，在content目录下配置一个book.json即可 { \"title\": \"运维操作指南\", \"description\": \"运维操作指南\", \"author\": \"vishon\", \"gitbook\": \">= 3.2.2\", \"language\": \"zh-hans\", \"links\": { \"sidebar\": { \"聂伟星个人博客\": \"https://www.niewx.cn\", \"TKE运维手册\": \"https://cloud.tencent.com/developer/column/87421\" } }, \"plugins\": [ \"github\", \"editlink\", \"-lunr\", \"-search\", \"search-plus\", \"tbfed-pagefooter\", \"splitter\", \"page-toc-button\", \"back-to-top-button\", \"-lunr\", \"-search\", \"search-plus\", \"github-buttons@2.1.0\", \"favicon@^0.0.2\", \"3-ba\", \"disqus\", \"theme-default\", \"pageview-count\", \"auto-scroll-table\", \"popup\", \"code\", \"-highlight\", \"prism\", \"prism-themes\" ], \"pdf\": { \"toc\": true, \"pageNumbers\": true, \"fontSize\": 11 }, \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/nieweixing\" }, \"editlink\": { \"base\": \"https://github.com/nieweixing/mybook/tree/gh-pages\", \"label\": \"编辑本页\" }, \"tbfed-pagefooter\": { \"copyright\":\"&copy vishon\", \"modify_label\": \"Updated at\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"image-captions\": { \"caption\": \"图片 - _CAPTION_\" }, \"github-buttons\": { \"repo\": \"nieweixing/mybook\", \"types\": [\"star\"], \"size\": \"small\" }, \"favicon\": { \"shortcut\": \"favicon.ico\", \"bookmark\": \"favicon.ico\" }, \"disqus\": { \"shortName\": \"nieweixing-github-io\" }, \"3-ba\": { \"token\": \"c03bec8e82bc53c064e0e648ffa54d88\" }, \"code\": { \"copyButtons\": true }, \"prism\": { \"css\": [ \"prismjs/themes/prism-okaidia.css\" ] } }, \"generator\": \"site\" } 配置好之后，需要执行npm install安装下插件，安装完成后会将插件放在node_modules目录，这里如果下载插件很慢，可以直接到我的github目录下拷贝对应的包https://github.com/nieweixing/mybook/tree/gh-pages/gitbook gitbook install ./content 配置discuss评论系统 这里可以给gitbook配置disscus评论系统 注册 disqus账号 https://disqus.com/ 右上角 Setting --> Add Disqus To Site --> 最下面 GET STARTED --> I want to install ... 填写网站 name 和分类 --> Create Site 填写 Website URL如 https://www.niewx.cn/mybook 配置好之后再book.json的disqus字段配置你创建的shortname，最后在gitbook目录下执行发布命令发布到github，这样我们就可以使用discuss评论功能了 gitbook插件列表 mygitalk 基于gitalk的评论插件 ➡️ https://github.com/snowdreams1006/gitbook-plugin-mygitalk theme-default GitBook的默认主题 ➡️ https://github.com/GitbookIO/theme-default autotheme 自动换肤插件 ➡️ https://github.com/willin/gitbook-plugin-autotheme sharing 默认的分享插件 ➡️ https://github.com/GitbookIO/plugin-sharing fontsettings 默认的字体、字号、颜色设置插件 ➡️ https://github.com/GitbookIO/plugin-fontsettings highlight 默认的代码高亮插件，通常会使用 prism 来替换 ➡️ https://github.com/GitbookIO/plugin-highlight search 默认搜索插件 ➡️ https://github.com/GitbookIO/plugin-search search-plus 支持中文搜索插件 ➡️ https://github.com/lwdgit/gitbook-plugin-search-plus prism 基于 Prism 的代码高亮 ➡️ https://github.com/gaearon/gitbook-plugin-prism favicon 更改网站的 favicon.ico ➡️ https://github.com/menduo/gitbook-plugin-favicon github 在右上角显示 github 仓库的图标链接 ➡️ https://github.com/GitbookIO/plugin-github github-buttons 显示 github 仓库的star和fork按钮 ➡️ https://github.com/azu/gitbook-plugin-github-buttons splitter 在左侧目录和右侧内容之间添加一个可以拖拽的栏，用来调整两边的宽度 ➡️ https://github.com/yoshidax/gitbook-plugin-splitter copy-code-button 为代码块添加复制的按钮 ➡️ https://github.com/WebEngage/gitbook-plugin-copy-code-button tbfed-pagefooter 自定义页脚，显示版权和最后修订时间 ➡️ https://github.com/zhj3618/gitbook-plugin-tbfed-pagefooter expandable-chapters 收起或展开章节目录中的父节点 ➡️ https://github.com/DomainDrivenArchitecture/gitbook-plugin-expandable-chapters expandable-chapters-small 比较好的折叠侧边栏 ➡️ https://github.com/lookdczar/gitbook-plugin-expandable-chapters-small-auto book-summary-scroll-position-saver 自动保存左侧目录区域导航条的位置 ➡️ https://github.com/yoshidax/gitbook-plugin-book-summary-scroll-position-saver ga 添加 Google 统计代码 ➡️ https://github.com/GitbookIO/plugin-ga sitemap 生成站点地图 ➡️ https://github.com/GitbookIO/plugin-sitemap baidu 使用百度统计 ➡️ https://github.com/poppinlp/gitbook-plugin-baidu Donate Gitbook 捐赠打赏插件 ➡️ https://github.com/willin/gitbook-plugin-donate anchors 标题带有 github 样式的锚点 ➡️ https://github.com/rlmv/gitbook-plugin-anchors anchor-navigation-ex 插件锚导航-EX ➡️ https://github.com/zq99299/gitbook-plugin-anchor-navigation-ex theme-api 编写 API 文档 ➡️ https://github.com/GitbookIO/theme-api katex 使用KaTex进行数学排版 ➡️ https://github.com/GitbookIO/plugin-katex editlink 内容顶部显示编辑本页链接 ➡️ https://github.com/zhaoda/gitbook-plugin-editlink ad 在每个页面顶部和底部添加广告或任何自定义内容 ➡️ https://github.com/zhaoda/gitbook-plugin-ad image-captions 抓取内容中图片的alt或title属性，在图片下面显示标题 ➡️ https://github.com/todvora/gitbook-plugin-image-captions chart 使用 C3.js 图表 ➡️ https://github.com/csbun/gitbook-plugin-chart styles-sass 使用 SASS 替换 CSS ➡️ https://github.com/GitbookIO/plugin-styles-sass styles-less 使用 LESS 替换 CSS ➡️ https://github.com/GitbookIO/plugin-styles-less disqus 添加 disqus 评论插件 ➡️ https://github.com/GitbookIO/plugin-disqus latex-codecogs 使用数学方程式 ➡️ https://github.com/GitbookIO/plugin-latex-codecogs mermaid 使用流程图 ➡️ https://github.com/JozoVilcek/gitbook-plugin-mermaid atoc 插入 TOC 目录 ➡️ https://github.com/willin/gitbook-plugin-atoc ace 插入代码高亮编辑器 ➡️ https://github.com/ymcatar/gitbook-plugin-ace sectionx 分离各个段落，并提供一个展开收起的按钮 ➡️ https://github.com/ymcatar/gitbook-plugin-sectionx mcqx 交互式多选插件 ➡️ https://github.com/ymcatar/gitbook-plugin-mcqx include-codeblock 通过引用文件插入代码 ➡️ https://github.com/azu/gitbook-plugin-include-codeblock fbqx 使用填空题 ➡️ https://github.com/Erwin-Chan/gitbook-plugin-fbqx spoiler 隐藏答案，当鼠标划过时才显示 ➡️ https://github.com/ymcatar/gitbook-plugin-spoiler anchor-navigation 锚点导航 ➡️ https://github.com/yaneryou/gitbook-plugin-anchor-navigation youtubex 插入 YouTube 视频 ➡️ https://github.com/ymcatar/gitbook-plugin-youtubex redirect 重定向页面跳转 ➡️ https://github.com/ketan/gitbook-plugin-redirect duoshuo 使用多说评论 ➡️ https://github.com/codepiano/gitbook-plugin-duoshuo jsfiddle 插入 JSFiddle 组件 ➡️ https://github.com/Mavrin/gitbook-plugin-jsfiddle jsbin 插入 JSBin 组件 ➡️ https://github.com/jcouyang/gitbook-plugin-jsbin Advanced Emoji 支持emoji表情 ➡️ https://github.com/codeclou/gitbook-plugin-advanced-emoji Puml 使用 PlantUML 展示 uml 图 ➡️ https://github.com/GitbookIO/plugin-puml Graph 使用 function-plot 绘制数学函数图 ➡️ https://github.com/cjam/gitbook-plugin-graph Todo 添加 Todo 功能 ➡️ https://github.com/ly-tools/gitbook-plugin-todo include-csv 展示 csv 文件内容 ➡️ https://github.com/TakuroFukamizu/gitbook-plugin-include-csv musicxml 支持 musicxml 格式的乐谱渲染 ➡️ https://github.com/ymcatar/gitbook-plugin-musicxml versions-select 添加版本选择的下拉菜单，针对文档有多个版本的情况 ➡️ https://github.com/prescottprue/gitbook-plugin-versions-select rss 添加 rss 订阅功能 ➡️ https://github.com/denysdovhan/gitbook-plugin-rss multipart 将书籍分成几个部分 ➡️ https://github.com/citizenmatt/gitbook-plugin-multipart url-embed 嵌入动态内容 ➡️ https://github.com/basilvetas/gitbook-plugin-url-embed © vishon all right reserved，powered by GitbookUpdated at 2021-05-23 14:26:04 "},"linux/shell-script-template.html":{"url":"linux/shell-script-template.html","title":"【linux】shell脚本模板","keywords":"","body":"作为一名运维，我们经常会需要编写脚本来完成一些自动化工作，这里提供一个shell脚本的模板 #!/bin/sh ################ Version Info ################## # Create Date: 2021-05-26 # Author: vishon # Mail: nwx_qdlg@163.com # Version: 1.0 # Attention: shell脚本模板 ################################################ # 加载环境变量 # 如果脚本放到crontab中执行，会缺少环境变量，所以需要添加以下3行 . /etc/profile . ~/.bash_profile . /etc/bashrc # 脚本所在目录即脚本名称 script_dir=$( cd \"$( dirname \"$0\" )\" && pwd ) script_name=$(basename ${0}) # 日志目录 log_dir=\"${script_dir}/log\" [ ! -d ${log_dir} ] && { mkdir -p ${log_dir} } errorMsg(){ echo \"USAGE:$0 arg1 arg2 arg3\" exit 2 } doCode() { echo $1 echo $2 echo $3 } main() { if [ $# -ne 3 ];then errorMsg fi doCode \"$1\" \"$2\" \"$3\" } # 需要把隐号加上，不然传入的参数就不能有空格 main \"$@\" © vishon all right reserved，powered by GitbookUpdated at 2021-05-26 12:51:13 "},"linux/cvm-build-openvpn.html":{"url":"linux/cvm-build-openvpn.html","title":"【linux】腾讯云cvm上搭建openvpn","keywords":"","body":"我们在使用共有云的时候，有时候会需要本地电脑访问到云上的vpc机器，但是云上vpc是网络隔离的，如果不加公网ip是无法直接本地访问vpc的，其实这里我们只需要在vpc内有一台机器可以访问公网，然后再这台集群上搭建openvpn，这样本地就可以通过openvpn去直接连接vpc内其他内网机器，不用每台机器都配置公网ip了，下面我们来说下如何在腾讯云的cvm上搭建openvpn。 网络规划 vpc网段：10.0.0.0/16 openvpn分配给客户端的网段：192.168.1.0/24 openvpn服务端ip：10.0.0.13(内网)，106.53.146.250(公网) 安装openvpn # yum install openvpn # wget https://github.com/OpenVPN/easy-rsa/archive/master.zip # unzip master.zip # mv easy-rsa-master/ easy-rsa # mkdir -p /etc/openvpn/ # cp -R easy-rsa/ /etc/openvpn/ # cd /etc/openvpn/ # mkdir client server # ls client easy-rsa server 配置vars文件 # cd /etc/openvpn/easy-rsa/easyrsa3 # cp vars.example vars # vim vars 根据实际修改对应的配置 ....... set_var EASYRSA_REQ_COUNTRY \"CN\" set_var EASYRSA_REQ_PROVINCE \"SZ\" set_var EASYRSA_REQ_CITY \"GD\" set_var EASYRSA_REQ_ORG \"test\" set_var EASYRSA_REQ_EMAIL \"nwx_qdlg@163.com\" set_var EASYRSA_REQ_OU \"test\" ....... 创建server端证书 初始化目录 [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf vars vars.example x509-types [root@VM-0-13-centos easyrsa3]# ./easyrsa init-pki Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/easy-rsa/easyrsa3/pki [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf pki vars vars.example x509-types 创建CA证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa build-ca Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Enter New CA Key Passphrase: #输入CA密码，记录下 Re-Enter New CA Key Passphrase: #确认密码 Generating RSA private key, 2048 bit long modulus ..................+++ ............................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:server # ca证书名称 CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt 创建服务端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-req server nopass Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ....................+++ ........................+++ writing new private key to '/etc/openvpn/easy-rsa/easyrsa3/pki/easy-rsa-32328.KOVmFR/tmp.kdL0Yx' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [server]:vpc-server #输入服务端名称 Keypair and certificate request completed. Your files are: req: /etc/openvpn/easy-rsa/easyrsa3/pki/reqs/server.req key: /etc/openvpn/easy-rsa/easyrsa3/pki/private/server.key 签约服务端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa sign server server Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a server certificate for 825 days: subject= commonName = vpc-server Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes #输入yes Using configuration from /etc/openvpn/easy-rsa/easyrsa3/pki/easy-rsa-345.HZwt53/tmp.7IIgHU Enter pass phrase for /etc/openvpn/easy-rsa/easyrsa3/pki/private/ca.key: #输入之前配置的CA密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'vpc-server' Certificate is to be certified until Jun 29 09:02:24 2023 GMT (825 days) Write out database with 1 new entries Data Base Updated Certificate created at: /etc/openvpn/easy-rsa/easyrsa3/pki/issued/server.crt 创建数据穿越密钥 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-dh Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating DH parameters, 2048 bit long safe prime, generator 2 This is going to take a long time ....................+..............................................................................................................................................................................................+..........................................................................................................................+..........................................+...................+............................... DH parameters of size 2048 created at /etc/openvpn/easy-rsa/easyrsa3/pki/dh.pem 创建client证书 初始化目录 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/client/ [root@VM-0-13-centos client]# cp -R /root/easy-rsa/easyrsa3/ . [root@VM-0-13-centos client]# ll drwxr-xr-x 3 root root 4096 Mar 26 17:07 easyrsa3 [root@VM-0-13-centos client]# cd easyrsa3/ [root@VM-0-13-centos easyrsa3]# ./easyrsa init-pki init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/client/easyrsa3/pki [root@VM-0-13-centos easyrsa3]# ls easyrsa openssl-easyrsa.cnf pki vars.example x509-types 创建客户端CA证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa build-ca Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Enter New CA Key Passphrase: #输入ca密码 Re-Enter New CA Key Passphrase: #确认CA密码 Generating RSA private key, 2048 bit long modulus .....................................+++ ...........................................+++ e is 65537 (0x10001) You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:client-ca #输入ca证书名称 CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/client/easyrsa3/pki/ca.crt 创建客户端证书 [root@VM-0-13-centos easyrsa3]# ./easyrsa gen-req client Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Generating a 2048 bit RSA private key ........................................................+++ .............................................+++ writing new private key to '/etc/openvpn/client/easyrsa3/pki/easy-rsa-1789.jZxBCq/tmp.1l4buX' Enter PEM pass phrase: #输入客户端CA密码，也是将来登录VPN客户密码！ Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [client]:niewx #起名字 Keypair and certificate request completed. Your files are: req: /etc/openvpn/client/easyrsa3/pki/reqs/client.req key: /etc/openvpn/client/easyrsa3/pki/private/client.key 导入客户端证书 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/easy-rsa/easyrsa3 [root@VM-0-13-centos easyrsa3]# ./easyrsa import-req /etc/openvpn/client/easyrsa3/pki/reqs/client.req client Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/easyrsa3/vars Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 The request has been successfully imported with a short name of: client You may now use this name to perform signing operations on this request. 签约客户端证书 [root@VM-0-13-centos easyrsa3]# cd /etc/openvpn/easy-rsa/easyrsa3 [root@VM-0-13-centos easyrsa3]# ./easyrsa sign client client Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a client certificate for 825 days: subject= commonName = niewx Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes # 输入yes Using configuration from /etc/openvpn/client/easyrsa3/pki/easy-rsa-2777.2aZHdK/tmp.9RSG1Q Enter pass phrase for /etc/openvpn/client/easyrsa3/pki/private/ca.key: #客户端ca密码 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'niewx' Certificate is to be certified until Jun 29 09:16:55 2023 GMT (825 days) Write out database with 1 new entries Data Base Updated Certificate created at: /etc/openvpn/easy-rsa/easyrsa3/pki/issued/client.crt openvpn服务端配置 拷贝服务端证书文件 [root@VM-0-13-centos pki]# cd /etc/openvpn/easy-rsa/easyrsa3/pki [root@VM-0-13-centos pki]# cp ca.crt /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp private/server.key /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp issued/server.crt /etc/openvpn/server/ [root@VM-0-13-centos pki]# cp dh.pem /etc/openvpn/server/ 拷贝客户端证书 [root@VM-0-13-centos pki]# cd /etc/openvpn/client/easyrsa3 [root@VM-0-13-centos pki]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/ca.crt /etc/openvpn/client [root@VM-0-13-centos private]# cp /etc/openvpn/client/easyrsa3/pki/private/client.key /etc/openvpn/client [root@VM-0-13-centos issued]# cp /etc/openvpn/easy-rsa/easyrsa3/pki/issued/client.crt /etc/openvpn/client [root@VM-0-13-centos issued]# cd /etc/openvpn/client/ [root@VM-0-13-centos client]# ls ca.crt client.crt client.key easyrsa3 [root@VM-0-13-centos client]# cd /etc/openvpn/server/ [root@VM-0-13-centos server]# ls ca.crt dh.pem server.crt server.key 配置server.conf [root@VM-0-13-centos openvpn]# cd /etc/openvpn [root@VM-0-13-centos openvpn]# vim server.conf local 0.0.0.0 port 55555 proto tcp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key # This file should be kept secret dh /etc/openvpn/server/dh.pem server 192.168.1.0 255.255.255.0 ifconfig-pool-persist ipp.txt keepalive 10 120 persist-key persist-tun status openvpn-status.log verb 3 comp-lzo push \"route 10.0.0.0 255.0.0.0\" client-to-client log /var/log/openvpn.log 配置转发参数和iptables规则 [root@VM-0-13-centos openvpn]# sed -i '/net.ipv4.ip_forward/ s/\\(.*= \\).*/\\11/' /etc/sysctl.conf [root@VM-0-13-centos client]# iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o eth0 -j MASQUERADE [root@VM-0-13-centos client]# iptables -nL -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 192.168.1.0/24 0.0.0.0/0 启动oepnven服务端 [root@VM-0-13-centos client]# openvpn /etc/openvpn/server.conf & [1] 5785 [root@VM-0-13-centos client]# ps -ef | grep openvpn root 5785 26254 0 17:37 pts/0 00:00:00 openvpn /etc/openvpn/server.conf 本地机器安装openvpn客户端 可以到https://openvpn.net/community-downloads/下载对应系统客户端安装包 拷贝客户端证书到本地目录 主要/etc/openvpn/client目录下拷贝ca.crt，client.crt，client.key，然后配置下文件client.ovpn，内容如下 client dev tun proto tcp remote 106.53.146.250 55555 resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client1.crt key client1.key comp-lzo verb 3 运行vpn连接服务端 连接成功后，我们直接内网访问下服务，发现可以直接内网ip访问到prometheus的UI界面，这里说明我们本地电脑成功连接了vpc 自动生成客户端的脚本 [root@VM-0-13-centos client]# cd /etc/openvpn/client [root@VM-0-13-centos client]# cat auto-generate-client.sh # ! /bin/bash set -e OVPN_USER_KEYS_DIR=/etc/openvpn/client/keys EASY_RSA_VERSION=easyrsa3 EASY_RSA_DIR=/etc/openvpn/easy-rsa PKI_DIR=$EASY_RSA_DIR/$EASY_RSA_VERSION/pki for user in \"$@\" do if [ -d \"$OVPN_USER_KEYS_DIR/$user\" ]; then rm -rf $OVPN_USER_KEYS_DIR/$user rm -rf $PKI_DIR/reqs/$user.req rm -rf $PKI_DIR/private/$user.key rm -rf $PKI_DIR/issued/$user.crt sed -i '/'\"$user\"'/d' $PKI_DIR/index.txt #通过index.txt文件查看到证书的情况，首字母为R的证书就是已经被吊销的证书。 exit 0 fi cd $EASY_RSA_DIR/$EASY_RSA_VERSION # 生成客户端 ssl 证书文件 ./easyrsa build-client-full $user nopass # 整理下生成的文件 mkdir -p $OVPN_USER_KEYS_DIR/$user cp $PKI_DIR/ca.crt $OVPN_USER_KEYS_DIR/$user/ # CA 根证书 cp $PKI_DIR/issued/$user.crt $OVPN_USER_KEYS_DIR/$user/ # 客户端证书 cp $PKI_DIR/private/$user.key $OVPN_USER_KEYS_DIR/$user/ # 客户端证书密钥 cp /etc/openvpn/client/sample.ovpn $OVPN_USER_KEYS_DIR/$user/$user.ovpn # 客户端配置文件 sed -i 's/admin/'\"$user\"'/g' $OVPN_USER_KEYS_DIR/$user/$user.ovpn cd $OVPN_USER_KEYS_DIR zip -r $user.zip $user done exit 0 脚本会自动生成客户端证书，执行方式如下，如果需要生成多个用户则在参数加上就行 # sh auto-generate-client.sh test1 test2 ..... 将对应的zip包拷贝给用户，然后再openvpn中指定对应的ovpn文件进行配置下连接即可 © vishon all right reserved，powered by GitbookUpdated at 2021-05-18 18:41:27 "},"linux/vmware-install-centos-set-static-ip-address.html":{"url":"linux/vmware-install-centos-set-static-ip-address.html","title":"【linux】vmware安装centos环境设置静态的ip地址","keywords":"","body":"vmware安装centos环境设置静态的ip地址 有的时候我们为了学习测试，会在自己的笔记本上搭建虚拟机，今天我们来讲下在windows上如何安装centos环境并且设置静态ip，如果是土豪，可以直接到云上购买机器，本篇文章可以忽略。 笔记本主机IP为设置自动获取，不管什么情况下，不受虚拟机影响，只要连接外网就可以正常上网； 只要笔记本主机可以正常访问外网，启动虚拟机中的CentOS 7系统就可以正常访问外网，无需再进行任何设置； 虚拟机设置为固定IP，不管主机在什么网络环境下，是断网环境，还是连接任何网段访问外网的环境下，虚拟机的IP都固定不变，而且使用终端连接，始终不变，正常连接； 虚拟机的固定IP可以按照自己想设置的IP地址网段随意设置，比如我就想设置固定IP为192.168.2.2。 以上4点，网上我没有找到一个帖子可以达到我要求的效果，经过我这段时间研究，经过各种尝试，期间出现各种问题，测试稳定后，总结如下分享给大家，希望对大家有所帮助，少走弯路。 采用方式为NAT模式+固定IP的模式。 配置环境说明：主机为Win10家庭版，虚拟机为VMware Workstation 12 Pro中文版，虚拟机中的Linux系统为CentOS 7 64位。 设置虚拟机的网络连接方式 按照如下图设置，英文版的对照设置即可 配置虚拟机的NAT模式具体地址参数 编辑--虚拟网络编辑器--更改设置 选择VMnet8--取消勾选使用本地DHCP--设置子网IP--网关IP设置（记住此处设置，后面要用到），如下图 说明：修改子网IP设置，实现自由设置固定IP，若你想设置固定IP为192.168.2.2-255，比如192.168.2.2，则子网IP为192.168.2.0；若你想设置固定IP为192.168.1.2-255，比如192.168.1.2，则子网IP为192.168.1.0； 网关IP可以参照如下格式修改：192.168.2.1 配置笔记本主机具体VMnet8本地地址参数 说明：第6步中的IP地址随意设置，但是要保证不能跟你要设置虚拟机的固定IP一样。 修改虚拟机中的CentOS 7系统为固定IP的配置文件 进入centos7命令行界面，修改如下内容 #cd /etc/sysconfig/network-scripts/ #vi ifcfg-eno16777736 说明： #将IPV6…..协议都注释； BOOTPROTO=static #开机协议，有dhcp及static； ONBOOT=yes #设置为开机启动； DNS1=114.114.114.114 #这个是国内的DNS地址，是固定的； IPADDR=192.168.2.2 #你想要设置的固定IP，理论上192.168.2.2-255之间都可以，请自行验证； NETMASK=255.255.255.0 #子网掩码，不需要修改； GATEWAY=192.168.2.1 #网关，这里是你在“2.配置虚拟机的NAT模式具体地址参数”中的（2）选择VMnet8--取消勾选使用本地DHCP--设置子网IP--网关IP设置。 重启网络服务 service network restart 检验配置是否成功 查看修改后的固定IP为192.168.2.2，配置正确 ifconfig 测试虚拟机中的CentOS 7系统是否能连外网，有数据返回，说明可以连接外网 ping www.baidu.com 测试本机是否能ping通虚拟机的固定IP 有数据返回，说明可以使用终端工具正常连接 鼠标放到开始菜单右键，选择命令提示符（管理员），打开命令操作界面： ping 192.168.2.2 远程终端连接 若连接失败是因为CentOS 7的防火墙端口没有打开，比如开启80，3306端口，最后一定要重启防火墙 #查看防火墙状态 systemctl status firewalld #开启80端口 firewall-cmd --zone=public --add-port=80/tcp --permanent #开启3306端口 firewall-cmd --zone=public --add-port=3306/tcp --permanent #重启防火墙： firewall-cmd --reload 连接成功 © vishon all right reserved，powered by GitbookUpdated at 2021-05-22 10:44:15 "},"linux/Use-awk-to-extract-field-information.html":{"url":"linux/Use-awk-to-extract-field-information.html","title":"【linux】使用awk处理字段信息","keywords":"","body":"日常工作中有的时候我们需要对我们的文本进行处理，比如获取某几列的内容，再列之间加入分割符，这里不得不用到awk这个工具了，awk是一种处理文本文件的语言，是一个强大的文本分析工具，下面我们来讲讲如何使用它 我们以下面这个region.txt文本来作为示例 [root@VM-0-3-centos ~]# cat region.txt 地域 名称 简写（不推荐） 是否全量 ap-beijing 华北地区(北京) bj 是 ap-chengdu 西南地区(成都) cd 是 ap-chongqing 西南地区(重庆) cq 是 ap-guangzhou 华南地区(广州) gz 是 获取文本的某几列 获取文本的地域和简写 [root@VM-0-3-centos ~]# cat region.txt | awk -F \" \" '{print $1,$3}' 地域 简写（不推荐） ap-beijing bj ap-chengdu cd ap-chongqing cq ap-guangzhou gz 输入文本指定分隔符 输入文本字段之间用|||分割 [root@VM-0-3-centos ~]# cat region.txt | awk -F \" \" -v OFS='|||' '{print $1,$3}' 地域|||简写（不推荐） ap-beijing|||bj ap-chengdu|||cd ap-chongqing|||cq ap-guangzhou|||gz 参考文档 awk还有很多其他用法，具体可以参考文档 https://www.runoob.com/linux/linux-comm-awk.html © vishon all right reserved，powered by GitbookUpdated at 2021-06-22 12:44:15 "},"database/mysql-devops-note.html":{"url":"database/mysql-devops-note.html","title":"【datebase】mysql运维笔记","keywords":"","body":"Mysql运维知识 这里总结下mysql数据库常用的运维操作 登录MySQL 登录MySQL的命令是mysql， mysql 的使用语法如下： mysql [-u username] [-h host] [-p[password]] [dbname] username 与 password 分别是 MySQL 的用户名与密码，mysql的初始管理帐号是root，没有密码，注意：这个root用户不是Linux的系统用户。MySQL默认用户是root，由于初始没有密码，第一次进时只需键入mysql即可。 [root@test1 local]# mysql Welcome to the MySQL monitor.　Commands end with ; or \\g. Your MySQL connection id is 1 to server version: 4.0.16-standard Type 'help;' or '\\h' for help. Type '\\c' to clear the buffer. mysql> 出现了“mysql>”提示符，恭喜你，安装成功！ 增加了密码后的登录格式如下： mysql -u root -p Enter password: (输入密码) 其中-u后跟的是用户名，-p要求输入密码，回车后在输入密码处输入密码。 注意：这个mysql文件在/usr/bin目录下，与后面讲的启动文件/etc/init.d/mysql不是一个文件。 MySQL的几个重要目录 MySQL安装完成后不象SQL Server默认安装在一个目录，它的数据库文件、配置文件和命令文件分别在不同的目录，了解这些目录非常重要，尤其对于Linux的初学者，因为Linux本身的目录结构就比较复杂，如果搞不清楚MySQL的安装目录那就无从谈起深入学习。 下面就介绍一下这几个目录。 数据库目录 /var/lib/mysql/ 配置文件 /usr/share/mysql（mysql.server命令及配置文件） 相关命令 /usr/bin(mysqladmin mysqldump等命令) 启动脚本 /etc/rc.d/init.d/（启动脚本文件mysql的目录） 修改登录密码 MySQL默认没有密码，安装完毕增加密码的重要性是不言而喻的。 命令 usr/bin/mysqladmin -u root password ‘new-password’ 格式：mysqladmin -u用户名 -p旧密码 password 新密码 例子 例1：给root加个密码123456。 键入以下命令 ： [root@test1 local]# /usr/bin/mysqladmin -u root password 123456 注：因为开始时root没有密码，所以-p旧密码一项就可以省略了。 测试是否修改成功 不用密码登录 [root@test1 local]# mysql ERROR 1045: Access denied for user: ‘root@localhost’ (Using password: NO) 显示错误，说明密码已经修改。 用修改后的密码登录 [root@test1 local]# mysql -u root -p Enter password: (输入修改后的密码123456) Welcome to the MySQL monitor.　Commands end with ; or \\g. Your MySQL connection id is 4 to server version: 4.0.16-standard Type 'help;' or '\\h' for help. Type '\\c' to clear the buffer. mysql> 成功！ 这是通过mysqladmin命令修改口令，也可通过修改库来更改口令。 启动与停止 启动 MySQL安装完成后启动文件mysql在/etc/init.d目录下，在需要启动时运行下面命令即可。 [root@test1 init.d]# /etc/init.d/mysql start 停止 /usr/bin/mysqladmin -u root -p shutdown 自动启动 1）察看mysql是否在自动启动列表中 [root@test1 local]#　/sbin/chkconfig –list 2）把MySQL添加到你系统的启动服务组里面去 [root@test1 local]#　/sbin/chkconfig　– add　mysql 3）把MySQL从启动服务组里面删除。 [root@test1 local]#　/sbin/chkconfig　– del　mysql 更改MySQL目录 MySQL默认的数据文件存储目录为/var/lib/mysql。假如要把目录移到/home/data下需要进行下面几步 1.home目录下建立data目录 cd /home mkdir data 2.把MySQL服务进程停掉 mysqladmin -u root -p shutdown 3.把/var/lib/mysql整个目录移到/home/data mv /var/lib/mysql　/home/data/ 这样就把MySQL的数据文件移动到了/home/data/mysql下 4.找到my.cnf配置文件 如果/etc/目录下没有my.cnf配置文件，请到/usr/share/mysql/下找到*.cnf文件，拷贝其中一个到/etc/并改名为my.cnf)中。命令如下： [root@test1 mysql]# cp /usr/share/mysql/my-medium.cnf　/etc/my.cnf 5.编辑MySQL的配置文件/etc/my.cnf 为保证MySQL能够正常工作，需要指明mysql.sock文件的产生位置。 修改socket=/var/lib/mysql/mysql.sock一行中等号右边的值为：/home/mysql/mysql.sock 。操作如下： vi　 my.cnf　　　 (用vi工具编辑my.cnf文件，找到下列数据修改之) # The MySQL server [mysqld] port　　　= 3306 #socket　 = /var/lib/mysql/mysql.sock（原内容，为了更稳妥用“#”注释此行） socket　 = /home/data/mysql/mysql.sock　　　（加上此行） 6.修改MySQL启动脚本/etc/rc.d/init.d/mysql 最后，需要修改MySQL启动脚本/etc/rc.d/init.d/mysql，把其中datadir=/var/lib/mysql一行中，等号右边的路径改成你现在的实际存放路径：home/data/mysql。 [root@test1 etc]# vi　/etc/rc.d/init.d/mysql #datadir=/var/lib/mysql　　　　（注释此行） datadir=/home/data/mysql　　 （加上此行） 7.重新启动MySQL服务 /etc/rc.d/init.d/mysql　start 或用reboot命令重启Linux，如果工作正常移动就成功了，否则对照前面的7步再检查一下。 MySQL的常用操作 注意：MySQL中每个命令后都要以分号；结尾。 显示数据库 mysql> show databases; +----------+ | Database | +----------+ | mysql　　| | test　　 | +----------+ rows in set (0.04 sec) Mysql刚安装完有两个数据库：mysql和test。mysql库非常重要，它里面有MySQL的系统信息，我们改密码和新增用户，实际上就是用这个库中的相关表进行操作。 显示数据库中的表 mysql> use mysql; （打开库，对每个库进行操作就要打开此库，类似于foxpro ） Database changed mysql> show tables; +-----------------+ | Tables_in_mysql | +-----------------+ | columns_priv　　| | db　　　　　　　| | func　　　　　　| | host　　　　　　| | tables_priv　　 | | user　　　　　　| +-----------------+ 6 rows in set (0.01 sec) 显示数据表的结构 describe 表名; 显示表中的记录 select * from 表名; 例如：显示mysql库中user表中的纪录。所有能对MySQL用户操作的用户都在此表中。 Select * from user; 建库 create database 库名; 例如：创建一个名字位aaa的库 mysql> create databases aaa; 建表 use 库名； create table 表名 (字段设定列表)； 例如：在刚创建的aaa库中建立表name,表中有id(序号，自动增长)，xm（姓名）,xb（性别）,csny（出身年月）四个字段 use aaa; mysql> create table name (id int(3) auto_increment not null primary key, xm char(8),xb char(2),csny date); 可以用describe命令察看刚建立的表结构。 mysql> describe name; +-------+---------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra　　　　　| +-------+---------+------+-----+---------+----------------+ | id　　| int(3)　|　　　| PRI | NULL　　| auto_increment | | xm　　| char(8) | YES　|　　 | NULL　　|　　　　　　　　| | xb　　| char(2) | YES　|　　 | NULL　　|　　　　　　　　| | csny　| date　　| YES　|　　 | NULL　　|　　　　　　　　| +-------+---------+------+-----+---------+----------------+ 增加记录 例如：增加几条相关纪录。 mysql> insert into name values(”,’张三’,’男’,’1971-10-01′); mysql> insert into name values(”,’白云’,’女’,’1972-05-20′); 可用select命令来验证结果。 mysql> select * from name; +—-+——+——+————+ | id | xm　 | xb　 | csny　　　 | +—-+——+——+————+ |　1 | 张三 | 男　 | 1971-10-01 | |　2 | 白云 | 女　 | 1972-05-20 | +—-+——+——+————+ 修改纪录 例如：将张三的出生年月改为1971-01-10 mysql> update name set csny=’1971-01-10′ where xm=’张三’; 删除纪录 例如：删除张三的纪录。 mysql> delete from name where xm=’张三’; 删库和删表 drop database 库名; drop table 表名； 增加MySQL用户 格式：grant select on 数据库.* to 用户名@登录主机 identified by “密码” 1、增加一个用户user_1密码为123，让他可以在任何主机上登录，并对所有数据库有查询、插入、修改、删除的权限。首先用以root用户连入MySQL，然后键入以下命令： mysql> grant select,insert,update,delete on *.* to user_1@”%” Identified by “123″; 例1增加的用户是十分危险的，如果知道了user_1的密码，那么他就可以在网上的任何一台电脑上登录你的MySQL数据库并对你的数据为所欲为了，解决办法见例2。 2、增加一个用户user_2密码为123,让此用户只可以在localhost上登录，并可以对数据库aaa进行查询、插入、修改、删除的操作（localhost指本地主机，即MySQL数据库所在的那台主机），这样用户即使用知道user_2的密码，他也无法从网上直接访问数据库，只能通过MYSQL主机来操作aaa库。 mysql>grant select,insert,update,delete on aaa.* to user_2@localhost identified by “123″; 用新增的用户如果登录不了MySQL，在登录时用如下命令： mysql -u user_1 -p　-h 192.168.113.50　（-h后跟的是要登录主机的ip地址） mysql的数据导入导出 mysql数据库执行导入导出命令在可执行的mysql目录下执行 从数据库导出数据库或表文件： mysqldump -u用戶名 -p密码 -d 数据库名 表名 > 脚本名; 导出整个数据库结构和数据 mysqldump -h localhost -uroot -p123456 database > e:\\dump.sql 导出单个数据表结构和数据 mysqldump -h localhost -uroot -p123456 database table > e:\\dump.sql 导出整个数据库结构（不包含数据） mysqldump -h localhost -uroot -p123456 -d database > e:\\dump.sql 导出单个数据表结构（不包含数据） mysqldump -h localhost -uroot -p123456 -d database table > e:\\dump.sql 导入数据库或表到数据库（数据库要先建好） 方法1：mysql -h localhost -uroot -p123456 -d database table mysql中用户的权限分配 用户管理 mysql>use mysql; 查看 mysql> select host,user,password from user ; 创建 mysql> create user zx_root IDENTIFIED by 'xxxxx'; //identified by 会将纯文本密码加密作为散列值存储 修改 mysql>rename user feng to newuser；//mysql 5之后可以使用，之前需要使用update 更新user表 删除 mysql>drop user newuser; //mysql5之前删除用户时必须先使用revoke 删除用户权限，然后删除用户，mysql5之后drop 命令可以删除用户的同时删除用户的相关权限 更改密码 mysql> set password for zx_root =password('xxxxxx'); mysql> update mysql.user set password=password('xxxx') where user='otheruser' 查看用户权限 mysql> show grants for zx_root; 赋予权限 mysql> grant select on dmc_db.* to zx_root; 回收权限 mysql> revoke select on dmc_db.* from zx_root; //如果权限不存在会报错 上面的命令也可使用多个权限同时赋予和回收，权限之间使用逗号分隔 mysql> grant select，update，delete ，insert on dmc_db.* to zx_root; 如果想立即看到结果使用 flush privileges ; 命令更新 设置权限时必须给出一下信息 要授予的权限 被授予访问权限的数据库或表 用户名 grant和revoke可以在几个层次上控制访问权限 整个服务器，使用 grant ALL 和revoke ALL 整个数据库，使用on database.* 特点表，使用on database.table 特定的列 特定的存储过程 user表中host列的值的意义 % 匹配所有主机 localhost localhost不会被解析成IP地址，直接通过UNIXsocket连接 127.0.0.1 会通过TCP/IP协议连接，并且只能在本机访问； ::1 ::1就是兼容支持ipv6的，表示同ipv4的127.0.0.1 grant 普通数据用户，查询、插入、更新、删除 数据库中所有表数据的权利。 grant select on testdb.* to common_user@’%’ grant insert on testdb.* to common_user@’%’ grant update on testdb.* to common_user@’%’ grant delete on testdb.* to common_user@’%’ 或者，用一条 MySQL 命令来替代： grant select, insert, update, delete on testdb.* to common_user@’%’ 9>.grant 数据库开发人员，创建表、索引、视图、存储过程、函数。。。等权限。 grant 创建、修改、删除 MySQL 数据表结构权限。 grant create on testdb.* to developer@’192.168.0.%’; grant alter on testdb.* to developer@’192.168.0.%’; grant drop on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 外键权限。 grant references on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 临时表权限。 grant create temporary tables on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 索引权限。 grant index on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 视图、查看视图源代码 权限。 grant create view on testdb.* to developer@’192.168.0.%’; grant show view on testdb.* to developer@’192.168.0.%’; grant 操作 MySQL 存储过程、函数 权限。 grant create routine on testdb.* to developer@’192.168.0.%’; -- now, can show procedure status grant alter routine on testdb.* to developer@’192.168.0.%’; -- now, you can drop a procedure grant execute on testdb.* to developer@’192.168.0.%’; 10>.grant 普通 DBA 管理某个 MySQL 数据库的权限。 grant all privileges on testdb to dba@’localhost’ 其中，关键字 “privileges” 可以省略。 11>.grant 高级 DBA 管理 MySQL 中所有数据库的权限。 grant all on *.* to dba@’localhost’ 12>.MySQL grant 权限，分别可以作用在多个层次上。 1. grant 作用在整个 MySQL 服务器上： grant select on *.* to dba@localhost; -- dba 可以查询 MySQL 中所有数据库中的表。 grant all on *.* to dba@localhost; -- dba 可以管理 MySQL 中的所有数据库 2. grant 作用在单个数据库上： grant select on testdb.* to dba@localhost; -- dba 可以查询 testdb 中的表。 3. grant 作用在单个数据表上： grant select, insert, update, delete on testdb.orders to dba@localhost; 4. grant 作用在表中的列上： grant select(id, se, rank) on testdb.apache_log to dba@localhost; 5. grant 作用在存储过程、函数上： grant execute on procedure testdb.pr_add to ’dba’@’localhost’ grant execute on function testdb.fn_add to ’dba’@’localhost’ 注意：修改完权限以后 一定要刷新服务，或者重启服务，刷新服务用：FLUSH PRIVILEGES。 mysql主从复制和读写分离的搭建 Mysql的主从复制的搭建，大家可以采用在linux，windows，docker，dockr-compose来搭建mysql 本次采用方式为docker-composer来搭建多个mysql服务端 目录结构如下 docker-compose.yml文件内容 version: '2' services: mysql-master: build: context: ./ dockerfile: master/Dockerfile environment: - \"MYSQL_ROOT_PASSWORD=root\" - \"MYSQL_DATABASE=replicas_db\" links: - mysql-slave ports: - \"33065:3306\" restart: always hostname: mysql-master mysql-slave: build: context: ./ dockerfile: slave/Dockerfile environment: - \"MYSQL_ROOT_PASSWORD=root\" - \"MYSQL_DATABASE=replicas_db\" ports: - \"33066:3306\" restart: always hostname: mysql-slave Master的dockerfile和my.cnf Dockerfile内容 FROM mysql:5.7 MAINTAINER harrison ADD ./master/my.cnf /etc/mysql/my.cnf my.cnf内容 [mysqld] ## 设置server_id，一般设置为IP，注意要唯一 server_id=100 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，可以随便取，最好有含义（关键就是这里了） log-bin=replicas-mysql-bin ## 为每个session分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 Slave的dockerfile和my.cnf Dockerfile内容 FROM mysql:5.7 MAINTAINER harrison ADD ./slave/my.cnf /etc/mysql/my.cnf my.cnf内容 [mysqld] ## 设置server_id，一般设置为IP，注意要唯一 server_id=101 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log-bin=replicas-mysql-slave1-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=replicas-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## 防止改变数据(除了特殊的线程) read_only=1 进入 docker 目录，运行 docker-compose 启动命令。 $ docker-compose up -d 检查从库的起始状态 $ show master status; 从数据库处于 未同步复制状态。 检查主库的状态 $ show master status; 记录 主数据库 binary-log的文件名称和数据同步起始位置。 File: replicas-mysql-bin.000003 Position: 154 从库配置主库信息 在 从数据库上运行主数据库的相关配置sql进行主从关联 CHANGE MASTER TO MASTER_HOST='mysql-master', MASTER_USER='root', MASTER_PASSWORD='root', MASTER_LOG_FILE='replicas-mysql-bin.000003', MASTER_LOG_POS=154; 重新启动 slave 服务 $ stop slave $ start slave 进一步检查从数据库的状态信息，两者已经进行数据同步关联。 master修改密码如何同步slave 所以，更新密码后，只需要： change master to master_user='replication user', master_password='new passwd'; mysql慢查询的原因 没有索引或者没有用到索引(这是查询慢最常见问题，是程序设计的缺陷) I/O吞吐量小，形成了瓶颈效应。 没有创建计算列导致查询不优化。 内存不足 网络速度慢 查询出的数据量过大(可以采用多次查询，其他的方法降低数据量) 锁或者死锁(这也是查询慢最常见的问题，是程序设计的缺陷) sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 返回了不必要的行和列 查询语句不好，没有优化 © vishon all right reserved，powered by GitbookUpdated at 2021-05-22 16:47:07 "},"database/mongodb-devops-note.html":{"url":"database/mongodb-devops-note.html","title":"【datebase】mongodb运维笔记","keywords":"","body":"Mongdb运维知识 mongodb集群的搭建 系统环境 Centos7.5、MongoDB4.0.2、关闭防火墙。 Ip 路由服务端口 配置服务端口 分片1端口 分片2端口 分片3端口 192.168.30.30 27017 27018 27001 27002 27003 192.168.30.31 27017 27018 27001 27002 27003 192.168.30.32 27017 27018 27001 27002 27003 三台机器的配置服务(27018)形成复制集，分片1、2、3也在各机器都部署一个实例，它们之间形成复制集，客户端直接连接3个路由服务与之交互，配置服务和分片服务对客户端是透明的。 服务器的安装及配置(3台服务器执行相同操作) 下载解压MongoDB 到MongoDB官网下载：https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.6.tgz 解压到/home/mongodb，设置环境变量: echo 'export PATH=$PATH:/home/mongodb/bin' >> /etc/profile 保存后执行： source /etc/profile 创建路由、配置、分片等的相关目录与文件 启动配置文件存放的文件夹：mkdir -p /home/mongodb/conf 配置服务数据存放目录：mkdir -p /home/mongodb/data/config 分片1服务数据存放目录：mkdir -p /home/mongodb/data/shard1 分片2服务数据存放目录：mkdir -p /home/mongodb/data/shard2 分片3服务数据存放目录：mkdir -p /home/mongodb/data/shard3 配置服务日志存放文件：touch /home/mongodb/log/config.log 路由服务日志存放文件：touch /home/mongodb/log/mongos.log 分片1服务日志存放文件：touch /home/mongodb/log/shard1.log 分片2服务日志存放文件：touch /home/mongodb/log/shard2.log 分片3服务日志存放文件：touch /home/mongodb/log/shard3.log mkdir -p /home/mongodb/conf mkdir -p /home/mongodb/data/config mkdir -p /home/mongodb/data/shard1 mkdir -p /home/mongodb/data/shard2 mkdir -p /home/mongodb/data/shard3 mkdir -p /home/mongodb/log touch /home/mongodb/log/config.log touch /home/mongodb/log/mongos.log touch /home/mongodb/log/shard1.log touch /home/mongodb/log/shard2.log touch /home/mongodb/log/shard3.log 配置服务器部署(3台服务器执行相同操作) 在/home/mongodb/conf目录创建config.conf [root@master conf]# cat config.conf dbpath=/home/mongodb/data/config logpath=/home/mongodb/log/config.log port=27018 logappend=true fork=true maxConns=5000 #复制集名称 replSet=configs #置参数为true configsvr=true #允许任意机器连接 bind_ip=0.0.0.0 配置复制集 scp config.conf root@192.168.30.31:/home/mongodb/conf scp config.conf root@192.168.30.32:/home/mongodb/conf 分别启动三台服务器的配置服务： mongod -f /home/mongodb/conf/config.conf 连接mongo,只需在任意一台机器执行即可： mongo --host 10.211.55.3 --port 27018 切换数据库： use admin 初始化复制集： rs.initiate({_id:\"configs\",members:[{_id:0,host:\"192.168.30.30:27018\"},{_id:1,host:\"192.168.30.31:27018\"}, {_id:2,host:\"192.168.30.32:27018\"}]}) 其中_id:\"configs\"的configs是上面config.conf配置文件里的复制集名称，把三台服务器的配置服务组成复制集。 查看状态： rs.status() 等几十秒左右，执行上面的命令查看状态，三台机器的配置服务就已形成复制集，其中1台为PRIMARY，其他2台为SECONDARY。 分片服务部署(3台服务器执行相同操作) 在/home/mongodb/conf目录创建shard1.conf、shard2.conf、shard3.conf，内容如下： [root@master conf]# cat shard1.conf dbpath=/home/mongodb/data/shard1 #其他2个分片对应修改为shard2、shard3文件夹 logpath=/home/mongodb/log/shard1.log #其他2个分片对应修改为shard2.log、shard3.log port=27001 #其他2个分片对应修改为27002、27003 logappend=true fork=true maxConns=5000 storageEngine=mmapv1 shardsvr=true replSet=shard1 #其他2个分片对应修改为shard2、shard3 bind_ip=0.0.0.0 端口分别是27001、27002、27003，分别对应shard1.conf、shard2.conf、shard3.conf。 还有数据存放目录、日志文件这几个地方都需要对应修改。 在3台机器的相同端口形成一个分片的复制集，由于3台机器都需要这3个文件，所以根据这9个配置文件分别启动分片服务： scp shard* root@192.168.30.30:/home/mongodb/conf scp shard* root@192.168.30.31:/home/mongodb/conf scp shard* root@192.168.30.32:/home/mongodb/conf mongod --smallfiles -f /home/mongodb/conf/shard1.conf mongod --smallfiles -f /home/mongodb/conf/shard2.conf mongod --smallfiles -f /home/mongodb/conf/shard3.conf2 将分片配置为复制集连接mongo，只需在任意一台机器执行即可： mongo --host 192.168.30.30 --port 27001 //这里以shard1为例，其他两个分片则再需对应连接到27002、27003的端口进行操作即可 切换数据库： use admin 初始化复制集： rs.initiate({_id:\"shard1\",members:[{_id:0,host:\"192.168.30.30:27001\"},{_id:1,host:\"192.168.30.31:27001\"},{_id:2,host:\"192.168.30.32:27001\"}]}) 以上是基于分片1来操作，同理，其他2个分片也要连到各自的端口来执行一遍上述的操作，让3个分片各自形成1主2从的复制集，注意端口及仲裁节点的问题即可，操作完成后3个分片都启动完成，并完成复制集模式。 mongo --host 192.168.30.30 --port 27002 use admin rs.initiate({_id:\"shard2\",members:[{_id:0,host:\"192.168.30.30:27002\"},{_id:1,host:\"192.168.30.31:27002\"},{_id:2,host:\"192.168.30.32:27002\"}]}) mongo --host 192.168.30.30 --port 27003 use admin rs.initiate({_id:\"shard3\",members:[{_id:0,host:\"192.168.30.30:27003\"},{_id:1,host:\"192.168.30.31:27003\"},{_id:2,host:\"192.168.30.32:27003\"}]}) 路由服务部署(3台服务器执行相同操作) 创建mongos.conf 在/home/mongodb/conf目录创建mongos.conf，内容如下： [root@master conf]# cat mongos.conf logpath=/home/mongodb/log/mongos.log logappend = true port = 27017 fork = true configdb = configs/192.168.30.30:27018,192.168.30.31:27018,192.168.30.32:27018 maxConns=20000 bind_ip=0.0.0.0 scp mongos.conf root@192.168.30.31:/home/mongodb/conf scp mongos.conf root@192.168.30.32:/home/mongodb/conf 启动mongos 分别在三台服务器启动： mongos -f /home/mongodb/conf/mongos.conf 启动分片功能 连接mongo： mongo --host 10.211.55.3 --port 27017 切换数据库： use admin 添加分片，只需在一台机器执行即可： sh.addShard(\"shard1/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") sh.addShard(\"shard2/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") sh.addShard(\"shard3/192.168.30:27001,192.168.30.31:27001,192.168.30.32:27001\") 查看集群状态： sh.status() 实现分片功能 设置分片chunk大小 use config db.setting.save({\"_id\":\"chunksize\",\"value\":1}) # 设置块大小为1M是方便实验，不然需要插入海量数据 模拟写入数据 use calon for(i=1;i创建索引对表进行分片 db.user.createIndex({\"id\":1}) # 以\"id\"作为索引 sh.shardCollection(calon.user\",{\"id\":1}) # 根据\"id\"对user表进行分片 sh.status() # 查看分片情况 到此，MongoDB分布式集群就搭建完毕。 docker中运行mongodb 镜像下载 执行 docker search mongo 命令和docker pull mongo 运行mongo镜像 docker run \\ --name mongodb_server \\ -p 27017:27017 \\ -v /mysoft/mongodb/configdb:/data/configdb/ \\ -v /mysoft/mongodb/db/:/data/db/ \\ -d mongo --auth 采用admin用户进入mongodb docker exec -it a7e5d4e4ca69 mongo admin 创建admin管理员账户 db.createUser({ user: 'admin', pwd: 'admin123456', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] }); 以 admin 用户身份进入mongo docker exec -it a7e5d4e4ca69 mongo admin 对 admin 用户 进行身份认证 db.auth(\"admin\",\"admin123456\"); 创建 用户、密码和数据库 db.createUser({ user: 'swen', pwd: 'swen123456', roles: [ { role: \"readWrite\", db: \"app\" } ] }); 以 admin 用户身份进入mongo docker exec -it a7e5d4e4ca69 mongo admin 对 swen 进行身份认证 db.auth(\"swen\",\"swen123456\"); 切换数据库 use app 添加数据 db.test.save({name:\"zhangsan\"}); 查看数据库 show dbs 数据库集合（类似于表）操作命令 show collections db.createCollection(\"mycol\", { capped : true, autoIndexId : true, size : 6142800, max : 10000 } ) db.mycol2.insert({\"name\" : \"菜鸟教程\"}) db.mycol2.drop() 数据库文档（类似于一行一行数据）操作命令 db.col.insert({title: 'MongoDB 教程', description: 'MongoDB 是一个 Nosql 数据库', by: '菜鸟教程', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 }) db.col.find() document=({title: 'MongoDB 教程', description: 'MongoDB 是一个 Nosql 数据库', by: '菜鸟教程', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100 }); db.col.insert(document) var document = db.collection.insertOne({\"a\": 3}) document { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"571a218011a82a1d94c02333\") } db.col.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}}) db.col.find().pretty() db.col.save({ \"_id\" : ObjectId(\"56064f89ade2f21f36b03136\"), \"title\" : \"MongoDB\", \"description\" : \"MongoDB 是一个 Nosql 数据库\", \"by\" : \"Runoob\", \"url\" : \"http://www.runoob.com\", \"tags\" : [ \"mongodb\", \"NoSQL\" ], \"likes\" : 110 }) -- pretty() 方法以格式化的方式来显示所有文档。 db.col.remove({'title':'MongoDB 教程'}) db.col.remove(DELETION_CRITERIA,1) db.repairDatabase() db.inventory.deleteMany({ \"likes\" : 110 }) db.col.find({\"likes\": {$gt:50}, $or: [{\"by\": \"菜鸟教程\"},{\"title\": \"MongoDB 教程\"}]}).pretty() mongodb的导入导出 导出工具mongoexport Mongodb中的mongoexport工具可以把一个collection导出成JSON格式或CSV格式的文件。可以通过参数指定导出的数据项，也可以根据指定的条件导出数据。 可通过 mongoexport --help 命令查看具体使用方法 参数说明： -h:指明数据库宿主机的IP -u:指明数据库的用户名 -p:指明数据库的密码 -d:指明数据库的名字 -c:指明collection的名字 -f:指明要导出那些列 -o:指明到要导出的文件名 -q:指明导出数据的过滤条件 示例： 导出goods数据库下students集合的数据 执行图中命令 bin目录下生成students.dat文件，内容如下(也可在命令行中执行 type students.dat 查看) 从上面的结果可以看出，我们在导出数据时没有显示指定导出样式 ，默认导出了JSON格式的数据。实际情况下常常需要导出csv格式的数据，命令如下 mongoexport -d goods -c students --csv -f classid,name,age -o students_csv.dat 参数详解： -d:指明使用的库，本例中为goods -c:指明要导出的集合，本例中为students -o:指明要导出的文件名，本例中为students_csv.dat -csv：指明要导出为csv格式 -f：指明需要导出classid、name、age这3列的数据 导入工具mongoimport Mongodb中的mongoimport工具可以把一个特定格式文件中的内容导入到指定的collection中。该工具可以导入JSON格式数据，也可以导入CSV格式数据。 可通过 mongoimport --help 命令查看具体使用方法 参数说明： -h:指明数据库宿主机的IP -u:指明数据库的用户名 -p:指明数据库的密码 -d:指明数据库的名字 -c:指明collection的名字 -f:指明要导入那些列 示例 先删除students集合数据，验证 db.students.remove({}); db.students.find(); 导入之前导出的students.dat文件 上面演示的是导入JSON格式的文件中的内容，如果要导入CSV格式文件中的内容，则需要通过--type参数指定导入格式 mongoimport -d goods -c students --type csv --headerline --file students_csv.dat 参数详解 -d:指明数据库名，本例中为goods -c:指明collection名，本例中为students -type:指明要导入的文件格式 -headerline:指明第一行是列名，不需要导入 -file：指明要导入的文件 students_csv.dat：导入的文件名 © vishon all right reserved，powered by GitbookUpdated at 2021-05-23 14:18:24 "},"database/redis-devops-note.html":{"url":"database/redis-devops-note.html","title":"【datebase】redis运维笔记","keywords":"","body":"redis cluster安装 下载并解压 cd /root/software wget http://download.redis.io/releases/redis-3.2.4.tar.gz tar -zxvf redis-3.2.4.tar.gz 编译安装 cd redis-3.2.4 make && make install 将redis-trib.rb复制到/usr/local/bin目录下 cd src cp redis-trib.rb /usr/local/bin/ 创建 Redis 节点 首先在 192.168.31.245 机器上 /root/software/redis-3.2.4 目录下创建 redis_cluster 目录； mkdir redis_cluster 在 redis_cluster 目录下，创建名为7000、7001、7002的目录，并将 redis.conf 拷贝到这三个目录中 mkdir 7000 7001 7002 cp redis.conf redis_cluster/7000 cp redis.conf redis_cluster/7001 cp redis.conf redis_cluster/7002 分别修改这三个配置文件，修改如下内容 port 7000 //端口7000,7002,7003 bind 本机ip //默认ip为127.0.0.1 需要改为其他节点机器可访问的ip 否则创建集群时无法访问对应的端口，无法创建集群 daemonize yes //redis后台运行 pidfile /var/run/redis_7000.pid //pidfile文件对应7000,7001,7002 cluster-enabled yes //开启集群 把注释#去掉 cluster-config-file nodes_7000.conf //集群的配置 配置文件首次启动自动生成 7000,7001,7002 cluster-node-timeout 15000 //请求超时 默认15秒，可自行设置 appendonly yes //aof日志开启 有需要就开启，它会每次写操作都记录一条日志　 masterauth abc //注意:如果对集群设置密码,需做以下改动,否则不能设置密码。 requirepass abc //requirepass和masterauth都需要设置，并且每个节点的密码需要一致，否则发生主从切换时，就会遇到授权问题，可以模拟并观察日志 接着在另外一台机器上（192.168.31.210），的操作重复以上三步，只是把目录改为7003、7004、7005，对应的配置文件也按照这个规则修改即可 启动各个节点 第一台机器上执行 redis-server redis_cluster/7000/redis.conf redis-server redis_cluster/7001/redis.conf redis-server redis_cluster/7002/redis.conf 另外一台机器上执行 redis-server redis_cluster/7003/redis.conf redis-server redis_cluster/7004/redis.conf redis-server redis_cluster/7005/redis.conf 检查 redis 启动情况 ps -ef | grep redis root 61020 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7000 [cluster] root 61024 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7001 [cluster] root 61029 1 0 02:14 ? 00:00:01 redis-server 127.0.0.1:7002 [cluster] netstat -tnlp | grep redis tcp 0 0 127.0.0.1:17000 0.0.0.0:* LISTEN 61020/redis-server tcp 0 0 127.0.0.1:17001 0.0.0.0:* LISTEN 61024/redis-server tcp 0 0 127.0.0.1:17002 0.0.0.0:* LISTEN 61029/redis-server tcp 0 0 127.0.0.1:7000 0.0.0.0:* LISTEN 61020/redis-server tcp 0 0 127.0.0.1:7001 0.0.0.0:* LISTEN 61024/redis-server tcp 0 0 127.0.0.1:7002 0.0.0.0:* LISTEN 61029/redis-server ##另外一台机器 ps -ef | grep redis root 9957 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7003 [cluster] root 9964 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7004 [cluster] root 9971 1 0 02:32 ? 00:00:01 redis-server 127.0.0.1:7005 [cluster] root 10065 4744 0 02:38 pts/0 00:00:00 grep --color=auto redis netstat -tlnp | grep redis tcp 0 0 127.0.0.1:17003 0.0.0.0:* LISTEN 9957/redis-server 1 tcp 0 0 127.0.0.1:17004 0.0.0.0:* LISTEN 9964/redis-server 1 tcp 0 0 127.0.0.1:17005 0.0.0.0:* LISTEN 9971/redis-server 1 tcp 0 0 127.0.0.1:7003 0.0.0.0:* LISTEN 9957/redis-server 1 tcp 0 0 127.0.0.1:7004 0.0.0.0:* LISTEN 9964/redis-server 1 tcp 0 0 127.0.0.1:7005 0.0.0.0:* LISTEN 9971/redis-server 1 创建集群 Redis 官方提供了 redis-trib.rb 这个工具，就在解压目录的 src 目录中，第三步中已将它复制到 /usr/local/bin 目录中，可以直接在命令行中使用了。使用下面这个命令即可完成安装。 redis-trib.rb create --replicas 1 192.168.31.245:7000 192.168.31.245:7001 192.168.31.245:7002 192.168.31.210:7003 192.168.31.210:7004 192.168.31.210:7005 其中，前三个 ip:port 为第一台机器的节点，剩下三个为第二台机器。 如果运行报下面错误， 是因为这个工具是用 ruby 实现的，所以需要安装 ruby。安装命令如下： yum -y install ruby ruby-devel rubygems rpm-build gem install redis 之后再运行 redis-trib.rb 命令，会出现如下提示： 输入 yes 即可，然后出现如下内容，说明安装成功。 　　 集群验证 在第一台机器上连接集群的7002端口的节点，在另外一台连接7005节点，连接方式为 redis-cli -h 192.168.31.245 -c -p 7002 ,加参数 -C 可连接到集群，因为上面 redis.conf 将 bind 改为了ip地址，所以 -h 参数不可以省略。 在7005节点执行命令 set hello world ，执行结果如下： 然后在另外一台7002端口，查看 key 为 hello 的内容， get hello ，执行结果如下： 说明集群运作正常。 简单说一下原理 redis cluster在设计的时候，就考虑到了去中心化，去中间件，也就是说，集群中的每个节点都是平等的关系，都是对等的，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。 Redis 集群没有并使用传统的一致性哈希来分配数据，而是采用另外一种叫做哈希槽 (hash slot)的方式来分配的。redis cluster 默认分配了 16384 个slot，当我们set一个key 时，会用CRC16算法来取模得到所属的slot，然后将这个key 分到哈希槽区间的节点上，具体算法就是：CRC16(key) % 16384。所以我们在测试的时候看到set 和 get 的时候，直接跳转到了7000端口的节点。 Redis 集群会把数据存在一个 master 节点，然后在这个 master 和其对应的salve 之间进行数据同步。当读取数据时，也根据一致性哈希算法到对应的 master 节点获取数据。只有当一个master 挂掉之后，才会启动一个对应的 salve 节点，充当 master 。 需要注意的是：必须要3个或以上的主节点，否则在创建集群时会失败，并且当存活的主节点数小于总节点数的一半时，整个集群就无法提供服务了。 Docker搭建Redis集群 集群规划 ip 节点名称 角色 192.168.30.30 master sentinel 192.168.30.31 slave1 sentinel 192.168.30.32 slave2 sentinel redis集群搭建 在30上创建目录 /root/redis/conf /root/redis/data /root/redis/log 用来存放容器中的配置文件，持久化数据和日志 修改master节点redis.conf,从网上下载一份拷贝到/root/redis/conf修改，修改如下： A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6379 C.pidfile 为 pidfile /var/run/redis_6379.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no Slave1修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6380 C.pidfile 为 pidfile /var/run/redis_6380.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no F.设置主服务器 replicaof 192.168.30.30 6379 Slave2修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 6381 C.pidfile 为 pidfile /var/run/redis_6381.pid D.logfile \"/log/redis.log\" E.保护模式为no:protected-mode no F.设置主服务器 replicaof 192.168.30.30 6379 在三台机器上分别执行容器运行命令 docker run -p 6379:6379 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-master --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes docker run -p 6380:6380 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-slave1 --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes docker run -p 6381:6381 -v /root/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf -v /root/redis/data:/data -v /root/redis/log:/log -d --name redis-slave2 --restart=always --network=host --privileged=true 106.54.126.251:5000/redis:5.0.4 redis-server /usr/local/etc/redis/redis.conf --appendonly yes sentinel集群搭建 修改master节点sentinel.conf,从网上下载一份拷贝到/root/redis/conf修改，修改如下： A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26379 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no Slave1修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26380 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no Slave2修改如下 A.注释掉IP绑定 #bind 127.0.0.1 B.端口设置为 port 26381 C.修改监控主机ip：sentinel monitor mymaster 192.168.30.30 6379 2 D.logfile \"/log/sentinel.log\" E.注释保护模式为:protected-mode no 在三台机器上分别执行容器运行命令 docker run -it --name sentinel-01 -p 26379:26379 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf docker run -it --name sentinel-02 -p 26380:26380 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf docker run -it --name sentinel-03 -p 26381:26381 -v /root/redis/conf/sentinel.conf:/usr/local/etc/redis/sentinel.conf -v /root/redis/log:/log -d --restart=always --network=host --privileged 106.54.126.251:5000/redis:5.0.4 redis-sentinel /usr/local/etc/redis/sentinel.conf 到此，集群搭建完毕，可以停掉master,看会不会更换master节点 Redis日志的格式 In the log files the various log levels are represented as follows: . debug - verbose * notice # warning The log output for Redis 2.x will look something like this: [pid] date loglevel message For instance: [4018] 14 Nov 07:01:22.119 * Background saving terminated with success The possible values for role are as follows: X sentinel C RDB/AOF writing child S slave M master © vishon all right reserved，powered by GitbookUpdated at 2021-05-23 15:39:50 "},"database/elasticsearch-devops-note.html":{"url":"database/elasticsearch-devops-note.html","title":"【datebase】elasticsearch运维笔记","keywords":"","body":"docker-compose搭建Elasticsearch集群 在主节点创建一个目录es,并创建docker-compose.yaml，主节点yaml文件如下 version: '2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.2 privileged: true environment: - cluster.name=docker-cluster - xpack.security.enabled=false - bootstrap.memory_lock=true - node.master=true - node.data=true # store data on master - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" #根据机器实际来分配 - \"discovery.zen.ping.unicast.hosts=192.168.30.30\" # put your master_ip here! make sure `telnet MASTER_IP 9300` is oK - \"transport.host=0.0.0.0\" - \"network.host=0.0.0.0\" - node.name=esmaster ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - /data/elasticsearch-service/data:/usr/share/elasticsearch/data ports: - 9208:9200 - 9308:9300 network_mode: \"host\" 执行命令docker-compose up -d启动es，一般会报错max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]，可以通过sudo sysctl -w vm.max_map_count=262144设置下值。 在node节点上同样操作创建es目录，创建docker-compose.yaml，node节点yaml文件如下 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.2 privileged: true environment: - cluster.name=docker-cluster - xpack.security.enabled=false - bootstrap.memory_lock=true - node.master=false - node.data=true - \"ES_JAVA_OPTS=-Xms12g -Xmx12g\" - \"discovery.zen.ping.unicast.hosts=192.168.30.30\" # put your master_ip here! make sure `telnet MASTER_IP 9300` is oK ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 cap_add: - IPC_LOCK volumes: - /data/elasticsearch-service/data:/usr/share/elasticsearch/data ports: - 9208:9200 - 9308:9300 network_mode: \"host\" 执行命令docker-compose up -d启动es，一般会报错max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]，可以通过sudo sysctl -w vm.max_map_count=262144设置下值 helm部署es集群到k8s上 # helm repo add elastic https://helm.elastic.co # helm install --name elasticsearch elastic/elasticsearch elasticsearch命令操作大全 命令格式 curl -X :/// ：REST风格的语法谓词 :节点ip :节点端口号，默认9200 :索引名 :索引类型 :操作对象的ID号 访问带鉴权的es curl -u 'elastic:Je2pW2SOa' http://192.168.30.32:9200/ 查看es信息 curl http://192.168.30.32:9200/ 查看集群状态 curl http://192.168.30.32:9200/_cat/nodes 查看所有索引 curl http://192.168.30.32:9200/_cat/indices 创建索引nwx curl -XPUT '192.168.30.32:9200/nwx?pretty' 查看customer索引状态: curl -XGET http://192.168.30.32:9200/_cat/indices/customer/?pretty 往索引customer中插入数据，类型为external，id为1 curl -XPUT '192.168.30.32:9200/customer/external/1?pretty' -d ' { \"name\": \"nie wei xing\" }' 往索引customer中插入数据，类型为aaa，id为1 curl -XPUT '192.168.30.32:9200/customer/aaa/1?pretty' -d ' { \"name\": \"nie wei xing\" }' 往索引customer中批量插入数据 curl -XPOST '192.168.30.32:9200/customer/external/_bulk?pretty' -d ' {\"index\":{\"_id\":\"3\"}} {\"name\": \"John Doe\" } {\"index\":{\"_id\":\"4\"}} {\"name\": \"Jane Doe\" } ' 导入数据集 curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/bank/account/_bulk?pretty' --data-binary @accounts.json curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/shakespeare/doc/_bulk?pretty' --data-binary @shakespeare_6.0.json curl -H 'Content-Type: application/x-ndjson' -XPOST '192.168.30.32:9200/_bulk?pretty' --data-binary @logs.jsonl 查询插入的数据 curl -XGET '192.168.30.32:9200/customer/external/1?pretty' curl -XGET '192.168.30.32:9200/customer/aaa/1?pretty' 查询某个索引的所有数据 curl '192.168.30.32:9200/bank/_search?q=*&pretty' curl '192.168.30.32:9200/customer/_search?q=*&pretty' curl '192.168.30.32:9200/shakespeare/_search?q=*&pretty' curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} } }' 匹配所有数据，但只返回1个 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"size\": 1 }' 注意：如果siez不指定，则默认返回10条数据。 返回从11到20的数据。（索引下标从0开始） curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"from\" : 10, \"size\" : 10 }' 匹配所有数据返回前10条 匹配所有的索引中的数据，按照balance字段降序排序，并且返回前10条 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"sort\": { \"balance\": { \"order\": \"desc\" } } }' 下面例子展示如何返回两个字段（account_number balance） curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] }' 返回account_number 为20 的数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { 　\"query\": { \"match\": { \"account_number\": 20 } } }' 返回address中包含mill的所有数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { 　\"query\": { \"match\": { \"address\": \"mill\" } } }' 　 返回地址中包含mill或者lane的所有数据 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match\": { \"address\": \"mill lane\" } } }' 这个例子是多匹配（match_phrase短语匹配） 返回地址中包含短语 “mill lane”的所有数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match_phrase\": { \"address\": \"mill lane\" } } }' must布尔查询 布尔查询允许我们将多个简单的查询组合成一个更复杂的布尔逻辑查询。 这个例子将两个查询组合，返回地址中含有mill和lane的所有记录数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' Should布尔查询 上述例子中，must表示所有查询必须都为真才被认为匹配。 相反, 这个例子组合两个查询，返回地址中含有mill或者lane的所有记录数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' must_not布尔查询 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must_not\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' 多级逻辑查询 上述例子中,must_not表示查询列表中没有为真的（也就是全为假）时则认为匹配。 我们可以组合must、should、must_not来实现更加复杂的多级逻辑查询。 下面这个例子返回年龄大于40岁、不居住在ID的所有数据： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"age\": \"40\" } } ], \"must_not\": [ { \"match\": { \"state\": \"ID\" } } ] } } }' 过滤filter(查询条件设置) 下面这个例子使用了布尔查询返回balance在20000到30000之间的所有数据。 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"range\": { \"balance\": { \"gte\": 20000, \"lte\": 30000 } } } } } }' 聚合 Aggregations 下面这个例子： 将所有的数据按照state分组（group），然后按照分组记录数从大到小排序，返回前十条（默认）： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" } } } }' 下面这个实例按照state分组，降序排序，返回balance的平均值： curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"size\": 0, \"aggs\": { \"group_by_state\": { \"terms\": { \"field\": \"state.keyword\" }, \"aggs\": { \"average_balance\": { \"avg\": { \"field\": \"balance.keyword\" } } } } } }' 带条件查询 curl -XPOST '192.168.30.32:9200/bank/_search?pretty' -d ' { \"query\": { \"match\": { \"account_number\": 20 } } }' 索引数据的修改 curl -XPUT '192.168.30.32:9200/customer/external/1?pretty' -d ' { \"name\": \"Jane Doe\" }' 索引数据的更新 curl -XPOST '192.168.30.32:9200/customer/external/1/_update?pretty' -d ' { \"doc\": {\"name\": \"Jane Doe1\"} }' 索引数据的删除 curl -XDELETE '192.168.30.32:9200/customer/external/2?pretty' 索引的删除 curl -XDELETE '192.168.30.32:9200/nwx?pretty' 更新id为1的内容并删除id为2的 curl -XPOST '192.168.30.32:9200/customer/external/_bulk?pretty' -d ' {\"update\":{\"_id\":\"1\"}} {\"doc\": { \"name\": \"John Doe becomes Jane Doe\" } } {\"delete\":{\"_id\":\"2\"}}' 设置全局的慢日志级别 curl -XPUT '192.168.30.32:9200/_cluster/settings/?pretty' -H 'Content-Type: application/json' -d '{ \"transient\": { \"logger.index.search.slowlog\":\"DEBUG\", \"logger.index.indexing.slowlog\":\"DEBUG\" } }' 设置customer索引的慢日志超时时间 curl -XPUT '192.168.30.32:9200/customer/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' curl -XPUT '192.168.30.32:9200/people/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' curl -XPUT '192.168.30.32:9200/bank/_settings/?pretty' -H 'Content-Type: application/json' -d '{ \"index.search.slowlog.threshold.query.debug\" : \"1ms\", \"index.search.slowlog.threshold.fetch.debug\": \"1ms\", \"index.indexing.slowlog.threshold.index.debug\": \"1ms\" }' © vishon all right reserved，powered by GitbookUpdated at 2021-07-30 15:29:24 "},"docker/dockerfile-study.html":{"url":"docker/dockerfile-study.html","title":"【docker】dockerfile学习笔记","keywords":"","body":"DockerFile的例子 FROM centos MAINTAINER nobody \"xx@qq.com\" RUN mkdir -p /opt/jdk/ RUN mkdir -p /opt/tomcat/ ADD jdk1.7.0_79 /opt/jdk/ ADD tomcat /opt/tomcat/ ENV CATALINA_HOME /opt/tomcat ENV JAVA_HOME /opt/jdk EXPOSE 8080 VOLUME /opt/tomcat/data ENV PATH $PATH:$JAVA_HOME/bin CMD [\"/opt/tomcat/bin/catalina.sh\",\"run\"] DockerFile的语法解析 Dockerfile的基本指令有十三个，分别是：FROM、MAINTAINER、RUN、CMD、EXPOSE、ENV、ADD、COPY、ENTRYPOINT、VOLUME、USER、WORKDIR、ONBUILD。下面对这些指令的用法一一说明。 FROM 用法：FROM 说明：第一个指令必须是FROM了，其指定一个构建镜像的基础源镜像，如果本地没有就会从公共库中拉取，没有指定镜像的标签会使用默认的latest标签，可以出现多次，如果需要在一个Dockerfile中构建多个镜像。 MAINTAINER 用法：MAINTAINER 说明：描述镜像的创建者，名称和邮箱 RUN 用法：RUN \"command\" \"param1\" \"param2\" 说明：RUN命令是一个常用的命令，RUN命令可以执行多次，执行完成之后会成为一个新的镜像，这里也是指镜像的分层构建。一句RUN就是一层，也相当于一个版本。这就是之前说的缓存的原理。我们知道docker是镜像层是只读的，所以你如果第一句安装了软件，用完在后面一句删除是不可能的。所以这种情况要在一句RUN命令中完成，可以通过&符号连接多个RUN语句。RUN后面的必须是双引号不能是单引号（没引号貌似也不要紧），command是不会调用shell的，所以也不会继承相应变量，要查看输入RUN \"sh\" \"-c\" \"echo\" \"$HOME\"，而不是RUN \"echo\" \"$HOME\"。 CMD 用法：CMD command param1 param2 说明：CMD在Dockerfile中只能出现一次，有多个，只有最后一个会有效。其作用是在启动容器的时候提供一个默认的命令项。如果用户执行docker run的时候提供了命令项，就会覆盖掉这个命令。没提供就会使用构建时的命令。 EXPOSE 用法：EXPOSE [...] 说明：告诉Docker服务器容器对外映射的容器端口号，在docker run -p的时候生效。 ENV 用法：EVN 只能设置一个 　　　EVN =允许一次设置多个 说明：设置容器的环境变量，可以让其后面的RUN命令使用，容器运行的时候这个变量也会保留。 ADD 用法：ADD 说明：复制本机文件或目录或远程文件，添加到指定的容器目录，支持GO的正则模糊匹配。路径是绝对路径，不存在会自动创建。如果源是一个目录，只会复制目录下的内容，目录本身不会复制。ADD命令会将复制的压缩文件夹自动解压，这也是与COPY命令最大的不同。 COPY 用法：COPY 说明：COPY除了不能自动解压，也不能复制网络文件。其它功能和ADD相同。 ENTRYPOINT(推荐使用这个作为启动命令) 用法：ENTRYPOINT \"command\" \"param1\" \"param2\" 说明：这个命令和CMD命令一样，唯一的区别是不能被docker run命令的执行命令覆盖，如果要覆盖需要带上选项--entrypoint，如果有多个选项，只有最后一个会生效。 VOLUME 用法：VOLUME [\"path\"] 说明：在主机上创建一个挂载，挂载到容器的指定路径。docker run -v命令也能完成这个操作，而且更强大。这个命令不能指定宿主机的需要挂载到容器的文件夹路径。但docker run -v可以，而且其还可以挂载数据容器。 USER 用法：USER daemon 说明：指定运行容器时的用户名或UID，后续的RUN、CMD、ENTRYPOINT也会使用指定的用户运行命令。 WORKDIR 用法:WORKDIR path 说明：为RUN、CMD、ENTRYPOINT指令配置工作目录。可以使用多个WORKDIR指令，后续参数如果是相对路径，则会基于之前的命令指定的路径。如：WORKDIR /home　　WORKDIR test 。最终的路径就是/home/test。path路径也可以是环境变量，比如有环境变量HOME=/home，WORKDIR $HOME/test也就是/home/test。 ONBUILD 用法：ONBUILD [INSTRUCTION] 说明：配置当前所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。意思就是，这个镜像创建后，如果其它镜像以这个镜像为基础，会先执行这个镜像的ONBUILD命令。 ARG 语法：ARG [=] 设置变量命令，ARG命令定义了一个变量，在docker build创建镜像的时候，使用 --build-arg =来指定参数 如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning © vishon all right reserved，powered by GitbookUpdated at 2021-06-01 12:49:46 "},"docker/docker-compose-study.html":{"url":"docker/docker-compose-study.html","title":"【docker】docker-compose学习笔记","keywords":"","body":"Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速在集群中部署分布式应用。 安装方式 二进制安装 $ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose bash 补全命令 $ curl -L https://raw.githubusercontent.com/docker/compose/1.8.0/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose 删除docker-compose sudo rm /usr/local/bin/docker-compose pip方式安装 安装python-pip yum -y install python-pip 安装docker-compose pip install docker-compose 待安装完成后，执行查询版本的命令，即可安装docker-compose docker-compose version docker-compose.yml文件的编写 version: '3' services: web: build: . ports: - \"5000:5000\" redis: image: \"redis:alpine\" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。 如果使用build指令，在Dockerfile中设置的选项(例如： CMD , EXPOSE , VOLUME , ENV等)将会自动被获取，无需在docker-compose.yml中再次设置。 build 指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。Compose将会利用它自动构建这个镜像，然后使用这个镜像 其他具体参数可以参考官方文档Docker Compose的Compose 模板文件部分 docker-compose命令 Options: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert deploy keys in v3 files to their non-Swarm equivalent Commands: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information © vishon all right reserved，powered by GitbookUpdated at 2021-06-01 12:55:17 "},"docker/Installation-and-use-of-containerd.html":{"url":"docker/Installation-and-use-of-containerd.html","title":"【containerd】containerd的安装和使用","keywords":"","body":"containerd的安装和使用 Containerd是一个工业标准的容器运行时，重点是它简洁，健壮，便携，在Linux和window上可以作为一个守护进程运行，它可以管理主机系统上容器的完整的生命周期：镜像传输和存储，容器的执行和监控，低级别的存储和网络。 containerd和docker不同，containerd重点是继承在大规模的系统中，例如kubernetes，而不是面向开发者，让开发者使用，更多的是容器运行时的概念，承载容器运行。 containerd的架构图如下： 安装containerd 安装containerd这里写了一个脚本来快速部署，脚本会部署containerd和crictl命令行工具到机器上，crictl是CRI兼容的容器运行时命令行接口，可以用来操作containerd的镜像和容器等。 运行下面脚本需要填写2个参数，第一个是crictl的版本，第二个参数填写containerd版本，注意传入的版本号需要去掉开头的v。 crictl版本号获取地址: https://github.com/kubernetes-sigs/cri-tools/tags containerd版本号获取地址: https://github.com/containerd/containerd/tags crictl 默认连接到 unix:///var/run/dockershim.sock。 对于其他的运行时，你可以用多种不同的方法设置端点： 通过设置参数 --runtime-endpoint 和 --image-endpoint 通过设置环境变量 CONTAINER_RUNTIME_ENDPOINT 和 IMAGE_SERVICE_ENDPOINT 通过在配置文件中设置端点 --config=/etc/crictl.yaml 你还可以在连接到服务器并启用或禁用调试时指定超时值，方法是在配置文件中指定 timeout 或 debug 值，或者使用 --timeout 和 --debug 命令行参数。 install-containerd.sh内容如下 #!/bin/bash crictl_version=$1 contained_version=$2 if [ $# = 0 ];then echo \"Run 'sh install-containerd.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Please enter the first parameter enters the crictl version, the second parameter enters the containerd version Usage: sh install-containerd.sh [crictl_version] [contained_version]\" fi main(){ # download crictl client wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v${crictl_version}/crictl-v${crictl_version}-linux-amd64.tar.gz tar -C /usr/local/bin -xf crictl-v${crictl_version}-linux-amd64.tar.gz rm -rf crictl-v${crictl_version}-linux-amd64.tar.gz # download containerd pkg wget https://github.com/containerd/containerd/releases/download/v${contained_version}/containerd-${contained_version}-linux-amd64.tar.gz tar -C /usr/local -xf containerd-${contained_version}-linux-amd64.tar.gz rm -rf containerd-${contained_version}-linux-amd64.tar.gz #crictl start config cat /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF # create contained config mkdir -p /etc/containerd cd /usr/local/bin/ ./containerd config default > /etc/containerd/config.toml # systemd manager containerd cat /lib/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Delegate=yes KillMode=process LimitNOFILE=1048576 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF # Start containerd and set it to start automatically sudo systemctl daemon-reload sudo systemctl enable containerd.service sudo systemctl start containerd.service sudo systemctl status containerd.service } if [ $# = 2 ];then main fi 检查containerd的运行状态，如果提示running则说明安装成功 [root@VM-0-13-centos ~]# systemctl status containerd.service ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-08-19 09:46:45 CST; 2 weeks 0 days ago Docs: https://containerd.io Main PID: 828192 (containerd) CGroup: /system.slice/containerd.service ................... crictl常用的命令 [root@VM-0-13-centos ~]# crictl -h NAME: crictl - client for CRI USAGE: crictl [global options] command [command options] [arguments...] VERSION: v1.22.0 COMMANDS: attach Attach to a running container create Create a new container exec Run a command in a running container version Display runtime version information images, image, img List images inspect Display the status of one or more containers inspecti Return the status of one or more images imagefsinfo Return image filesystem info inspectp Display the status of one or more pods logs Fetch the logs of a container port-forward Forward local port to a pod ps List containers pull Pull an image from a registry run Run a new container inside a sandbox runp Run a new pod rm Remove one or more containers rmi Remove one or more images rmp Remove one or more pods pods List pods start Start one or more created containers info Display information of the container runtime stop Stop one or more running containers stopp Stop one or more running pods update Update one or more running containers config Get and set crictl client configuration options stats List container(s) resource usage statistics completion Output shell completion code help, h Shows a list of commands or help for one command crictl是没有构建命令的，如果想构建镜像可以用docker或者用buildah工具，buildah可以参考文档https://github.com/containers/buildah containerd也有自带客户端工具，叫ctr，执行ctr命令时需要带上--namespace http://k8s.io，建议还是安装crictl工具操作。 crictl配置私有镜像仓库 crictl是没有login命令的，如果需要拉取私有镜像仓库的镜像，需要在containerd的配置文件/etc/containerd/config.toml中配置私有镜像仓库的登录信息 [plugins] [plugins.cri.registry.mirrors.\"ccr.ccs.tencentyun.com\"] endpoint = [\"https://ccr.ccs.tencentyun.com\"] [plugins.cri.registry.configs.\"ccr.ccs.tencentyun.com\".auth] username = \"xxxxx\" password = \"xxxxx\" 配置好之后在重启containerd即可拉取私有镜像 # systemctl restart containerd.service # crictl pull ccr.ccs.tencentyun.com/xxx/xxx:v1 配置镜像加速 有的时候为了能够加速镜像的拉取，需要配置镜像加速的代理，可以在/etc/containerd/config.toml配置镜像加速 [plugins] [plugins.cri.registry.mirrors.\"docker.io\"] endpoint = [\"https://mirror.ccs.tencentyun.com\",\"https://xxxx.mirror.aliyuncs.com\"] 配置好之后在重启containerd即可生效 # systemctl restart containerd.service © vishon all right reserved，powered by GitbookUpdated at 2021-09-02 13:28:23 "},"docker/Docker-limits-the-disk-space-that-containers-can-occupy.html":{"url":"docker/Docker-limits-the-disk-space-that-containers-can-occupy.html","title":"【docker】docker限制容器可占用的磁盘空间","keywords":"","body":"docker限制容器可占用的磁盘空间 Docker容器默认启动的虚拟机，会占用宿主机的资源（CPU、内存、硬盘），例如默认Docker基于Overlay2驱动方式，容器硬盘的rootfs根分区空间是整个宿主机的空间大小。 可以指定默认容器的大小（在启动容器的时候指定），可以在docker配置文件指定Docker容器rootfs容量大小。 如果这里需要设置容器可用磁盘空间大小，需要保证节点的文件系统是xfs 具体配置如下，修改/etc/docker/daemon.json文件，设置容器的磁盘空间大小为20G。 { \"data-root\": \"/data/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\", \"overlay2.size=20G\" ] } © vishon all right reserved，powered by GitbookUpdated at 2021-10-21 23:04:52 "},"k8s/k8s-common-commands.html":{"url":"k8s/k8s-common-commands.html","title":"【k8s】k8s常用命令总结","keywords":"","body":"k8s常用命令 获取hostNetwork网络模式的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostNetwork==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取特权模式的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.privileged==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取控制进程可以获得超出其父进程的特权的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.allowPrivilegeEscalation==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有deployment的容器名称和namespace kubectl get deployment --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*])]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有deployment的容器端口 kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' 获取配置了hostport的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' |grep map | grep hostPort | awk '{print $1 $2 \" \"$3 $4}' 获取配置了capabilities属性的deployment kubectl get deployment --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].securityContext} {\"\\n\"} {end}' |grep capabilities | awk '{print $1 $2 \" \"$3 $4}' 获取所有配置了hostNetwork的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostNetwork==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了hostIPC模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostIPC==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了hostPID模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.hostPID==true)]} {\"namespace: \"} {.metadata.namespace}{\" name:\"} {.metadata.name} {\"\\n\"} {end}' 获取所有配置了allowPrivilegeEscalation模式的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[?(@.spec.template.spec.containers[*].securityContext.allowPrivilegeEscalation==true)]} {\"namespace: \"}{.metadata.namespace}{\"name:\"} {.metadata.name} {\"\\n\"} {end}' 获取配置了hostport的DaemonSet kubectl get ds --all-namespaces -o jsonpath='{range .items[*]} {\"namespace: \"}{.metadata.namespace}{\" name:\"} {.metadata.name} {\" \"}{.spec.template.spec.containers[*].ports} {\"\\n\"} {end}' |grep map | grep hostPort | awk '{print $1 $2 \" \"$3 $4}' 获取所有pod的ip和所在node的ip kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}[nodeip:{.status.hostIP}, podip:{.status.podIP}]{\"\\n\"}{end}' © vishon all right reserved，powered by GitbookUpdated at 2021-05-21 17:07:57 "},"k8s/k8s-force-delete-terminating-ns.html":{"url":"k8s/k8s-force-delete-terminating-ns.html","title":"【k8s】强制删除Terminating状态ns","keywords":"","body":"强制删除Terminating状态ns kubectl get ns 查看处于Terminating的ns [root@VM_1_4_centos ~]# kubectl get ns | grep testns testns Terminating 21d 将处于Terminating的ns的描述文件保存下来 [root@VM_1_4_centos ~]# kubectl get ns testns -o json > tmp.json [root@VM_1_4_centos ~]# cat tmp.json { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2020-10-13T14:28:07Z\", \"name\": \"testns\", \"resourceVersion\": \"13782744400\", \"selfLink\": \"/api/v1/namespaces/testns\", \"uid\": \"9ff63d71-a4a1-43bc-89e3-78bf29788844\" }, \"spec\": { \"finalizers\": [ \"kubernetes\" ] }, \"status\": { \"phase\": \"Terminating\" } } 本地启动kube proxy kubectl proxy --port=8081 新开窗口执行删除操作 curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8081/api/v1/namespaces/testns/finalize 如果上面方法无法删除namespace，可以通过如下方法看下namespace是不是还有什么资源没有清理 若命名空间依然无法删除，则查询命名空间哪些资源 kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n 然后删除这些资源： $ kubectl -n p-4q9rv delete projectalertgroup.management.cattle.io/projectalert-workload-alert --grace-period=0 --force 若 Pod 还是无法删除，可以在 Pod 中添加补丁： kubectl -n p-4q9rv patch projectalertgroup.management.cattle.io/projectalert-workload-alert -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' 添加补丁后，强制删除： kubectl -n p-4q9rv delete projectalertrule.management.cattle.io/memory-close-to-resource-limited --grace-period=0 --force 然后执行下面命令删除namespace kubectl patch namespace -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' kubectl delete namespace cattle-system --grace-period=0 --force 其实也可以直接将修改对应ns生成json文件 [root@master-1 ~]# vim tmp.json 删除spec字段后，执行以下curl命令，使用kube-apiserver的8081端口，执行删除操作 #注意修改@XXX.json ，修改 namespaces/XXX/finalize ,其中XXX 表示你要删除的命名空间名称 [root@master-1 ~]# curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8081/api/v1/namespaces/mysql/finalize 用下面命令清理也可以 $ kubectl get ns delete-me -o json | jq '.spec.finalizers=[]' > ns-without-finalizers.json cat ns-without-finalizers.json $ kubectl proxy & $ PID=$! $ curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize -H \"Content-Type: application/json\" --data-binary @ns-without-finalizers.json $ kill $PID © vishon all right reserved，powered by GitbookUpdated at 2021-11-04 11:58:08 "},"k8s/userd-filebeat-as-sidecar-collect-log.html":{"url":"k8s/userd-filebeat-as-sidecar-collect-log.html","title":"【k8s】k8s中filebeat作为sidecar采集容器日志","keywords":"","body":"使用k8s的时候，会遇到一种情况就是需要单独采集下某个服务的日志进行过滤分析，这时候我们可以单独给服务部署一个filebeat的sidecar来采集过滤日志，下面我们来讲下如何部署 这里我们举例收集nginx服务的容器日志，直接部署下面的yaml文件即可 apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: log spec: replicas: 2 selector: matchLabels: project: www app: nginx template: metadata: labels: project: www app: nginx spec: imagePullSecrets: - name: qcloudregistrykey containers: - name: nginx image: nginx ports: - containerPort: 80 name: web protocol: TCP resources: requests: cpu: 0.5 memory: 256Mi limits: cpu: 1 memory: 1Gi volumeMounts: - name: nginx-logs mountPath: /var/log/nginx - name: filebeat image: elastic/filebeat:7.3.1 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] resources: limits: memory: 500Mi requests: cpu: 100m memory: 100Mi securityContext: runAsUser: 0 volumeMounts: - name: filebeat-config mountPath: /etc/filebeat.yml subPath: filebeat.yml - name: nginx-logs mountPath: /var/log/nginx volumes: - name: nginx-logs emptyDir: {} - name: filebeat-config configMap: name: filebeat-nginx-config --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-nginx-config namespace: log data: filebeat.yml: |- filebeat.inputs: - type: log paths: - /var/log/nginx/access.log # tags: [\"access\"] fields: app: www type: nginx-access fields_under_root: true setup.ilm.enabled: false setup.template.name: \"nginx-access\" setup.template.pattern: \"nginx-access-*\" output.elasticsearch: hosts: ['elasticsearch-master.log:9200'] index: \"nginx-access-%{+yyyy.MM.dd}\" 这里我们对上面的yaml进行说明下，我们将filebeat和nginx的/var/log/nginx目录挂载到emptyDir下，这样filebeat就可以直接读取到了nginx容器的日志目录，然后配置了一下filebeat的配置项，里面配置了日志采集的路径，以及输出的地址和索引名称，这里是直接收集到es里，当然你也可以投递到logstash和kafka logstash和kafka的配置参考如下 output.logstash: #logstash输出模块 enabled: true #启用模块 hosts: [\"localhost:5044\"] #logstash地址 worker: 1 #每个logstash的worker数？？？？？，默认1 compression_level: 3 #压缩级别，默认3 loadbalance: true #负载均衡开关，在不同的logstash间负载 pipelining: 0 #在处理新的批量期间，异步发送至logstash的批量次数？？？？？ index: 'filebeat' #可选配置，索引名称，默认为filebeat proxy_url: socks5://user:password@socks5-server:2233 #socks5代理服务器地址 proxy_use_local_resolver: false #使用代理时是否使用本地解析，默认false output.kafka: #kafka输出模块 output.redis: #redis输出模块 enabled: true #启用模块 hosts: [\"localhost:6379\"] #redis地址，地址为一个列表，如果loadbalance开启，则负载到里表中的服务器，当一个redis服务器不可达，事件将被分发到可到达的redis服务器 port: 6379 #redis端口，如果hosts内未包含端口信息，默认6379 key: filebeat #事件发布到redis的list或channel，默认filebeat password: #redis密码，默认无 db: 0 #redis的db值，默认0 datatype: list #发布事件使用的redis数据类型，如果为list，使用RPUSH命令（生产消费模式）。如果为channel，使用PUBLISH命令{发布订阅模式}。默认为list worker: 1 #为每个redis服务器启动的工作进程数，会根据负载均衡配置递增 loadbalance: true #负载均衡，默认开启 timeout: 5s #redis连接超时时间，默认5s max_retries: 3 #filebeat会忽略此设置，并一直重试到全部发送为止，其他beat设置为0即忽略，默认3次 bulk_max_size: 2048 ##对一个redis请求或管道批量的最大事件数，默认2048 proxy_url: #socks5代理地址，必须使用socks5:// proxy_use_local_resolver: false #使用代理时是否使用本地解析，默认false 因为我的es以及接到kibana里面了，这里我们试试在集群内访问下nginx服务的svc，在kibana是否能检索到日志 [root@VM-0-3-centos block]# for i in {1..100}; do curl http://172.16.90.29/; done 访问后，这里刷新下kibana，可以收到刚访问日志，我们访问了100次，这里也刚好收集100条 © vishon all right reserved，powered by GitbookUpdated at 2021-05-26 12:07:10 "},"k8s/k8s-generate-kubeonfig.html":{"url":"k8s/k8s-generate-kubeonfig.html","title":"【k8s】k8s中生成自定义用户kubeconfig","keywords":"","body":"这里我们讲一下如何用客户端证书和ca证书生成一份自定义用户的kubeconfig 进入节点上/etc/kubernetes/目录，我们发现节点有下面这几个文件，client的证书和秘钥是从kubelet-kubeconfig这个文件中提取出来的，提取方式参考文档https://cloud.tencent.com/developer/article/1814668，cluster-ca.crt是每个节点默认有的。 [root@VM-0-3-centos kubernetes]# ll total 56 -rw-r--r-- 1 root root 1135 Apr 17 21:55 client-cert.pem -rw-r--r-- 1 root root 1679 Apr 17 21:55 client-key.pem -rw-r--r-- 1 root root 1025 Nov 26 2020 cluster-ca.crt -rw-r--r-- 1 root root 5483 Nov 26 2020 kubelet-kubeconfig 下面我们执行下面命令来生成一份niewx.kubeconfig的kubeconfig，这里我们是指定了kubeconfig的名称，会生成在当前目录下，如果你不想指定名称，去掉--kubeconfig=niewx.kubeconfig这个，kubeconfig会默认生成在$HOME/.kube/config # 配置kubernetes集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/cluster-ca.crt \\ --embed-certs=true \\ --server=https://cls-xxxxx.ccs.tencent-cloud.com \\ --kubeconfig=niewx.kubeconfig # 配置客户端认证参数 kubectl config set-credentials niewx \\ --client-certificate=/etc/kubernetes/client-cert.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/client-key.pem \\ --kubeconfig=niewx.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=niewx \\ --kubeconfig=niewx.kubeconfig # 设置默认上下文 kubectl config --kubeconfig=niewx.kubeconfig use-context kubernetes 执行完上述命令就发现会生成一个user为niewx的kubeconfig文件，然后我们可以指定这个kubeconfig来访问集群，设置上下文非必须设置，不设置直接指定kubeconfg访问即可，或者将niewx.kubeconfig拷贝到$HOME/.kube/config这个文件，进行访问。 [root@VM-0-3-centos kubernetes]# ll | grep niewx -rw------- 1 root root 5477 Jun 2 19:10 niewx.kubeconfig [root@VM-0-3-centos kubernetes]# cat niewx.kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01UR server: https://cls-xxxxxx.ccs.tencent-cloud.com name: kubernetes contexts: - context: cluster: kubernetes user: niewx name: kubernetes current-context: \"\" kind: Config preferences: {} users: - name: niewx user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURHVENDQWdHZ0F3SUJBZ0lJS2NXVHpIY0ZudFl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6Q client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBd3NGWHdtNjlFVkY1WW1DNGx5bXVocFR2cGt6bCsxd0dTdWxGSnJqU0VpSTlWSTV6Ck9Zclh1UkM2VmtTTnRVa [root@VM-0-3-centos kubernetes]# kubectl --kubeconfig=niewx.kubeconfig get node NAME STATUS ROLES AGE VERSION 10.0.0.10 Ready 60d v1.18.4-tke.8 10.0.0.157 Ready 147d v1.18.4-tke.6 10.0.0.2 Ready 55d v1.18.4-tke.8 10.0.0.3 Ready 189d v1.18.4-tke.8 © vishon all right reserved，powered by GitbookUpdated at 2021-12-14 09:56:00 "},"k8s/kubecm-manages-k8s-clusters.html":{"url":"k8s/kubecm-manages-k8s-clusters.html","title":"【k8s】kubecm管理多k8s集群","keywords":"","body":"kubecm是一个k8s集群管理工具，可以合并多个kubeconfig文件，切换集群等 安装kubecm curl -Lo kubecm.tar.gz https://github.com/sunny0826/kubecm/releases/download/v0.15.3/kubecm_0.15.3_Linux_x86_64.tar.gz tar -zxvf kubecm.tar.gz kubecm cd kubecm sudo mv kubecm /usr/local/bin/ 设置自动补全 $ source > ~/.bashrc $ source ~/.bashrc kubecm的使用 执行kubecm，如果显示下面内容说明安装成功 [root@VM-0-13-centos ~]# kubecm Manage your kubeconfig more easily. ██ ██ ██ ██ ██████ ███████ ██████ ███ ███ ██ ██ ██ ██ ██ ██ ██ ██ ████ ████ █████ ██ ██ ██████ █████ ██ ██ ████ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██ ██████ ██████ ███████ ██████ ██ ██ Tips Find more information at: https://kubecm.cloud Usage: kubecm [command] Available Commands: add Add KubeConfig to $HOME/.kube/config alias Generate alias for all contexts clear Clear lapsed context, cluster and user completion Generates bash/zsh completion scripts create Create new KubeConfig(experiment) delete Delete the specified context from the kubeconfig help Help about any command list List KubeConfig merge Merge the KubeConfig files in the specified directory namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively version Print version info Flags: --config string path of kubeconfig (default \"/root/.kube/config\") -h, --help help for kubecm Use \"kubecm [command] --help\" for more information about a command. 添加集群 这里准备了3个集群的kubeconfig文件 [root@VM-0-13-centos .kube]# ll | grep config -rw-r--r-- 1 root root 1823 Jun 2 14:25 eks.config -rw-r--r-- 1 root root 5545 Jun 2 15:12 test.config -rw-r--r-- 1 root root 5541 Jun 2 14:54 tke.config 这里我们先创建一个config文件，用来生成合并后的kubeconfig [root@VM-0-13-centos .kube]# touch config [root@VM-0-13-centos .kube]# kubecm add -f tke.config Add Context: tke 👻 True 「tke.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ [root@VM-0-13-centos .kube]# kubecm add -f eks.config Add Context: eks 👻 True 「eks.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ [root@VM-0-13-centos .kube]# kubecm add -f test.config Add Context: test 👻 True 「test.config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | | test | cluster-9f86dg8h88 | user-9f86dg8h88 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ kubecm查看集群 [root@VM-0-13-centos .kube]# kubecm ls +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ xxx| default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ Cluster check succeeded! Kubernetes version v1.18.4-tke.6 Kubernetes master is running at https://cls-xxxxx.ccs.tencent-cloud.com [Summary] Namespace: 63 Node: 4 Pod: 155 kubecm删除集群 [root@VM-0-13-centos .kube]# kubecm delete test Context Delete:「test」 「/root/.kube/config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | * | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ 这里我们删除了test集群 kubecm切换集群 [root@VM-0-13-centos .kube]# kubectl get node NAME STATUS ROLES AGE VERSION 10.0.0.10 Ready 61d v1.18.4-tke.8 10.0.0.157 Ready 147d v1.18.4-tke.6 10.0.0.2 Ready 56d v1.18.4-tke.8 10.0.0.3 Ready 189d v1.18.4-tke.8 [root@VM-0-13-centos .kube]# kubecm switch eks 「/root/.kube/config」 write successful! +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | CURRENT | NAME | CLUSTER | USER | SERVER | Namespace | +============+=========+=======================+====================+===================================+==============+ | * | eks | cluster-6t8847hhfb | user-6t8847hhfb | https://xx.xx.xx.xx:443/ | default | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ | | tke | cluster-k4m2g9mf44 | user-k4m2g9mf44 | https://cls-xxxxxxxx.ccs.tence | default | | | | | | nt-cloud.com | | +------------+---------+-----------------------+--------------------+-----------------------------------+--------------+ Switched to context 「eks」 [root@VM-0-13-centos .kube]# kubectl get node NAME STATUS ROLES AGE VERSION eklet-subnet-ktam6hp8 Ready 56d v2.4.4-dirty 一开始我们默认操作集群是tke，现在切换到eks © vishon all right reserved，powered by GitbookUpdated at 2021-08-17 18:29:15 "},"k8s/getting-started-with-kustomize-actually.html":{"url":"k8s/getting-started-with-kustomize-actually.html","title":"【k8s】kustomize入门实践","keywords":"","body":"kustomize是kubernetes原生的配置管理，以无模板方式来定制应用的配置。kustomize使用k8s原生概念帮助创建并复用资源配置(YAML)，允许用户以一个应用描述文件（YAML 文件）为基础（Base YAML），然后通过Overlay的方式生成最终部署应用所需的描述文件。 这里简单了解下几个概念 overlay overlay 是一个 kustomization, 它修改(并因此依赖于)另外一个kustomization. overlay中的kustomization指的是一些其它的kustomization, 称为其 base. 没有 base, overlay 无法使用，并且一个 overlay 可以用作 另一个 overlay 的 base(基础)。简而言之，overlay 声明了与 base 之间的差异。通过 overlay 来维护基于 base 的不同 variants(变体)，例如开发、QA 和生产环境的不同variants，其实overlay就是不同版本的工作空间，依赖于base工作空间。 variant variant 是在集群中将 overlay 应用于 base 的结果。例如开发和生产环境都修改了一些共同 base 以创建不同的 variant。这些 variant 使用相同的总体资源，并与简单的方式变化，例如 deployment 的副本数、ConfigMap使用的数据源等。简而言之，variant 是含有同一组 base 的不同 kustomization，其实variant就是某一个版本环境的所有资源文件。 resource 在kustomize的上下文中，resource 是描述 k8s API 对象的 YAML 或 JSON 文件的相对路径。即是指向一个声明了 kubernetes API对象的YAML文件 patch 修改文件的一般说明。文件路径，指向一个声明了 kubernetes API patch 的 YAML 文件 kustomize安装 curl -s \"https://raw.githubusercontent.com/\\ kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash 安装成功后，执行kustomize可以查看帮助指导 [root@VM-0-13-centos mesh]# kustomize Manages declarative configuration of Kubernetes. See https://sigs.k8s.io/kustomize Usage: kustomize [command] Available Commands: build Print configuration per contents of kustomization.yaml cfg Commands for reading and writing configuration. completion Generate shell completion script create Create a new kustomization in the current directory edit Edits a kustomization file fn Commands for running functions against configuration. help Help about any command version Prints the kustomize version Flags: -h, --help help for kustomize --stack-trace print a stack-trace on error Additional help topics: kustomize docs-fn [Alpha] Documentation for developing and invoking Configuration Functions. kustomize docs-fn-spec [Alpha] Documentation for Configuration Functions Specification. kustomize docs-io-annotations [Alpha] Documentation for annotations used by io. kustomize docs-merge [Alpha] Documentation for merging Resources (2-way merge). kustomize docs-merge3 [Alpha] Documentation for merging Resources (3-way merge). kustomize tutorials-command-basics [Alpha] Tutorials for using basic config commands. kustomize tutorials-function-basics [Alpha] Tutorials for using functions. Use \"kustomize [command] --help\" for more information about a command. kustomize部署helloword kustomize的demo示例可以参考链接https://github.com/kubernetes-sigs/kustomize/tree/master/examples，下面我们以helloworld为例进行示范下 创建base 首先我们创建一个一个helloworld的工作空间，在/tmp下创建一个临时目录 DEMO_HOME=$(mktemp -d) 如果我们需要用到overlay，则需要创建base工作空间，让集群的资源放在base下 BASE=$DEMO_HOME/base mkdir -p $BASE curl -s -o \"$BASE/#1.yaml\" \"https://raw.githubusercontent.com\\ /kubernetes-sigs/kustomize\\ /master/examples/helloWorld\\ /{configMap,deployment,kustomization,service}.yaml\" 这样我们就将基础的yaml文件放到了base下 [root@VM-0-13-centos base]# tree $DEMO_HOME /tmp/tmp.w5Ic40K11n └── base ├── configMap.yaml ├── deployment.yaml ├── kustomization.yaml └── service.yaml 如果你想部署这些资源，可以用kubectl命令部署 kubectl apply -f $DEMO_HOME/base 我们可以预览下base的资源，会将base下的yaml内容打印在标准输出 [root@VM-0-13-centos base]# kustomize build $BASE apiVersion: v1 data: altGreeting: Good Morning! enableRisky: \"false\" kind: ConfigMap metadata: labels: app: hello name: the-map --- apiVersion: v1 kind: Service metadata: labels: app: hello name: the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: hello deployment: hello type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: hello name: the-deployment spec: replicas: 3 selector: matchLabels: app: hello deployment: hello template: metadata: labels: app: hello deployment: hello spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 当然我们也可以订制base下的，下面我们订制下app的label [root@VM-0-13-centos base]# sed -i.bak 's/app: hello/app: my-hello/' \\ > $BASE/kustomization.yaml [root@VM-0-13-centos base]# ll total 20 -rw-r--r-- 1 root root 117 Jun 4 12:10 configMap.yaml -rw-r--r-- 1 root root 750 Jun 4 12:10 deployment.yaml -rw-r--r-- 1 root root 266 Jun 4 12:37 kustomization.yaml -rw-r--r-- 1 root root 263 Jun 4 12:10 kustomization.yaml.bak -rw-r--r-- 1 root root 183 Jun 4 12:10 service.yaml [root@VM-0-13-centos base]# kustomize build $BASE | grep -C 3 app: kind: ConfigMap metadata: labels: app: my-hello name: the-map --- apiVersion: v1 kind: Service metadata: labels: app: my-hello name: the-service spec: ports: -- protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello type: LoadBalancer --- -- kind: Deployment metadata: labels: app: my-hello name: the-deployment spec: replicas: 3 selector: matchLabels: app: my-hello deployment: hello template: metadata: labels: app: my-hello deployment: hello spec: containers: 下面我们来部署多个Overlays来对应多个helloword订制版本 OVERLAYS=$DEMO_HOME/overlays mkdir -p $OVERLAYS/staging mkdir -p $OVERLAYS/production 创建staging Overlays 在staging目录中创建一个kustomization 文件，用来定义一个新的名称前缀和一些不同的 labels 。 cat $OVERLAYS/staging/kustomization.yaml namePrefix: staging- commonLabels: variant: staging org: acmeCorporation commonAnnotations: note: Hello, I am staging! resources: - ../../base patchesStrategicMerge: - map.yaml EOF 新增一个自定义的 configMap 将问候消息从 Good Morning! 改为 Have a pineapple! 。 同时，将 risky 标记设置为 true 。 cat $OVERLAYS/staging/map.yaml apiVersion: v1 kind: ConfigMap metadata: name: the-map data: altGreeting: \"Have a pineapple!\" enableRisky: \"true\" EOF 创建production Overlays 在 production 目录中创建一个 kustomization 文件，用来定义一个新的名称前缀和 labels 。 cat $OVERLAYS/production/kustomization.yaml namePrefix: production- commonLabels: variant: production org: acmeCorporation commonAnnotations: note: Hello, I am production! resources: - ../../base patchesStrategicMerge: - deployment.yaml EOF Production Patch，因为生产环境需要处理更多的流量，新建一个production patch来增加副本数。 cat $OVERLAYS/production/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: the-deployment spec: replicas: 5 EOF 比较overlays DEMO_HOME 现在包含： base目录：对拉取到的源配置进行了简单定制 overlays目录：包含在集群中创建不同 staging 和 production variants 的 kustomizations 和 patches 。 查看目录结构和差异： [root@VM-0-13-centos base]# tree $DEMO_HOME /tmp/tmp.w5Ic40K11n ├── base │ ├── configMap.yaml │ ├── deployment.yaml │ ├── kustomization.yaml │ ├── kustomization.yaml.bak │ └── service.yaml └── overlays ├── production │ ├── deployment.yaml │ └── kustomization.yaml └── staging ├── kustomization.yaml └── map.yaml 直接比较 staging 和 production 输出的不同： [root@VM-0-13-centos base]# diff \\ > more 3,4c3,4 altGreeting: Good Morning! > enableRisky: \"false\" 8c8 note: Hello, I am production! 12,13c12,13 variant: production > name: production-the-map 19c19 note: Hello, I am production! 23,24c23,24 variant: production > name: production-the-service 34c34 variant: production 41c41 note: Hello, I am production! 45,46c45,46 variant: production > name: production-the-deployment 48c48 replicas: 5 54c54 variant: production 58c58 note: Hello, I am production! 63c63 variant: production 75c75 部署不同的overlys 输出不同 overlys 的配置： [root@VM-0-13-centos base]# kustomize build $OVERLAYS/staging apiVersion: v1 data: altGreeting: Have a pineapple! enableRisky: \"true\" kind: ConfigMap metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-map --- apiVersion: v1 kind: Service metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello org: acmeCorporation variant: staging type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: annotations: note: Hello, I am staging! labels: app: my-hello org: acmeCorporation variant: staging name: staging-the-deployment spec: replicas: 3 selector: matchLabels: app: my-hello deployment: hello org: acmeCorporation variant: staging template: metadata: annotations: note: Hello, I am staging! labels: app: my-hello deployment: hello org: acmeCorporation variant: staging spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: staging-the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: staging-the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 [root@VM-0-13-centos base]# kustomize build $OVERLAYS/production apiVersion: v1 data: altGreeting: Good Morning! enableRisky: \"false\" kind: ConfigMap metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-map --- apiVersion: v1 kind: Service metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-service spec: ports: - port: 8666 protocol: TCP targetPort: 8080 selector: app: my-hello deployment: hello org: acmeCorporation variant: production type: LoadBalancer --- apiVersion: apps/v1 kind: Deployment metadata: annotations: note: Hello, I am production! labels: app: my-hello org: acmeCorporation variant: production name: production-the-deployment spec: replicas: 5 selector: matchLabels: app: my-hello deployment: hello org: acmeCorporation variant: production template: metadata: annotations: note: Hello, I am production! labels: app: my-hello deployment: hello org: acmeCorporation variant: production spec: containers: - command: - /hello - --port=8080 - --enableRiskyFeature=$(ENABLE_RISKY) env: - name: ALT_GREETING valueFrom: configMapKeyRef: key: altGreeting name: production-the-map - name: ENABLE_RISKY valueFrom: configMapKeyRef: key: enableRisky name: production-the-map image: monopole/hello:1 name: the-container ports: - containerPort: 8080 将上述命令传递给kubectl进行部署 kustomize build $OVERLAYS/staging |\\ kubectl apply -f - kustomize build $OVERLAYS/production |\\ kubectl apply -f - 也可直接使用kubectl部署，但是需要注意的是kubectl版本需要在v1.14.0以上 kubectl apply -k $OVERLAYS/staging kubectl apply -k $OVERLAYS/production © vishon all right reserved，powered by GitbookUpdated at 2021-06-04 12:58:17 "},"k8s/Use-scripts-to-perform-health-detection-on-pods.html":{"url":"k8s/Use-scripts-to-perform-health-detection-on-pods.html","title":"【k8s】用脚本对pod进行健康探测","keywords":"","body":"Kubernetes 支持对容器进行周期性探测，并根据探测结果判断容器的健康状态，执行额外的操作。 健康检查类别 健康检查分为以下类别： 容器存活检查：用于检测容器是否存活，类似于执行 ps 命令检查进程是否存在。如果容器的存活检查失败，集群会对该容器执行重启操作。如果容器的存活检查成功，则不执行任何操作。 容器就绪检查：用于检测容器是否准备好开始处理用户请求。例如，程序的启动时间较长时，需要加载磁盘数据或者要依赖外部的某个模块启动完成才能提供服务。此时，可通过容器就绪检查方式检查程序进程，确认程序是否启动完成。如果容器的就绪检查失败，集群会屏蔽请求访问该容器。如果容器的就绪检查成功，则会开放对该容器的访问。 健康检查方式 TCP 端口探测 TCP 端口探测的原理如下： 对于提供 TCP 通信服务的容器，集群周期性地对该容器建立 TCP 连接。如果连接成功，证明探测成功，否则探测失败。选择 TCP 端口探测方式，必须指定容器监听的端口。 例如，一个 redis 容器，它的服务端口是6379。我们对该容器配置了 TCP 端口探测，并指定探测端口为6379，那么集群会周期性地对该容器的6379端口发起 TCP 连接。如果连接成功，证明检查成功，否则检查失败。 HTTP 请求探测 HTTP 请求探测是针对于提供 HTTP/HTTPS 服务的容器，并集群周期性地对该容器发起 HTTP/HTTPS GET 请求。如果 HTTP/HTTPS response 返回码属于200 - 399范围，证明探测成功，否则探测失败。使用 HTTP 请求探测必须指定容器监听的端口和 HTTP/HTTPS 的请求路径。 例如，提供 HTTP 服务的容器，服务端口为 80，HTTP 检查路径为 /health-check，那么集群会周期性地对容器发起GET http://containerIP:80/health-check 请求。 执行命令检查 执行命令检查是一种强大的检查方式，该方式要求用户指定一个容器内的可执行命令，集群会周期性地在容器内执行该命令。如果命令的返回结果是0，检查成功，否则检查失败。 对于 TCP 端口探测 和 HTTP 请求探测，都可以通过执行命令检查的方式来替代： 对于 TCP 端口探测，可以写一个程序对容器的端口进行 connect。如果 connect 成功，脚本返回0，否则返回-1。 对于 HTTP 请求探测，可以写一个脚本来对容器进行 wget 并检查 response 的返回码。例如，wget http://127.0.0.1:80/health-check。如果返回码在200 - 399的范围，脚本返回0，否则返回 -1。 脚本进行健康探测 其实很多时候，我们对业务容器进行健康探测可能无法用简单的一条命令或者探测端口是否可达就判断业务是否正常，而是需要用较复杂的脚本的来实现，这样我们就需要来判断脚本的返回值了，很多人会直接在脚本输出0或者-1来进行判断，但是发现健康检查不生效。 其实这里不能用echo来输出对应的返回值，因为你用echo输出值，实际上执行脚本还是成功的，探针是获取的echo $?的值，这个值还是0，那么用脚本进行健康探测，需要怎么来写呢？ #!/bin/sh num=$1 if [ $num = 1 ]; then echo \"ok\" else echo \"fail\" exit 1 fi 这里我们如果想探测失败需要用exit 1，这样执行脚本获取的返回值就是1，探针就会认为这个是失败的。 下面我们把脚本放到镜像里面测试下，然后配置下探针，看下是否会生效，这里可以用这个镜像测试ccr.ccs.tencentyun.com/nwx_registry/health-check:latest 下面我们看下我们的deployment对应的健康检查配置 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: health-check qcloud-app: health-check name: health-check namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: health-check qcloud-app: health-check strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: k8s-app: health-check qcloud-app: health-check spec: containers: - image: ccr.ccs.tencentyun.com/nwx_registry/health-check:latest imagePullPolicy: Always livenessProbe: exec: command: - sh - /tmp/test.sh - \"2\" failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: health-check resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 根据我们配置的存活探针，这里检查脚本会返回1，也就是失败，连续探测3次失败后就会重启容器，我们验证下是不是这样的 Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 42s default-scheduler Successfully assigned tke-test/health-check-65b4d88575-sg7p9 to 10.0.0.157 Normal Started 40s kubelet, 10.0.0.157 Started container health-check Warning Unhealthy 2s (x3 over 22s) kubelet, 10.0.0.157 Liveness probe failed: fail Normal Killing 2s kubelet, 10.0.0.157 Container health-check failed liveness probe, will be restarted Normal Pulling 0s (x2 over 40s) kubelet, 10.0.0.157 Pulling image \"ccr.ccs.tencentyun.com/nwx_registry/health-check:latest\" Normal Pulled 0s (x2 over 40s) kubelet, 10.0.0.157 Successfully pulled image \"ccr.ccs.tencentyun.com/nwx_registry/health-check:latest\" Normal Created 0s (x2 over 40s) kubelet, 10.0.0.157 Created container health-check 从事件日志，可以发现，这里返回1，探针就认为是失败的，所以这里连续3次就重启容器。 综上所述，用探针来检查容器状态，如果用脚本进行判断，当失败时候，用exit 1来返回脚本返回结果。 © vishon all right reserved，powered by GitbookUpdated at 2021-08-04 16:58:13 "},"k8s/Batch-delete-ns-in-terminating-state-in-the-cluster.html":{"url":"k8s/Batch-delete-ns-in-terminating-state-in-the-cluster.html","title":"【k8s】批量删除集群内terminating状态的ns","keywords":"","body":"批量删除k8s集群terminating状态的ns 有的时候删除ns会卡主，处于terminating状态，这里为什么会卡主可以参考这个文档https://cloud.tencent.com/developer/article/1802531 如果集群存在多个terminating状态的ns，一个个删除比较麻烦，这里提供下一个简单的小脚本batch-delete-terminating-ns.sh来删除下，脚本内容如下 #!/bin/bash set -euxo pipefail if [ -f \"/etc/redhat-release\" ]; then yum install jq -y fi if [ -f \"/etc/lsb-release\" ]; then apt-get install jq -y fi kubectl proxy & PID=$! for i in `kubectl get ns | grep Terminating | awk -F \" \" '{print $1}'`; do kubectl get ns $i -o json | jq '.spec.finalizers=[]' > $i.json curl -X PUT http://localhost:8001/api/v1/namespaces/$i/finalize -H \"Content-Type: application/json\" --data-binary @${i}.json rm -rf $i.json done kill $PID © vishon all right reserved，powered by GitbookUpdated at 2021-12-13 13:43:52 "},"k8s/Modify-the-resources-configuration-of-the-container-under-the-namespace.html":{"url":"k8s/Modify-the-resources-configuration-of-the-container-under-the-namespace.html","title":"【k8s】批量修改命名空间下容器的resources配置","keywords":"","body":"修改命名空间下容器的resources配置 统一给单个命名空间下的容器配置resources，可以用下面脚本，具体的cpu和limit值设置，可以修改脚本，脚本内容如下： #!/bin/sh ns=$1 if [ $# = 0 ];then echo \"Run 'sh modify-workload-resorces.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Enter the namespace to be repaired Usage: sh modify-workload-resorces.sh [namespace]\" fi main(){ for i in `kubectl get deploy,sts -n $ns |awk -F ' ' '{print $1}' | grep -v NAME`; do for j in `kubectl get $i -n $ns -o=jsonpath='{.spec.template.spec.containers[*].name}'`; do kubectl set resources $i -c=$j -n $ns --requests=cpu=50m,memory=512Mi --limits=cpu=500m,memory=1000Mi done done } if [ $# = 1 ];then main fi © vishon all right reserved，powered by GitbookUpdated at 2021-08-25 18:25:18 "},"tke/tke-used-docker-in-docker.html":{"url":"tke/tke-used-docker-in-docker.html","title":"【tke】tke上使用docker-in-docker","keywords":"","body":"这里我们讲一下如何在tke集群上部署docker in docker的pod，这里前提条件是集群的runtime需要用的是docker类型，如果是containerd是不行的。 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: docker-in-docker qcloud-app: docker-in-docker name: docker-in-docker namespace: tke-test spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: docker-in-docker qcloud-app: docker-in-docker strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: docker-in-docker qcloud-app: docker-in-docker spec: containers: - command: - sleep - 70d image: docker:latest imagePullPolicy: Always name: docker-in-docker resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/docker.sock name: vol subPath: docker.sock dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - hostPath: path: /var/run type: DirectoryOrCreate name: vol 这里我们用官方提供的docker in docker镜像，镜像默认是没有常驻进程，需要加上sleep命令起一个常驻进程，然后我们将节点的/var/run/docker.sock挂载容器内。 接下来我们进入容器就可以执行docker命令了 [root@VM-0-13-centos ~]# kubectl exec -it docker-in-docker-5b49479696-cm6gd -n tke-test /bin/sh / # docker version Client: Version: 20.10.6 API version: 1.40 Go version: go1.13.15 Git commit: 370c289 Built: Fri Apr 9 22:42:10 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 19.03.9 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 9d988398e7 Built: Fri May 15 00:28:17 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.13 GitCommit: 7ad184331fa3e55e52b890ea95e65ba581ae3429 runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 / # © vishon all right reserved，powered by GitbookUpdated at 2021-05-21 20:55:29 "},"tke/tke-deploy-jumpserver.html":{"url":"tke/tke-deploy-jumpserver.html","title":"【tke】tke上部署Jumpserver跳板机","keywords":"","body":"本篇文章主要讲述如何在tke集群上部署Jumpserver跳板机，本次采用的1.18.4版本的集群。 部署mysql数据库 这里我们通过helm部署mysql数据库 helm install nwx-mysql stable/mysql --namespace mysql 这里我们需要获取下mysql的root用户数据库密码 kubectl get secret nwx-mysql -n mysql -o jsonpath={.data.mysql-root-password} |base64 -d 注意这里还需要给Jumpserver创建好数据库，登录mysql执行下面这条sql create database jumpserver default charset 'utf8'; 部署redis数据库 redis数据库我们也通过helm部署下，也部署在mysql命名空间 helm install nwx-redis bitnami/redis --namespace mysql 然后获取下redis数据库的密码 kubectl get secret nwx-redis -n mysql -o jsonpath={.data.redis-password} |base64 -d 部署Jumpserver 这里我们先在控制台创建一个jumpserver-datadir的pvc，使用了20G的云硬盘 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jumpserver-datadir namespace: jumpserver annotations: volume.beta.kubernetes.io/storage-provisioner: cloud.tencent.com/qcloud-cbs spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 然后生成下jumpserver需要到的SECRET_KEY和BOOTSTRAP_TOKEN # SECRET_KEY 生成方式： cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 50 # BOOTSTRAP_TOKEN生成方式： cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 16 apiVersion: apps/v1 kind: Deployment metadata: name: jumpserver namespace: jumpserver labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: replicas: 1 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate selector: matchLabels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver template: metadata: labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: containers: - env: - name: SECRET_KEY value: \"j9fKwmVV39DzqZ27tWnMffpLzP6TsvQkHCaxJRcKn\" - name: BOOTSTRAP_TOKEN value: \"nWZStpQ1UTO\" - name: DB_ENGINE value: \"mysql\" - name: DB_HOST value: \"nwx-mysql.mysql\" - name: DB_PORT value: \"3306\" - name: DB_USER value: \"root\" - name: \"DB_PASSWORD\" value: \"2hblVjr\" - name: DB_NAME value: \"jumpserver\" - name: REDIS_HOST value: \"nwx-redis-master.mysql\" - name: REDIS_PORT value: \"6379\" - name: REDIS_PASSWORD value: \"twsnty9\" image: jumpserver/jms_all:1.5.9 imagePullPolicy: IfNotPresent name: jumpserver ports: - containerPort: 80 name: http protocol: TCP - containerPort: 2222 name: ssh protocol: TCP volumeMounts: - mountPath: /opt/jumpserver/data/media name: datadir volumes: - name: datadir persistentVolumeClaim: claimName: jumpserver-datadir --- apiVersion: v1 kind: Service metadata: name: jumpserver namespace: jumpserver labels: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver spec: ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: ssh port: 2222 targetPort: 2222 protocol: TCP selector: app.kubernetes.io/instance: jumpserver app.kubernetes.io/name: jumpserver 创建ingress提供访问域名 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: ingress name: jumpserver-ingress namespace: jumpserver spec: rules: - host: jumpserver.tke.niewx.cn http: paths: - backend: serviceName: jumpserver servicePort: 80 path: / 然后再控制台输入访问域名 ，jumpserver默认的登录密码是admin/admin © vishon all right reserved，powered by GitbookUpdated at 2021-05-23 12:16:29 "},"tke/get-the-cbsId-of-pvc-in-the-tke-cluster.html":{"url":"tke/get-the-cbsId-of-pvc-in-the-tke-cluster.html","title":"【tke】获取pvc对应的cbs-id","keywords":"","body":"使用tke的过程中，有时候我们需要获取下pvc对应的cbs id，可以用下面get_pvc_cbs_id.sh脚本去快速获取 #!/bin/bash if [ $# = 0 ];then echo \"Use \"--h\" for more information about a given command.\" fi if [[ $1 = \"--h\" ]];then echo \" Please enter the pvc name for the first parameter, and empty the name for the second parameter Usage: sh get_pvc_cbs_id.sh pvc-name namespace\" fi ns=$2 pvc_name=$1 main(){ pv=`kubectl get pvc ${pvc_name} -n $ns -o jsonpath={.spec.volumeName}` cbs_id=`kubectl get pv ${pv} -o jsonpath={.spec.qcloudCbs.cbsDiskId}` echo $cbs_id } if [ $# = 2 ];then main fi 执行脚本，第一个参数输入pvc名称，第二个参数输入对应命名空间 [root@VM-0-13-centos script]# sh get_pvc_cbs_id.sh nwx-mysql mysql disk-xxxxx © vishon all right reserved，powered by GitbookUpdated at 2021-06-07 12:59:42 "},"tke/clean-the-evicted-state-pod-in-the-cluster.html":{"url":"tke/clean-the-evicted-state-pod-in-the-cluster.html","title":"【tke】清除集群中所有的evited状态pod","keywords":"","body":"有时候由于节点的内存或者磁盘使用率较高导致集群中产生了大量的evited状态pod，这些pod如果不手动删除，会一直存在集群中，这里我们提供了脚本clean-evicted-pod.sh来一键清理集群中的evited状态pod。 #!/bin/bash basepath=$(cd `dirname $0`; pwd) kubectl get pods -A | grep Evicted | awk -F \" \" '{print $1,$2}' >> $basepath/tmp.file while read line do ns=`echo $line | awk -F \" \" '{print $1}'` podname=`echo $line | awk -F \" \" '{print $2}'` kubectl delete pod $podname -n $ns done 复制上面脚本，直接执行即可，如果想清除其他状态的pod，可以将grep Evited改成其他的，比如说grep Pending © vishon all right reserved，powered by GitbookUpdated at 2021-08-14 13:40:33 "},"tke/adjust-workload-replicas-under-the-namespace.html":{"url":"tke/adjust-workload-replicas-under-the-namespace.html","title":"【tke】调整命名空间所有deploy和sts的副本数","keywords":"","body":"有时候我们为了测试我们的服务，不需要他一直运行着，当我们测试完，就希望将服务都停掉，避免占用集群资源，一般都是将负载的副本数设置为0，但是又希望对应的deployment还存在集群中，这样下次测试直接调整副本数就可以运行服务了。 下面提供了一个脚本scale-workload-replicas.sh可以调整命名空间的所有的deployment和statefulset的副本数量 #!/bin/sh ns=$1 replicas=$2 if [ $# = 0 ];then echo \"Run 'sh scale-workload-replicas.sh --h' for more information on a command.\" fi if [[ $1 = \"--h\" ]];then echo \"Please enter the first parameter enters the namespace, the second parameter enters the number of replicas Usage: sh scale-workload-replicas.sh [namespace] [replicas]\" fi main(){ for i in `kubectl get deploy,sts -n $ns |awk -F ' ' '{print $1}' | grep -v NAME`; do kubectl scale --replicas=$replicas -n $ns $i done } if [ $# = 2 ];then main fi 如果我们需要停掉这个命名空间下所有服务，则执行脚本第一个参数输入命名空间，第二个参数输入副本数0 [root@VM-0-13-centos script]# kubectl get pod -n mesh NAME READY STATUS RESTARTS AGE client-6fd64cfc6b-q9ml7 2/2 Running 0 4d23h nginx-5dbf784b68-bmr86 2/2 Running 0 16d nginx-v1-647887d8fd-qllgx 2/2 Running 0 39d nginx-v2-bcccc8f9f-jb926 2/2 Running 0 39d sleep-7f474bbcbd-tdgmf 2/2 Running 0 39d [root@VM-0-13-centos script]# sh scale-workload-replicas.sh mesh 0 deployment.apps/client scaled deployment.apps/nginx scaled deployment.apps/nginx-v1 scaled deployment.apps/nginx-v2 scaled deployment.apps/sleep scaled [root@VM-0-13-centos script]# kubectl get pod -n mesh No resources found. 如果需要拉起我们的服务，也只需要执行脚本，第一个参数输入命名空间，第二个参数输入副本数（输入1代表所有负载起一个副本） [root@VM-0-13-centos script]# sh scale-workload-replicas.sh mesh 1 deployment.apps/client scaled deployment.apps/nginx scaled deployment.apps/nginx-v1 scaled deployment.apps/nginx-v2 scaled deployment.apps/sleep scaled [root@VM-0-13-centos script]# kubectl get pod -n mesh NAME READY STATUS RESTARTS AGE client-6fd64cfc6b-ggz58 2/2 Running 0 23s nginx-5dbf784b68-c22hl 2/2 Running 0 22s nginx-v1-647887d8fd-n7bfh 2/2 Running 0 22s nginx-v2-bcccc8f9f-lnm6q 2/2 Running 0 22s sleep-7f474bbcbd-z2jj7 2/2 Running 0 22s © vishon all right reserved，powered by GitbookUpdated at 2021-08-25 17:58:30 "},"tke/modify-the-permissions-of-the-container-mount-directory.html":{"url":"tke/modify-the-permissions-of-the-container-mount-directory.html","title":"【tke】如何在非root用户启动的镜像中设置挂载目录权限","keywords":"","body":"我们在部署应用到k8s集群中，很多时候容器的启动用户不是root，并且会挂载数据目录到pvc上，但是又要通过启动用户写文件到挂载目录上，这样就会出现一个问题，就是启动用户没有权限读我们的数据挂载目录，今天我们提供一个init修改目录权限的方法来解决这个问题 首先我们部署一个grafana，并将grafana的数据存储目录/var/lib/grafana挂载到cbs上，这里pvc提前创建好了 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: grafana qcloud-app: grafana name: grafana namespace: monitor spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: grafana qcloud-app: grafana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: grafana qcloud-app: grafana spec: containers: - image: grafana/grafana:master imagePullPolicy: IfNotPresent name: grafana resources: limits: cpu: \"1\" memory: 2Gi requests: cpu: \"1\" memory: 1Gi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol subPath: grafana dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: vol persistentVolumeClaim: claimName: grafana 当我们启动pod之后会发现，pod一直在重启，查看日志报错是没有/var/lib/grafana这个目录的权限，因为grafana镜像的启动用户是grafana，而/var/lib/grafana这个目录，grafana是没有权限读写的。 [root@VM-0-13-centos ~]# kubectl logs -f grafana-84774b67d9-r79fv -n monitor GF_PATHS_DATA='/var/lib/grafana' is not writable. You may have issues with file permissions, more information here: http://docs.grafana.org/installation/docker/#migrate-to-v51-or-later mkdir: can't create directory '/var/lib/grafana/plugins': Permission denied 下面我们用一个init容器来修改/var/lib/grafana这个目录的权限， 再看下pod能否启动成功，这里重点关注init容器的配置 apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: grafana qcloud-app: grafana name: grafana namespace: monitor spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: grafana qcloud-app: grafana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: grafana qcloud-app: grafana spec: containers: - image: grafana/grafana:master imagePullPolicy: IfNotPresent name: grafana resources: limits: cpu: \"1\" memory: 2Gi requests: cpu: \"1\" memory: 1Gi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol subPath: grafana dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey initContainers: - args: - -c - mkdir -p /var/lib/grafana && chmod -R 777 /var/lib/grafana command: - /bin/sh image: centos:7 imagePullPolicy: Always name: init resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/grafana name: vol restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: vol persistentVolumeClaim: claimName: grafana 这里将上面的yaml部署后，查看pod是running，直接访问3000端口也是可以通的，说明grafana是可以正常启动的。 [root@VM-0-13-centos ~]# kubectl get pod -n monitor -o wide | grep grafana grafana-84dd4d4454-sp2xx 1/1 Running 0 2m4s 10.0.2.83 10.0.0.3 1/1 [root@VM-0-13-centos ~]# curl 10.0.2.83:3000 Found. 这里我们简要说下我们的解决方案，其实就是用一个init容器创建/var/lib/grafana这个目录，然后挂载到cbs卷上，并修改权限为777，init容器执行完之后，grafana再运行去读这个目录的权限就是777，即使grafana的启动用户不是root，也有权限读/var/lib/grafana这个目录，容器也就可以正常运行了 [root@VM-0-13-centos ~]# kubectl exec -it grafana-84dd4d4454-sp2xx bash Error from server (NotFound): pods \"grafana-84dd4d4454-sp2xx\" not found [root@VM-0-13-centos ~]# kubectl exec -it grafana-84dd4d4454-sp2xx bash -n monitor bash-5.0$ cd var/lib/ bash-5.0$ ls -al total 24 drwxr-xr-x 1 root root 4096 Nov 26 2020 . drwxr-xr-x 1 root root 4096 Oct 21 2020 .. drwxr-xr-x 2 root root 4096 Oct 21 2020 apk drwxrwxrwx 4 root root 4096 Jun 13 12:03 grafana drwxr-xr-x 2 root root 4096 Oct 21 2020 misc drwxr-xr-x 2 root root 4096 Oct 21 2020 udhcpd © vishon all right reserved，powered by GitbookUpdated at 2021-06-13 20:10:32 "},"tke/The-domain-name-resolved-in-the-alpine-container-exceeds-5s.html":{"url":"tke/The-domain-name-resolved-in-the-alpine-container-exceeds-5s.html","title":"【tke】alpine镜像内解析域名超5s","keywords":"","body":"问题现象 tke集群的pod访问某个外部域名发现很慢，超5s以上，访问其他域名不会有问题，在节点访问是正常，而且不是所有的容器都有问题，这里到底是怎么回事呢？ 排查思路 首先只在某个pod出现这个问题，说明不是网络或者dns有问题，后面发现出现问题的都是alphie系统，为什么alphine会出现这个问题，这里我分别在centos和alpine抓包，看下导致耗时在哪里 抓包测试 首先我们在alpine镜像中测试访问域名，然后抓包 bash-4.4# time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}5.073069 real 0m5.278s user 0m0.017s sys 0m0.003s bash-4.4# tcpdump -i any port 53 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes 09:13:49.074130 IP go-test-5f78b5569f-r2q9v.59071 > kube-dns.kube-system.svc.cluster.local.53: 47843+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:13:49.074164 IP go-test-5f78b5569f-r2q9v.59071 > kube-dns.kube-system.svc.cluster.local.53: 48334+ AAAA? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:13:49.074257 IP go-test-5f78b5569f-r2q9v.44323 > kube-dns.kube-system.svc.cluster.local.53: 52788+ PTR? 140.52.16.172.in-addr.arpa. (44) 09:13:49.074596 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.59071: 47843 NXDomain*- 0/1/0 (155) 09:13:49.074660 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.59071: 48334 NXDomain*- 0/1/0 (155) 09:13:49.074710 IP go-test-5f78b5569f-r2q9v.42646 > kube-dns.kube-system.svc.cluster.local.53: 51281+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:13:49.074744 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.44323: 52788*- 1/0/0 PTR kube-dns.kube-system.svc.cluster.local. (122) 09:13:49.075046 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.42646: 51682 NXDomain*- 0/1/0 (146) 09:13:49.075115 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.42646: 51281 NXDomain*- 0/1/0 (146) 09:13:49.075191 IP go-test-5f78b5569f-r2q9v.35865 > kube-dns.kube-system.svc.cluster.local.53: 61054+ A? elink.spic.com.cn.cluster.local. (49) 09:13:49.075242 IP go-test-5f78b5569f-r2q9v.35865 > kube-dns.kube-system.svc.cluster.local.53: 61474+ AAAA? elink.spic.com.cn.cluster.local. (49) 09:13:49.075419 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.35865: 61054 NXDomain*- 0/1/0 (142) 09:13:49.075477 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.35865: 61474 NXDomain*- 0/1/0 (142) 09:13:49.075535 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26687+ A? elink.spic.com.cn. (35) 09:13:49.075607 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:49.085340 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26687 1/0/0 A 39.155.244.138 (68) 09:13:50.761856 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail 0/0/0 (35) 09:13:50.761896 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762014 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762043 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762114 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762149 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762247 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:50.762280 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:50.762341 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576211 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576403 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576442 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576660 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576687 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.576851 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.576886 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.577039 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 09:13:51.577073 IP go-test-5f78b5569f-r2q9v.56020 > kube-dns.kube-system.svc.cluster.local.53: 26917+ AAAA? elink.spic.com.cn. (35) 09:13:51.577153 IP kube-dns.kube-system.svc.cluster.local.53 > go-test-5f78b5569f-r2q9v.56020: 26917 ServFail* 0/0/0 (35) 从上面发现，在alphine镜像会多次解析AAAA记录，造成netfilter race，详细的分析可以参考这个https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/ 我们又在对应的centos上测试了下，发现每次轮询dns server只会出现一个AAAA记录，时间比起alpine会短很多 [root@centos-684c48fccd-k24tp /]# time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}1.510 real 0m1.836s user 0m0.035s sys 0m0.075s 09:17:13.970833 IP centos-684c48fccd-k24tp.41304 > kube-dns.kube-system.svc.cluster.local.domain: 7375+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:17:13.970870 IP centos-684c48fccd-k24tp.41304 > kube-dns.kube-system.svc.cluster.local.domain: 38125+ AAAA? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:17:13.971520 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.41304: 38125 NXDomain*- 0/1/0 (155) 09:17:13.971567 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.41304: 7375 NXDomain*- 0/1/0 (155) 09:17:13.971625 IP centos-684c48fccd-k24tp.45222 > kube-dns.kube-system.svc.cluster.local.domain: 65110+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:17:13.971669 IP centos-684c48fccd-k24tp.45222 > kube-dns.kube-system.svc.cluster.local.domain: 3169+ AAAA? elink.spic.com.cn.svc.cluster.local. (53) 09:17:13.971996 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.45222: 65110 NXDomain*- 0/1/0 (146) 09:17:13.972018 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.45222: 3169 NXDomain*- 0/1/0 (146) 09:17:13.972044 IP centos-684c48fccd-k24tp.36358 > kube-dns.kube-system.svc.cluster.local.domain: 23192+ A? elink.spic.com.cn.cluster.local. (49) 09:17:13.972069 IP centos-684c48fccd-k24tp.36358 > kube-dns.kube-system.svc.cluster.local.domain: 34973+ AAAA? elink.spic.com.cn.cluster.local. (49) 09:17:13.972427 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36358: 23192 NXDomain*- 0/1/0 (142) 09:17:13.972658 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36358: 34973 NXDomain*- 0/1/0 (142) 09:17:13.972698 IP centos-684c48fccd-k24tp.36416 > kube-dns.kube-system.svc.cluster.local.domain: 11372+ A? elink.spic.com.cn. (35) 09:17:13.972728 IP centos-684c48fccd-k24tp.36416 > kube-dns.kube-system.svc.cluster.local.domain: 3185+ AAAA? elink.spic.com.cn. (35) 09:17:13.973772 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36416: 11372 1/0/0 A 39.155.244.138 (68) 09:17:15.218386 IP kube-dns.kube-system.svc.cluster.local.domain > centos-684c48fccd-k24tp.36416: 3185 ServFail 0/0/0 (35) 这边用优化后的alphine镜像geekidea/alpine-a:3.7，该镜像移除了AAAA记录，然后测试发现没有AAAA解析记录后会非常快 / # time curl -w \"%{time_namelookup}\" --location --request POST 'https://elink.spic.com.cn/' {\"errcode\":40014,\"errmsg\":\"invalid access_token [logid:]\"}0.003296 real 0m 0.17s user 0m 0.01s sys 0m 0.00s 09:12:07.919297 IP alpine-a-5684fcc8c7-mnfhp.37258 > kube-dns.kube-system.svc.cluster.local.53: 11480+ A? elink.spic.com.cn.tke-test.svc.cluster.local. (62) 09:12:07.919630 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.37258: 11480 NXDomain*- 0/1/0 (155) 09:12:07.919686 IP alpine-a-5684fcc8c7-mnfhp.59507 > kube-dns.kube-system.svc.cluster.local.53: 18200+ A? elink.spic.com.cn.svc.cluster.local. (53) 09:12:07.920076 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.59507: 18200 NXDomain*- 0/1/0 (146) 09:12:07.920119 IP alpine-a-5684fcc8c7-mnfhp.33037 > kube-dns.kube-system.svc.cluster.local.53: 63110+ A? elink.spic.com.cn.cluster.local. (49) 09:12:07.920426 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.33037: 63110 NXDomain*- 0/1/0 (142) 09:12:07.920508 IP alpine-a-5684fcc8c7-mnfhp.58282 > kube-dns.kube-system.svc.cluster.local.53: 51705+ A? elink.spic.com.cn. (35) 09:12:07.921322 IP kube-dns.kube-system.svc.cluster.local.53 > alpine-a-5684fcc8c7-mnfhp.58282: 51705 1/0/0 A 106.38.29.46 (68) 结论 这里在容器内解析域名需要5s以上是因为基础镜像的操作系统导致，主要看镜像内对AAAA记录解析次数，像alpine默认会对域名解析多次AAAA记录。 参考文档 https://openforum.hand-china.com/t/topic/1111 © vishon all right reserved，powered by GitbookUpdated at 2021-06-21 13:57:08 "},"tke/TKE-redeploys-cert-manager-and-keeps-getting-stuck.html":{"url":"tke/TKE-redeploys-cert-manager-and-keeps-getting-stuck.html","title":"【tke】TKE重新部署cert-manager一直卡主","keywords":"","body":"最近研究了一下在tke上部署cert-manager来实现为ingress免费签发域名证书，具体的部署文档可以参考https://www.niewx.cn/cert-manager/2021/08/07/cert-manager-issues-free-certificates-for-domain-names/ 这里为了测试，我先部署了cert-manager，然后又删除了，第二次部署的时候发现一直卡在创建crd上，导致一直部署不成功，具体现象如下。 [root@VM-0-13-centos ~]# kubectl apply --validate=false -f https://raw.githubusercontent.com/TencentCloudContainerTeam/manifest/master/cert-manager/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created ^C 那么这里是怎么回事呢？既然创建crd一直卡在，那么肯定是创建crd的过程哪里除了问题，当时想的是不是之前创建的crd资源没有清理干净导致的呢？ 首先我查下和cert-manager相关的crd资源 [root@VM-0-13-centos ~]# kubectl get crd | grep cert certificaterequests.cert-manager.io 2021-08-09T10:55:18Z certificates.cert-manager.io 2021-08-09T10:55:18Z challenges.acme.cert-manager.io 2021-06-02T06:45:43Z 这里我们清理下未删除的crd，当我们删除challenges.acme.cert-manager.io 这个crd一直无法删除成功，那么问题就明朗了，就是crd无法删除，导致创建crd一直卡主，我这边尝试强制删除下 [root@VM-0-13-centos ~]# kubectl delete crd challenges.acme.cert-manager.io --grace-period=0 --force warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. customresourcedefinition.apiextensions.k8s.io \"challenges.acme.cert-manager.io\" force deleted [root@VM-0-13-centos ~]# kubectl get crd challenges.acme.cert-manager.io NAME CREATED AT challenges.acme.cert-manager.io 2021-06-02T06:45:43Z 强制删除后发现crd资源还存在，这里删除失败了，那么要清楚这种删除不掉的crd资源呢？ [root@VM-0-13-centos ~]# kubectl patch crd challenges.acme.cert-manager.io -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io patched [root@VM-0-13-centos ~]# kubectl delete crd challenges.acme.cert-manager.io --grace-period=0 --force warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. Error from server (NotFound): customresourcedefinitions.apiextensions.k8s.io \"challenges.acme.cert-manager.io\" not found 执行上面命令，先将finalizers字段配置信息删除，然后强制删除即可，删除了相关crd后，我们再部署下 [root@VM-0-13-centos ~]# kubectl apply --validate=false -f https://raw.githubusercontent.com/TencentCloudContainerTeam/manifest/master/cert-manager/cert-manager.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-view unchanged clusterrole.rbac.authorization.k8s.io/cert-manager-edit unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges unchanged clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim unchanged role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged role.rbac.authorization.k8s.io/cert-manager:leaderelection unchanged role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection unchanged rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection configured rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook configured 再次部署就可以成功了。 总结 当我们在集群重新部署crd资源卡主的时候，可以搜下集群是不是存在上次未删除的crd资源，然后清理下，如强制删也无法删除，用patch修改下finalizers字段后再删除，清理成功后，再部署即可。 © vishon all right reserved，powered by GitbookUpdated at 2021-08-09 20:52:58 "},"cloud/aws/awscli-command-operation.html":{"url":"cloud/aws/awscli-command-operation.html","title":"【aws】awscli命令行操作","keywords":"","body":"aws cli命令行操作 首先获取账号的access Access key ID,Secret access key AKIATRxxxxxxxxJX5WL,ZP2enHUH3XegoGhZxxxxxxxxfDMZo43ohz 登录ec2的实例下载awscli pip安装awscli ssh -i xxxxx.pem ec2-user@10.0.64.11 sudo -i yum -y install python-pip pip install awscli 2进制包安装awscli curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install 登录awscli [root@ip-10-0-64-11 ~]# aws configure AWS Access Key ID [None]: AKIATRMLTGCFIY5WRHGG AWS Secret Access Key [None]: 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 Default region name [None]: cn-northwest-1 Default output format [None]: 可以不输入 从S3桶下载对应的文件 aws s3 ls aws s3 cp s3://xxxx/jobs.tgz /root 其他的awscli命令 EC2 挂载 EBS linux 查看块设备： lsblk 格式化磁盘： sudo mkfs -t ext4 /dev/xvdb 挂载卷： sudo mount /dev/xvdb /mnt/mydir 卸载卷： sudo umount /dev/xvdb windows diskpart san policy=onlineall list disk disk yourdiskid attributes disk clear readonly online disk ec2操作 aws ec2 describe-instances aws ec2 describe-instances --instance-ids \"instanceid1\" \"instanceid2\" aws ec2 start-instances --instance-ids \"instanceid1\" \"instanceid2\" aws ec2 stop-intances --instance-ids \"instanceid1\" \"instanceid2\" aws ec2 run-instances --image-id ami-b6b62b8f --security-group-ids sg-xxxxxxxx --key-name mytestkey --block-device-mappings \"[{\\\"DeviceName\\\": \\\"/dev/sdh\\\",\\\"Ebs\\\":{\\\"VolumeSize\\\":100}}]\" --instance-type t2.medium --count 1 --subnet-id subnet-e8330c9c --associate-public-ip-address (Note: 若不指定subnet-id则会在默认vpc中去选，此时若指定了非默认vpc的安全组会出现请求错误。如无特殊要求，建议安全组和子网都不指定，就不会出现这种问题。) 查看region与AZ aws ec2 describe-region aws ec2 describe-availability-zones --region region-name 查看ami aws ec2 describe-images key-pair aws ec2 create-key-pair --key-name mykeyname 安全组 aws ec2 create-security-group --group-name mygroupname --description mydescription --vpc-id vpc-id (若不指定vpc，则在默认vpc中创建安全组) aws ec2 authorize-security-group-ingress --group-id sg-xxxxyyyy --protocol tcp --port 22 --cidr 0.0.0.0/0 aws ec2 authorize-security-group-ingress --group-id sg-xxxxyyyy --protocol tcp --port 9999 --source-group sg-xxxxxxxx AutoScaling 列出AS组 aws autoscaling describe-auto-scaling-groups 列出AS实例 aws autoscaling describe-auto-scaling-instances --instance-ids [instance-id-1 instance-id-2 ...] 从组中分离实例 aws autoscaling detach-instances --auto-scaling-group-name myasgroup --instance-ids instanceid1 instanceid2 [--should-decrement-desired-capacity|--no-should-decrement-desired-capacity] 附加实例到组 aws autoscaling attach-instances --auto-scaling-group-name myasgroup --instance-ids instanceid1 instanceid2 挂起AS流程 aws autoscaling suspend-process --auto-scaling-group-name myasgroup --scaling-processes AZRebalance|AlarmNotification|... 删除AS组 aws autoscaling delete-auto-scaling-group --auto-scaling-group-name myasgroup S3 查看 aws s3 ls aws s3 ls s3://bucket aws s3 ls s3://bucket/prefix 拷贝 aws s3 cp /to/local/path s3://bucket/prefix aws s3 cp s3://bucket/prefix /to/local/path aws s3 cp s3://bucket1/prefix1 s3://bucket2/prefix2 同步 aws sync [--delete] /to/local/dir s3://bucket/prefixdir aws sync [--delete] s3://bucket/prefixdir /to/local/dir aws sync [--delete] s3://bucket1/prefixdir1 s3://bucket2/prefixdir2 手动分片上传 文件分片 split -b 40m myfile myfile-part- 创建分片上传任务 aws s3api create-multipart-upload --bucket bucketname --key prefix 记录返回值 { \"Bucket\": \"bucketname\", \"UploadId\": \"uploadeid\", \"Key\": \"prefix\" } 上传分片 aws s3api upload-part --bucket bucketname --key prefix --part-number [分片上传编号(e.g. 1,2,3...)] --body myfile-[x] --upload-id uploadid 列出已上传分片，创建分片结构文件 aws s3api list-parts --bucket bucketname --key prefix --upload-id uploadid 将上命令结果中的parts部分保存为 temp 文件 {\"Parts\": [ { \"PartNumber\": 1, \"ETag\": \"\\\"xxxxxxx\\\"\" }, { \"PartNumber\": 2, \"ETag\": \"\\\"xxxxxxxx\\\"\" } ] } 结束分片上传任务 aws s3api complete-multipart-upload --multipart-upload file://temp --bucket bucketname --key prefix --upload-id uploadid AWSCLI访问阿里云OSS aws configure --p aliyun #设置key与secret,其他默认 aws configure set s3.addressing_style virtual --p aliyun aws s3 ls --endpoint-url [url/(e.g. http://oss-cn-hangzhou.aliyuncs.com)] --p aliyun IAM Role操作 aws iam create-role MY-ROLE-NAME --assum-role-policy-document file://path/to/trustpolicy.json aws iam put-role-policy --role-name MY-ROLE-NAME --policy-name MY-PERM-POLICY --policy-document file://path/to/permissionpolicy.json aws iam create-instance-profile --instance-profile-name MY-INSTANCE-PROFILE aws iam add-role-to-instance-profile --instance-profile-name MY-INSTANCE-PROFILE --role-name MY-ROLE-NAME AUTO-SCALING 查看信息 aws autoscaling describe-auto-scaling-groups aws autoscaling describe-auto-scaling-instances kinesis 创建流 aws kinesis create-stream –stream-name mystream –shard-count 列出流 aws kinesis list-streams 获取指定流的分片迭代器 aws kinesis get-shard-iterator –stream-name mystream –shard-id shard-1 –shard-iterator-type TRIM_HORIZON 发送数据到流 aws kinesis put-record –stream-name mystream –partition-key mykey –data test 获取流数据 aws kinesis get-records –shard-iterator myiterator © vishon all right reserved，powered by GitbookUpdated at 2021-05-22 11:05:01 "},"prometheus/basic-knowledge-of-prometheus.html":{"url":"prometheus/basic-knowledge-of-prometheus.html","title":"【prometheus】prometheus入门知识概念","keywords":"","body":"prometheus简介 Prometheus是一个开源监控系统，它前身是SoundCloud的警告工具包。从2012年开始，许多公司和组织开始使用Prometheus。该项目的开发人员和用户社区非常活跃，越来越多的开发人员和用户参与到该项目中。目前它是一个独立的开源项目，且不依赖与任何公司。 为了强调这点和明确该项目治理结构，Prometheus在2016年继Kurberntes之后，加入了Cloud Native Computing Foundation。 特征 Prometheus的主要特征有： 多维度数据模型 灵活的查询语言 不依赖分布式存储，单个服务器节点是自主的 以HTTP方式，通过pull模型拉去时间序列数据 也通过中间网关支持push模型 通过服务发现或者静态配置，来发现目标服务对象 支持多种多样的图表和界面展示，grafana也支持它 组件 Prometheus生态包括了很多组件，它们中的一些是可选的： 主服务Prometheus Server负责抓取和存储时间序列数据 客户库负责检测应用程序代码 支持短生命周期的PUSH网关 基于Rails/SQL仪表盘构建器的GUI 多种导出工具，可以支持Prometheus存储数据转化为HAProxy、StatsD、Graphite等工具所需要的数据存储格式 警告管理器 命令行查询工具 其他各种支撑工具 多数Prometheus组件是Go语言写的，这使得这些组件很容易编译和部署。 架构 下面这张图说明了Prometheus的整体架构，以及生态中的一些组件作用: Prometheus服务，可以直接通过目标拉取数据，或者间接地通过中间网关拉取数据。它在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中，PromQL和其他API可视化地展示收集的数据 适用场景 Prometheus在记录纯数字时间序列方面表现非常好。它既适用于面向服务器等硬件指标的监控，也适用于高动态的面向服务架构的监控。对于现在流行的微服务，Prometheus的多维度数据收集和数据筛选查询语言也是非常的强大。 Prometheus是为服务的可靠性而设计的，当服务出现故障时，它可以使你快速定位和诊断问题。它的搭建过程对硬件和服务没有很强的依赖关系。 不适用场景 Prometheus，它的价值在于可靠性，甚至在很恶劣的环境下，你都可以随时访问它和查看系统服务各种指标的统计信息。 如果你对统计数据需要100%的精确，它并不适用，例如：它不适用于实时计费系统 基本概念 job: Prometheus的采集任务由配置文件中一个个的Job组成，一个Job里包含该Job下的所有监控目标的公共配置，比如使用哪种服务发现去获取监控目标，比如抓取时使用的证书配置，请求参数配置等等。 target: 一个监控目标就是一个target，一个job通过服务发现会得到多个需要监控的target，其包含一些label用于描述target的一些属性。 relabel_configs: 每个job都可以配置一个或多个relabel_config，relabel_config会对target的label集合进行处理，可以根据label过滤一些target或者修改，增加，删除一些label。relabel_config过程发生在target开始进行采集之前,针对的是通过服务发现得到的label集合。 metrics_relabel_configs：每个job还可以配置一个或者多个metrics_relabel_config，其配置方式和relabel_configs一模一样，但是其用于处理的是从target采集到的数据中的label。也就是发送在采集之后。 series(序列)：一个series就是指标名+label集合。 head series：Prometheus会将近2小时的series缓存在内测中，称为head series。 指标类型 Prometheus定义了4中不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） Counter: 一种累加的metric，如请求的个数，结束的任务数，出现的错误数等 Gauge: 常规的metric,如温度，可任意加减。其为瞬时的，与时间没有关系的，可以任意变化的数据。 Histogram: 柱状图，用于观察结果采样，分组及统计，如：请求持续时间，响应大小。其主要用于表示一段时间内对数据的采样，并能够对其指定区间及总数进行统计。根据统计区间计算 Summary: 类似Histogram，用于表示一段时间内数据采样结果，其直接存储quantile数据，而不是根据统计区间计算出来的。不需要计算，直接存储结果 promql PromQL (Prometheus Query Language) 是 Prometheus 自己开发的数据查询 DSL 语言。 结果类型 查询结果类型： 瞬时数据 (Instant vector): 包含一组时序，每个时序只有一个点，例如：http_requests_total 区间数据 (Range vector): 包含一组时序，每个时序有多个点，例如：http_requests_total[5m] 纯量数据 (Scalar): 纯量只有一个数字，没有时序，例如：count(http_requests_total) 匹配模式 PromQL支持使用=和!=两种完全匹配模式： 通过使用label=value可以选择那些标签满足表达式定义的时间序列； 反之使用label!=value则可以根据标签匹配排除时间序列； 除了使用完全匹配的方式对时间序列进行过滤以外，PromQL还可以支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离： 使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除 时间位移 如果我们想查询，5分钟前的瞬时样本数据，或昨天一天的区间内的样本数据呢? 这个时候我们就可以使用位移操作，位移操作的关键字为offset。 可以使用offset时间位移操作： http_request_total{} offset 5m http_request_total{}[1d] offset 1d 数学运算符 PromQL支持的所有数学运算符如下所示： + (加法) - (减法) * (乘法) / (除法) % (求余) ^ (幂运算) node_memory_free_bytes_total / (1024 * 1024) 布尔运算符 Prometheus支持以下布尔运算符如下： == (相等) != (不相等) > (大于) >= (大于等于) (node_memory_bytes_total - node_memory_free_bytes_total) / node_memory_bytes_total > 0.95 布尔运算符的默认行为是对时序数据进行过滤。而在其它的情况下我们可能需要的是真正的布尔结果。例如，只需要知道当前模块的HTTP请求量是否>=1000，如果大于等于1000则返回1（true）否则返回0（false）。这时可以使用bool修饰符改变布尔运算的默认行为 2 == bool 2 # 结果为1 集合运算符 使用瞬时向量表达式能够获取到一个包含多个时间序列的集合，我们称为瞬时向量。 通过集合运算，可以在两个瞬时向量与瞬时向量之间进行相应的集合操作。目前，Prometheus支持以下集合运算符： and (并且) or (或者) unless (排除) vector1 and vector2 会产生一个由vector1的元素组成的新的向量。该向量包含vector1中完全匹配vector2中的元素组成。 vector1 or vector2 会产生一个新的向量，该向量包含vector1中所有的\b样本数据，以及vector2中没有与vector1匹配到的样本数据。 vector1 unless vector2 会产生一个新的向量，新向量中的元素由vector1中没有与vector2匹配的元素组成。 聚合操作符 sum (在维度上求和) max (在维度上求最大值) min (在维度上求最小值) avg (在维度上求平均值) stddev (求标准差) stdvar (求方差) count (统计向量元素的个数) count_values (统计相同数据值的元素数量) bottomk (样本值第k个最小值) topk (样本值第k个最大值) quantile (统计分位数) 这些操作符被用于聚合所有标签维度，或者通过without或者by子句来保留不同的维度。 ([parameter,] ) [without | by ()] [keep_common] parameter只能用于count_values, quantile, topk和bottomk。without移除结果向量中的标签集合，其他标签被保留输出。by关键字的作用正好相反，即使它们的标签值在向量的所有元素之间。keep_common子句允许保留额外的标签（在元素之间相同，但不在by子句中的标签） count_values对每个唯一的样本值输出一个时间序列。每个时间序列都附加一个标签。这个标签的名字有聚合参数指定，同时这个标签值是唯一的样本值。每一个时间序列值是结果样本值出现的次数。 topk和bottomk与其他输入样本子集聚合不同，返回的结果中包括原始标签。by和without仅仅用在输入向量的桶中 如果度量指标名称http_requests_total包含由group, application, instance的标签组成的时间序列数据，我们可以通过以下方式计算去除instance标签的http请求总数： sum(http_requests_total) without (instance) 如果我们对所有应用程序的http请求总数，我们可以简单地写下： sum(http_requests_total) 统计每个编译版本的二进制文件数量，我们可以如下写： count_values(\"version\", build_version) 通过所有实例，获取http请求第5个最大值，我们可以简单地写下： topk(5, http_requests_total) 函数 函数一般使用较少，具体参考官方文档https://prometheus.io/docs/prometheus/latest/querying/functions/ 向量匹配 向量之间的匹配是指右边向量中的每一个元素，在左边向量中也存在。这里有两种基本匹配行为特征： 一对一，找到这个操作符的两边向量元素的相同元素。默认情况下，操作符的格式是vector1 [operate] vector2。如果它们有相同的标签和值，则表示相匹配。ingoring关键字是指，向量匹配时，可以忽略指定标签。on关键字是指，在指定标签上进行匹配。格式如下所示： [vector expr] [bin-op] ignoring([label list]) [vector expr] [vector expr] [bin-op] on([lable list]) [vector expr] 例如样本数据： method_code:http_errors:rate5m{method=\"get\", code=\"500\"} 24 method_code:http_errors:rate5m{method=\"get\", code=\"404\"} 30 method_code:http_errors:rate5m{method=\"put\", code=\"501\"} 3 method_code:http_errors:rate5m{method=\"post\", code=\"404\"} 21 method:http_requests:rate5m{method=\"get\"} 600 method:http_requests:rate5m{method=\"delete\"} 34 method:http_requests:rate5m{method=\"post\"} 120 查询例子： method_code:http_errors:rate5m{code=\"500\"} / ignoring(code) method:http_requests:rate5m 两个向量之间的除法操作运算的向量结果是，每一个向量样本http请求方法标签值是500，且在过去5分钟的运算值。如果没有忽略code=\"500\"的标签，这里不能匹配到向量样本数据。两个向量的请求方法是put和delete的样本数据不会出现在结果列表中 {method=\"get\"} 0.04 // 24 / 600 {method=\"post\"} 0.05 // 6 / 120 多对一和一对多的匹配，是指向量元素中的一个样本数据匹配标签到了多个样本数据标签。这里必须直接指定两个修饰符group_left或者group_right， 左或者右决定了哪边的向量具有较高的子集。 ignoring() group_left() ignoring() group_right() on() group_left() on() group_right() 这个group带标签的修饰符标签列表包含了“一对多”中的“一”一侧的额外标签。对于on标签只能是这些列表中的一个。结果向量中的每一个时间序列数据都是唯一的。 group修饰符只能被用在比较操作符和算术运算符。 method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 在这个例子中，左向量的标签数量多于左边向量的标签数量，所以我们使用group_left。右边向量的时间序列元素匹配左边的所有相同method标签: {method=\"get\", code=\"500\"} 0.04 // 24 /600 {method=\"get\", code=\"404\"} 0.05 // 30 /600 {method=\"post\", code=\"500\"} 0.05 // 6 /600 {method=\"post\", code=\"404\"} 0.175 // 21 /600 多对一和一对多匹配应该更多地被谨慎使用。经常使用ignoring(\\)输出想要的结果。 api访问prometheus k8s集群内我们可以通过kubectl get --raw / 获取根下的接口，然后层层下探，直到找到需要的监控数据。 pod：kubectl get --raw /api/v1/nodes/10.168.1.4/proxy/metrics/cadvisor node: kubectl get --raw /api/v1/nodes/10.168.1.4:9100/proxy/metrics kubelet: kubectl get --raw /api/v1/nodes/10.168.1.4:10250/proxy/metrics 用curl请求prometheus的地址来进行查询，查询所有up的job curl http://10.0.0.234:9090/api/v1/query?query=up http api来查询prometheus可以参考官方文档https://prometheus.io/docs/prometheus/latest/querying/api/ © vishon all right reserved，powered by GitbookUpdated at 2021-09-20 21:05:23 "},"prometheus/Prometheus-collects-Docker-Engine-Metrics.html":{"url":"prometheus/Prometheus-collects-Docker-Engine-Metrics.html","title":"【prometheus】Prometheus采集Docker Engine Metrics","keywords":"","body":"Prometheus采集Docker Engine Metrics 从Docker 1.13开始引入了一个特性：将Docker Engine Metrix以Prometheus语法暴露出来，使得Prometheus服务器可以收集并展示。 开启节点docker mertrics 节点的docker的daemon文件中加上如下配置，就可以开启 { \"metrics-addr\" : \"0.0.0.0:9323\", \"experimental\" : true } prometheus配置Rawjob 配置rawjob采集数据到prometheus scrape_configs: - job_name: docker-daemon honor_timestamps: true metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: node relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - source_labels: [__address__] separator: ; regex: ([^:;]+):(\\d+) target_label: __address__ replacement: ${1}:9323 action: replace 查看指标 通过接口查看数据指标，根据指标编写promsql进行数据的展示和聚合 [root@VM-0-2-centos ~]# curl http://10.0.0.2:9323/metrics # HELP builder_builds_failed_total Number of failed image builds # TYPE builder_builds_failed_total counter builder_builds_failed_total{reason=\"build_canceled\"} 0 builder_builds_failed_total{reason=\"build_target_not_reachable_error\"} 0 builder_builds_failed_total{reason=\"command_not_supported_error\"} 0 builder_builds_failed_total{reason=\"dockerfile_empty_error\"} 0 builder_builds_failed_total{reason=\"dockerfile_syntax_error\"} 0 builder_builds_failed_total{reason=\"error_processing_commands_error\"} 0 builder_builds_failed_total{reason=\"missing_onbuild_arguments_error\"} 0 builder_builds_failed_total{reason=\"unknown_instruction_error\"} 0 # HELP builder_builds_triggered_total Number of triggered image builds # TYPE builder_builds_triggered_total counter builder_builds_triggered_total 0 # HELP engine_daemon_container_actions_seconds The number of seconds it takes to process each container action # TYPE engine_daemon_container_actions_seconds histogram engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"changes\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"changes\"} 0 engine_daemon_container_actions_seconds_count{action=\"changes\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"commit\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"commit\"} 0 engine_daemon_container_actions_seconds_count{action=\"commit\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.1\"} 2 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.25\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"0.5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"1\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"2.5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"5\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"10\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"create\",le=\"+Inf\"} 5 engine_daemon_container_actions_seconds_sum{action=\"create\"} 0.430104514 engine_daemon_container_actions_seconds_count{action=\"create\"} 5 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.025\"} 3 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.05\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.1\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.25\"} 6 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"0.5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"1\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"2.5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"5\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"10\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"delete\",le=\"+Inf\"} 7 engine_daemon_container_actions_seconds_sum{action=\"delete\"} 0.45672375400000004 engine_daemon_container_actions_seconds_count{action=\"delete\"} 7 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.005\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.01\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.025\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.05\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.25\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"0.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"1\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"2.5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"5\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"10\"} 1 engine_daemon_container_actions_seconds_bucket{action=\"start\",le=\"+Inf\"} 1 engine_daemon_container_actions_seconds_sum{action=\"start\"} 0 engine_daemon_container_actions_seconds_count{action=\"start\"} 1 # HELP engine_daemon_container_states_containers The count of containers in various states # TYPE engine_daemon_container_states_containers gauge engine_daemon_container_states_containers{state=\"paused\"} 0 engine_daemon_container_states_containers{state=\"running\"} 143 engine_daemon_container_states_containers{state=\"stopped\"} 19 # HELP engine_daemon_engine_cpus_cpus The number of cpus that the host system of the engine has # TYPE engine_daemon_engine_cpus_cpus gauge engine_daemon_engine_cpus_cpus 8 # HELP engine_daemon_engine_info The information related to the engine and the OS it is running on # TYPE engine_daemon_engine_info gauge engine_daemon_engine_info{architecture=\"x86_64\",commit=\"9d988398e7\",daemon_id=\"YCLF:XBU5:6B75:3CYI:YM2J:4ES4:JH7P:XAUT:Q2NK:SR6B:3LQP:XOVR\",graphdriver=\"overlay2\",kernel=\"3.10.0-1062.1.2.el7.x86_64\",os=\"CentOS Linux 7 (Core)\",os_type=\"linux\",version=\"19.03.9\"} 1 # HELP engine_daemon_engine_memory_bytes The number of bytes of memory that the host system of the engine has # TYPE engine_daemon_engine_memory_bytes gauge engine_daemon_engine_memory_bytes 1.655640064e+10 # HELP engine_daemon_events_subscribers_total The number of current subscribers to events # TYPE engine_daemon_events_subscribers_total gauge engine_daemon_events_subscribers_total 0 # HELP engine_daemon_events_total The number of events logged # TYPE engine_daemon_events_total counter engine_daemon_events_total 3166 # HELP engine_daemon_health_checks_failed_total The total number of failed health checks # TYPE engine_daemon_health_checks_failed_total counter engine_daemon_health_checks_failed_total 0 # HELP engine_daemon_health_checks_total The total number of health checks # TYPE engine_daemon_health_checks_total counter engine_daemon_health_checks_total 0 # HELP engine_daemon_network_actions_seconds The number of seconds it takes to process each network action # TYPE engine_daemon_network_actions_seconds histogram engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.005\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.01\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.025\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.05\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.1\"} 0 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.25\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"0.5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"1\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"2.5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"5\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"10\"} 1 engine_daemon_network_actions_seconds_bucket{action=\"release\",le=\"+Inf\"} 1 engine_daemon_network_actions_seconds_sum{action=\"release\"} 0.109992905 engine_daemon_network_actions_seconds_count{action=\"release\"} 1 # HELP etcd_debugging_snap_save_marshalling_duration_seconds The marshalling cost distributions of save called by snapshot. # TYPE etcd_debugging_snap_save_marshalling_duration_seconds histogram etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.001\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.002\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.004\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.008\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.016\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.032\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.064\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.128\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.256\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"0.512\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"1.024\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"2.048\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"4.096\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"8.192\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_debugging_snap_save_marshalling_duration_seconds_sum 0 etcd_debugging_snap_save_marshalling_duration_seconds_count 0 # HELP etcd_debugging_snap_save_total_duration_seconds The total latency distributions of save called by snapshot. # TYPE etcd_debugging_snap_save_total_duration_seconds histogram etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.001\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.002\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.004\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.008\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.016\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.032\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.064\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.128\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.256\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"0.512\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"1.024\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"2.048\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"4.096\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"8.192\"} 0 etcd_debugging_snap_save_total_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_debugging_snap_save_total_duration_seconds_sum 0 etcd_debugging_snap_save_total_duration_seconds_count 0 # HELP etcd_disk_wal_fsync_duration_seconds The latency distributions of fsync called by wal. # TYPE etcd_disk_wal_fsync_duration_seconds histogram etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.001\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.002\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.004\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.008\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.016\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.032\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.064\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.128\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.256\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"0.512\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"1.024\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"2.048\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"4.096\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"8.192\"} 0 etcd_disk_wal_fsync_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_disk_wal_fsync_duration_seconds_sum 0 etcd_disk_wal_fsync_duration_seconds_count 0 # HELP etcd_snap_db_fsync_duration_seconds The latency distributions of fsyncing .snap.db file # TYPE etcd_snap_db_fsync_duration_seconds histogram etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.001\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.002\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.004\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.008\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.016\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.032\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.064\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.128\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.256\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"0.512\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"1.024\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"2.048\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"4.096\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"8.192\"} 0 etcd_snap_db_fsync_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_snap_db_fsync_duration_seconds_sum 0 etcd_snap_db_fsync_duration_seconds_count 0 # HELP etcd_snap_db_save_total_duration_seconds The total latency distributions of v3 snapshot save # TYPE etcd_snap_db_save_total_duration_seconds histogram etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.1\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.4\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"0.8\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"1.6\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"3.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"6.4\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"12.8\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"25.6\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"51.2\"} 0 etcd_snap_db_save_total_duration_seconds_bucket{le=\"+Inf\"} 0 etcd_snap_db_save_total_duration_seconds_sum 0 etcd_snap_db_save_total_duration_seconds_count 0 # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 1.0529e-05 go_gc_duration_seconds{quantile=\"0.25\"} 1.4528e-05 go_gc_duration_seconds{quantile=\"0.5\"} 1.7833e-05 go_gc_duration_seconds{quantile=\"0.75\"} 2.4036e-05 go_gc_duration_seconds{quantile=\"1\"} 0.004878993 go_gc_duration_seconds_sum 0.023893318 go_gc_duration_seconds_count 397 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 647 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge go_memstats_alloc_bytes 6.322868e+07 # HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed. # TYPE go_memstats_alloc_bytes_total counter go_memstats_alloc_bytes_total 1.2068908064e+10 # HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table. # TYPE go_memstats_buck_hash_sys_bytes gauge go_memstats_buck_hash_sys_bytes 1.984451e+06 # HELP go_memstats_frees_total Total number of frees. # TYPE go_memstats_frees_total counter go_memstats_frees_total 9.9325482e+07 # HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata. # TYPE go_memstats_gc_sys_bytes gauge go_memstats_gc_sys_bytes 4.907008e+06 # HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use. # TYPE go_memstats_heap_alloc_bytes gauge go_memstats_heap_alloc_bytes 6.322868e+07 # HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used. # TYPE go_memstats_heap_idle_bytes gauge go_memstats_heap_idle_bytes 5.7245696e+07 # HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use. # TYPE go_memstats_heap_inuse_bytes gauge go_memstats_heap_inuse_bytes 6.8386816e+07 # HELP go_memstats_heap_objects Number of allocated objects. # TYPE go_memstats_heap_objects gauge go_memstats_heap_objects 433408 # HELP go_memstats_heap_released_bytes_total Total number of heap bytes released to OS. # TYPE go_memstats_heap_released_bytes_total counter go_memstats_heap_released_bytes_total 5.0003968e+07 # HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system. # TYPE go_memstats_heap_sys_bytes gauge go_memstats_heap_sys_bytes 1.25632512e+08 # HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection. # TYPE go_memstats_last_gc_time_seconds gauge go_memstats_last_gc_time_seconds 1.6289054392397587e+09 # HELP go_memstats_lookups_total Total number of pointer lookups. # TYPE go_memstats_lookups_total counter go_memstats_lookups_total 0 # HELP go_memstats_mallocs_total Total number of mallocs. # TYPE go_memstats_mallocs_total counter go_memstats_mallocs_total 9.975889e+07 # HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures. # TYPE go_memstats_mcache_inuse_bytes gauge go_memstats_mcache_inuse_bytes 13888 # HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system. # TYPE go_memstats_mcache_sys_bytes gauge go_memstats_mcache_sys_bytes 16384 # HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures. # TYPE go_memstats_mspan_inuse_bytes gauge go_memstats_mspan_inuse_bytes 814776 # HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system. # TYPE go_memstats_mspan_sys_bytes gauge go_memstats_mspan_sys_bytes 884736 # HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place. # TYPE go_memstats_next_gc_bytes gauge go_memstats_next_gc_bytes 7.104408e+07 # HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations. # TYPE go_memstats_other_sys_bytes gauge go_memstats_other_sys_bytes 1.384757e+06 # HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator. # TYPE go_memstats_stack_inuse_bytes gauge go_memstats_stack_inuse_bytes 8.585216e+06 # HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator. # TYPE go_memstats_stack_sys_bytes gauge go_memstats_stack_sys_bytes 8.585216e+06 # HELP go_memstats_sys_bytes Number of bytes obtained by system. Sum of all system allocations. # TYPE go_memstats_sys_bytes gauge go_memstats_sys_bytes 1.43395064e+08 # HELP http_request_duration_microseconds The HTTP request latencies in microseconds. # TYPE http_request_duration_microseconds summary http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.5\"} 2470.569 http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.9\"} 2847.254 http_request_duration_microseconds{handler=\"prometheus\",quantile=\"0.99\"} 8325.889 http_request_duration_microseconds_sum{handler=\"prometheus\"} 241802.613 http_request_duration_microseconds_count{handler=\"prometheus\"} 92 # HELP http_request_size_bytes The HTTP request sizes in bytes. # TYPE http_request_size_bytes summary http_request_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} 238 http_request_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} 238 http_request_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} 439 http_request_size_bytes_sum{handler=\"prometheus\"} 23035 http_request_size_bytes_count{handler=\"prometheus\"} 92 # HELP http_requests_total Total number of HTTP requests made. # TYPE http_requests_total counter http_requests_total{code=\"200\",handler=\"prometheus\",method=\"get\"} 92 # HELP http_response_size_bytes The HTTP response sizes in bytes. # TYPE http_response_size_bytes summary http_response_size_bytes{handler=\"prometheus\",quantile=\"0.5\"} 4159 http_response_size_bytes{handler=\"prometheus\",quantile=\"0.9\"} 4167 http_response_size_bytes{handler=\"prometheus\",quantile=\"0.99\"} 4174 http_response_size_bytes_sum{handler=\"prometheus\"} 381997 http_response_size_bytes_count{handler=\"prometheus\"} 92 # HELP logger_log_entries_size_greater_than_buffer_total Number of log entries which are larger than the log buffer # TYPE logger_log_entries_size_greater_than_buffer_total counter logger_log_entries_size_greater_than_buffer_total 3901 # HELP logger_log_read_operations_failed_total Number of log reads from container stdio that failed # TYPE logger_log_read_operations_failed_total counter logger_log_read_operations_failed_total 0 # HELP logger_log_write_operations_failed_total Number of log write operations that failed # TYPE logger_log_write_operations_failed_total counter logger_log_write_operations_failed_total 0 # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds. # TYPE process_cpu_seconds_total counter process_cpu_seconds_total 105.94 # HELP process_max_fds Maximum number of open file descriptors. # TYPE process_max_fds gauge process_max_fds 1.048576e+06 # HELP process_open_fds Number of open file descriptors. # TYPE process_open_fds gauge process_open_fds 808 # HELP process_resident_memory_bytes Resident memory size in bytes. # TYPE process_resident_memory_bytes gauge process_resident_memory_bytes 8.4279296e+07 # HELP process_start_time_seconds Start time of the process since unix epoch in seconds. # TYPE process_start_time_seconds gauge process_start_time_seconds 1.62890400767e+09 # HELP process_virtual_memory_bytes Virtual memory size in bytes. # TYPE process_virtual_memory_bytes gauge process_virtual_memory_bytes 5.925912576e+09 # HELP swarm_dispatcher_scheduling_delay_seconds Scheduling delay is the time a task takes to go from NEW to RUNNING state. # TYPE swarm_dispatcher_scheduling_delay_seconds histogram swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.005\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.01\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.025\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.05\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.1\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.25\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"0.5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"1\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"2.5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"5\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"10\"} 0 swarm_dispatcher_scheduling_delay_seconds_bucket{le=\"+Inf\"} 0 swarm_dispatcher_scheduling_delay_seconds_sum 0 swarm_dispatcher_scheduling_delay_seconds_count 0 # HELP swarm_manager_configs_total The number of configs in the cluster object store # TYPE swarm_manager_configs_total gauge swarm_manager_configs_total 0 # HELP swarm_manager_leader Indicates if this manager node is a leader # TYPE swarm_manager_leader gauge swarm_manager_leader 0 # HELP swarm_manager_networks_total The number of networks in the cluster object store # TYPE swarm_manager_networks_total gauge swarm_manager_networks_total 0 # HELP swarm_manager_nodes The number of nodes # TYPE swarm_manager_nodes gauge swarm_manager_nodes{state=\"disconnected\"} 0 swarm_manager_nodes{state=\"down\"} 0 swarm_manager_nodes{state=\"ready\"} 0 swarm_manager_nodes{state=\"unknown\"} 0 # HELP swarm_manager_secrets_total The number of secrets in the cluster object store # TYPE swarm_manager_secrets_total gauge swarm_manager_secrets_total 0 # HELP swarm_manager_services_total The number of services in the cluster object store # TYPE swarm_manager_services_total gauge swarm_manager_services_total 0 # HELP swarm_manager_tasks_total The number of tasks in the cluster object store # TYPE swarm_manager_tasks_total gauge swarm_manager_tasks_total{state=\"accepted\"} 0 swarm_manager_tasks_total{state=\"assigned\"} 0 swarm_manager_tasks_total{state=\"complete\"} 0 swarm_manager_tasks_total{state=\"failed\"} 0 swarm_manager_tasks_total{state=\"new\"} 0 swarm_manager_tasks_total{state=\"orphaned\"} 0 swarm_manager_tasks_total{state=\"pending\"} 0 swarm_manager_tasks_total{state=\"preparing\"} 0 swarm_manager_tasks_total{state=\"ready\"} 0 swarm_manager_tasks_total{state=\"rejected\"} 0 swarm_manager_tasks_total{state=\"remove\"} 0 swarm_manager_tasks_total{state=\"running\"} 0 swarm_manager_tasks_total{state=\"shutdown\"} 0 swarm_manager_tasks_total{state=\"starting\"} 0 # HELP swarm_node_manager Whether this node is a manager or not # TYPE swarm_node_manager gauge swarm_node_manager 0 # HELP swarm_raft_snapshot_latency_seconds Raft snapshot create latency. # TYPE swarm_raft_snapshot_latency_seconds histogram swarm_raft_snapshot_latency_seconds_bucket{le=\"0.005\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.01\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.025\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.05\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.1\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.25\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"0.5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"1\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"2.5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"5\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"10\"} 0 swarm_raft_snapshot_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_raft_snapshot_latency_seconds_sum 0 swarm_raft_snapshot_latency_seconds_count 0 # HELP swarm_raft_transaction_latency_seconds Raft transaction latency. # TYPE swarm_raft_transaction_latency_seconds histogram swarm_raft_transaction_latency_seconds_bucket{le=\"0.005\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.01\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.025\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.05\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.1\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.25\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"0.5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"1\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"2.5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"5\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"10\"} 0 swarm_raft_transaction_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_raft_transaction_latency_seconds_sum 0 swarm_raft_transaction_latency_seconds_count 0 # HELP swarm_store_batch_latency_seconds Raft store batch latency. # TYPE swarm_store_batch_latency_seconds histogram swarm_store_batch_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"1\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"5\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"10\"} 0 swarm_store_batch_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_batch_latency_seconds_sum 0 swarm_store_batch_latency_seconds_count 0 # HELP swarm_store_lookup_latency_seconds Raft store read latency. # TYPE swarm_store_lookup_latency_seconds histogram swarm_store_lookup_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"1\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"5\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"10\"} 0 swarm_store_lookup_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_lookup_latency_seconds_sum 0 swarm_store_lookup_latency_seconds_count 0 # HELP swarm_store_memory_store_lock_duration_seconds Duration for which the raft memory store lock was held. # TYPE swarm_store_memory_store_lock_duration_seconds histogram swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.005\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.01\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.025\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.05\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.1\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.25\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"0.5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"1\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"2.5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"5\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"10\"} 0 swarm_store_memory_store_lock_duration_seconds_bucket{le=\"+Inf\"} 0 swarm_store_memory_store_lock_duration_seconds_sum 0 swarm_store_memory_store_lock_duration_seconds_count 0 # HELP swarm_store_read_tx_latency_seconds Raft store read tx latency. # TYPE swarm_store_read_tx_latency_seconds histogram swarm_store_read_tx_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"1\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"5\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"10\"} 0 swarm_store_read_tx_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_read_tx_latency_seconds_sum 0 swarm_store_read_tx_latency_seconds_count 0 # HELP swarm_store_write_tx_latency_seconds Raft store write tx latency. # TYPE swarm_store_write_tx_latency_seconds histogram swarm_store_write_tx_latency_seconds_bucket{le=\"0.005\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.01\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.025\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.05\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.1\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.25\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"0.5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"1\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"2.5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"5\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"10\"} 0 swarm_store_write_tx_latency_seconds_bucket{le=\"+Inf\"} 0 swarm_store_write_tx_latency_seconds_sum 0 swarm_store_write_tx_latency_seconds_count 0 © vishon all right reserved，powered by GitbookUpdated at 2021-08-14 09:45:21 "},"prometheus/Blackbox_exporter-monitoring-url.html":{"url":"prometheus/Blackbox_exporter-monitoring-url.html","title":"【prometheus】使用Blackbox_exporter服务探测","keywords":"","body":"Blackbox_exporter监控url 有的时候，我们需要对域名或者一些接口url进行检测，看下是否可以，我们可以部署Blackbox_exporter来采集这些监控数据到prometheus Blackbox_exporter是Prometheus官方提供的exporter之一，可以提供 http、dns、tcp、icmp的监控数据采集，具体部署可以参考文档https://cloud.tencent.com/developer/article/1808998 blackbox模块配置 下面我们说说怎么配置各种协议的探测，首先需要在blackbox的配置文件设置模块 modules: http_2xx: prober: http timeout: 15s http: prober: http timeout: 2s http: valid_http_versions: [\"HTTP/1.1\", \"HTTP/2\"] valid_status_codes: [200,301,302] method: GET preferred_ip_protocol: \"ip4\" tcp_connect: prober: tcp ssh: prober: tcp tcp: query_response: - expect: \"^SSH-2.0-\" icmp: prober: icmp timeout: 5s icmp: 上面我们定义了4个模块，下面我们只需要在job中配置下模块的检查内容 http测试job配置 scrape_configs: - job_name: http-blackbox honor_timestamps: true params: module: - http_2xx metrics_path: /probe scheme: http static_configs: - targets: - https://www.niewx.cn/ labels: domain: www.niewx.cn instance: https://www.niewx.cn/ ip: githubpage port: \"443\" project: blog service: none - targets: - https://www.niewx.cn/mybook/ labels: domain: www.niewx.cn instance: https://www.niewx.cn/mybook/ ip: githubpage port: \"443\" project: mybook service: none relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace tcp测试job配置 scrape_configs: - job_name: tcp-blackbox honor_timestamps: true params: module: - tcp_connect metrics_path: /probe static_configs: - targets: - xxx.xxx.xxx.xxx:80 labels: instance: xxx.xxx.xxx.xxx:80 ip: xxx.xxx.xxx.xxx port: \"80\" - targets: - xxx.xxx.xxx.xxx:8080 labels: instance: xxx.xxx.xxx.xxx:8080 ip: xxx.xxx.xxx.xxx port: \"8080\" relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace icmp测试job配置 scrape_configs: - job_name: icmp-blackbox honor_timestamps: true params: module: - icmp metrics_path: /probe static_configs: - targets: - xx.xx.xx.xx labels: instance: xx.xx.xx.xx ip: xx.xx.xx.xx - targets: - xxx.xxx.xxx.xxx labels: instance: xxx.xxx.xxx.xxx ip: xxx.xxx.xxx.xxx relabel_configs: - source_labels: [__address__] separator: ; regex: (.*) target_label: __param_target replacement: $1 action: replace - separator: ; regex: (.*) target_label: __address__ replacement: blackbox-exporter.monitor.svc.cluster.local:9115 action: replace 配置好之后，数据就可以采集到prometheus了，然后配置下grafana面板查看我们的监控信息即可。 问题 问题：当http探测域名的是时候发现面板的连通性是离线，但是http的状态却是200 解决方案：可以将http模块中配置的协议HTTP/2改成HTTP/2.0，然后再重启blackbox即可 © vishon all right reserved，powered by GitbookUpdated at 2021-09-01 13:46:48 "},"prometheus/prometheus-monitors-k8s-cluster-coredns.html":{"url":"prometheus/prometheus-monitors-k8s-cluster-coredns.html","title":"【prometheus】prometheus监控k8s集群coredns","keywords":"","body":"prometheus监控k8s集群coredns k8s中的域名解析的主流方案是coredns，kubeadm默认是会在集群部署coredns，一般k8s都会部署coredns，并且coredns有默认提供监控metrics的接口，下面我们来来说说如何用prometheus来采集coredns的监控数据。这里我们prometheus的用的是operater创建的，通过serviceMontor来监控coredns的service。 创建监控接口的servcie 正常我们在集群默认部署的coredns，会自动创建一个kube-dns的service，业务pod内就是通过访问这个service来用coredns进行域名的解析。coredns默认用9153端口提供了metrics接口，因此我们还需要新建一个service，用来暴露coredns的9153端口。 apiVersion: v1 kind: Service metadata: name: coredns-metrics namespace: kube-system labels: app: cordns spec: ports: - name: 9153-9153-tcp port: 9153 protocol: TCP targetPort: 9153 selector: k8s-app: kube-dns sessionAffinity: None type: ClusterIP 配置serviceMontor采集监控数据 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: coredns-servicemonitors namespace: kube-system spec: endpoints: - bearerTokenSecret: key: \"\" interval: 15s port: 9153-9153-tcp namespaceSelector: matchNames: - kube-system selector: matchLabels: app: cordns 除了用serviceMontor配置，也可以用RawJobs配置 scrape_configs: - job_name: coredns-metrics honor_labels: true honor_timestamps: true scrape_interval: 15s metrics_path: /metrics scheme: http kubernetes_sd_configs: - role: endpoints namespaces: names: - kube-system relabel_configs: - source_labels: [__config_type] separator: ; regex: service target_label: __config_type replacement: $1 action: replace - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: 9153-9153-tcp replacement: $1 action: keep - source_labels: [__meta_kubernetes_service_name] separator: ; regex: coredns-metrics replacement: $1 action: keep - source_labels: [__meta_kubernetes_pod_node_name] separator: ; regex: (.*) target_label: node replacement: $1 action: replace - source_labels: [__meta_kubernetes_namespace] separator: ; regex: (.*) target_label: namespace replacement: $1 action: replace - source_labels: [__meta_kubernetes_service_name] separator: ; regex: (.*) target_label: service replacement: $1 action: replace - source_labels: [__meta_kubernetes_pod_name] separator: ; regex: (.*) target_label: pod replacement: $1 action: replace - source_labels: [__meta_kubernetes_endpoint_port_name] separator: ; regex: (.*) target_label: endpoint replacement: $1 action: replace grafana配置coredns监控面板 可以到grafana的官网文档中找相关的dashboard，可以用5926这个面板，导入面板后，就可以查看coredns的监控了。 © vishon all right reserved，powered by GitbookUpdated at 2021-10-30 12:29:33 "},"istio/istio-annotation-list.html":{"url":"istio/istio-annotation-list.html","title":"【istio】istio注解列表","keywords":"","body":"istio注解列表 注解加在workload的spec.template.metadata.annotations字段下 apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: sidecar.istio.io/status: xxxxx Istio支持控制其行为的各种资源注释列表如下 Annotation Name Resource Types Description kubernetes.io/ingress.class [Ingress] Ingress 资源上的注释，表示负责它的控制器类。 networking.istio.io/exportTo [Service] 指定此服务应导出到的命名空间。 '*' 值表示它可以在网格 '.' 内到达。表示它在其命名空间内是可访问的。 policy.istio.io/check [pod] 确定无法连接到 Mixer 时的行为策略。如果未设置，则设置 FAIL_CLOSE，拒绝请求。 policy.istio.io/checkBaseRetryWaitTime [pod] 重试之间等待的基本时间，将通过退避和抖动进行调整。以持续时间格式。如果未设置，则为 80 毫秒。 policy.istio.io/checkMaxRetryWaitTime [pod] 重试 Mixer 之间等待的最长时间。以持续时间格式。如果未设置，则为 1000 毫秒。 policy.istio.io/checkRetries [pod] 传输错误到 Mixer 的最大重试次数。如果未设置，则为 0，表示不重试。 policy.istio.io/lang [pod] 为 Mixer 选择属性表达式语言运行时。 readiness.status.sidecar.istio.io/applicationPorts [pod] 指定应用容器暴露的端口列表。由 Envoy sidecar 就绪探测器使用，以确定 Envoy 已配置并准备好接收流量。 readiness.status.sidecar.istio.io/failureThreshold [pod] 指定 Envoy sidecar 就绪探测的失败阈值。 readiness.status.sidecar.istio.io/initialDelaySeconds [pod] 指定 Envoy sidecar 就绪探测的初始延迟（以秒为单位）。 readiness.status.sidecar.istio.io/periodSeconds [pod] 指定 Envoy sidecar 就绪探测的周期（以秒为单位）。 sidecar.istio.io/bootstrapOverride [pod] 指定一个替代的 Envoy 引导程序配置文件。 sidecar.istio.io/componentLogLevel [pod] 指定 Envoy 的组件日志级别。 sidecar.istio.io/controlPlaneAuthPolicy [pod] 指定 Istio 控制平面使用的身份验证策略。如果为 NONE，则不会加密流量。如果是 MUTUAL_TLS，Envoy sidecar 之间的流量将被包装到相互的 TLS 连接中。 sidecar.istio.io/discoveryAddress [pod] 指定 Envoy sidecar 使用的 XDS 发现地址。 sidecar.istio.io/inject [pod] 指定 Envoy sidecar 是否应该自动注入到工作负载中。 sidecar.istio.io/interceptionMode [pod] 指定用于将入站连接重定向到 Envoy（REDIRECT 或 TPROXY）的模式。 sidecar.istio.io/logLevel [pod] 指定 Envoy 的日志级别。 sidecar.istio.io/proxyCPU [pod] 为 Envoy sidecar 指定请求的 CPU 设置。 sidecar.istio.io/proxyImage [pod] 指定 Envoy sidecar 使用的 Docker 镜像。 sidecar.istio.io/proxyMemory [pod] 指定 Envoy sidecar 请求的内存设置。 sidecar.istio.io/rewriteAppHTTPProbers [pod] 重写 HTTP 准备和活动探测器以重定向到 Envoy sidecar。 sidecar.istio.io/statsInclusionPrefixes [pod] 指定由 Envoy 发出的统计信息前缀的逗号分隔列表。 sidecar.istio.io/statsInclusionRegexps [pod] 指定要由 Envoy 发出的统计信息应匹配的逗号分隔的正则表达式列表。 sidecar.istio.io/statsInclusionSuffixes [pod] 指定由 Envoy 发出的统计数据后缀的逗号分隔列表。 sidecar.istio.io/status [pod] 由 Envoy sidecar 注入生成，指示操作的状态。包括执行模板的版本哈希，以及注入资源的名称。 sidecar.istio.io/userVolume [pod] 指定要添加到 Envoy sidecar 的一个或多个用户卷（作为 JSON 数组）。 sidecar.istio.io/userVolumeMount [pod] 指定要添加到 Envoy sidecar 的一个或多个用户卷挂载（作为 JSON 数组）。 status.sidecar.istio.io/port [pod] 指定 Envoy sidecar 的 HTTP 状态端口。如果为零，sidecar 将不提供状态。 traffic.sidecar.istio.io/excludeInboundPorts [pod] 一个逗号分隔的入站端口列表，要从重定向到 Envoy 中排除。仅当所有入站流量（即“*”）被重定向时才适用。 traffic.sidecar.istio.io/excludeOutboundIPRanges [pod] 以逗号分隔的 CIDR 格式的 IP 范围列表，要从重定向中排除。仅当所有出站流量（即“*”）都被重定向时才适用。 traffic.sidecar.istio.io/excludeOutboundPorts [pod] 一个逗号分隔的出站端口列表，要从重定向到 Envoy 中排除。 traffic.sidecar.istio.io/includeInboundPorts [pod] 以逗号分隔的入站端口列表，其流量将被重定向到 Envoy。通配符“*”可用于为所有端口配置重定向。空列表将禁用所有入站重定向。 traffic.sidecar.istio.io/includeOutboundIPRanges [pod] 以逗号分隔的 CIDR 形式的 IP 范围列表，用于重定向到 Envoy（可选）。通配符“*”可用于重定向所有出站流量。空列表将禁用所有出站重定向。 traffic.sidecar.istio.io/kubevirtInterfaces [pod] 以逗号分隔的虚拟接口列表，其入站流量（来自 VM）将被视为出站流量。 © vishon all right reserved，powered by GitbookUpdated at 2021-10-22 00:02:44 "},"istio/Sidecar-startup-sequence-problem-in-istio.html":{"url":"istio/Sidecar-startup-sequence-problem-in-istio.html","title":"【istio】istio中sidecar启动顺序问题","keywords":"","body":"问题 当我们的一些服务往istio上迁移的时候，会出现一个问题，就是某些依赖数据库的服务会一直起不了，pod启动失败，这里排查原因是envoy容器还没起来，服务容器就起来了，导致业务流量无法被转发出去，从而连接数据库异常。 解决方案 这里解决问题的方案就是保证envoy这个sidecar容器先于业务容器启动，那么怎么保证sidecar容器先于业务容器启动呢？ istio1.7之后版本解决方案 istio1.7通过给istio-injector注入逻辑增加一个叫HoldApplicationUntilProxyStarts的开关来解决了该问题 添加了 values.global.proxy.holdApplicationUntilProxyStarts config选项，它使sidecar注入器在pod容器列表的开始处注入sidecar，并将其配置为阻止所有其他容器的开始，直到代理就绪为止。默认情况下禁用此选项。(#11130) 也就是说只要开启个这个特性，那么就可以保证pod里的sidecar容器内先于业务容器启动，这样就不会出现pod内容器启动顺序的问题 这里我们可以全局开启这个特性和单独给某个deployment开启这个特性 全局开启HoldApplicationUntilProxyStarts 全局开启只需要修改istiod的全局配置即可 kubectl edit cm -n istio-system istio-1-8-1 在defaultConfig字段下加上holdApplicationUntilProxyStarts: true apiVersion: v1 data: mesh: | accessLogEncoding: JSON accessLogFile: /dev/stdout accessLogFormat: \"\" defaultConfig: holdApplicationUntilProxyStarts: true discoveryAddress: istiod-1-8-1.istio-system.svc:15012 局部开启HoldApplicationUntilProxyStarts 如果单独给某个deployment开启这个特性，需要在pod的注解加上proxy.istio.io/config，将 holdApplicationUntilProxyStarts 置为 true apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: annotations: proxy.istio.io/config: | holdApplicationUntilProxyStarts: true labels: app: nginx spec: containers: - name: nginx image: \"nginx\" istio1.7版本之前的解决方案 如果是istio1.7之前的版本，是没有这个特性的，那么需要采用另外一种方案来解决这个问题，我们在业务容器启动前判断下envoy服务是否已经启动成功了 command: [\"/bin/bash\", \"-c\"] args: [\"while [[ \\\"$(curl -s -o /dev/null -w ''%{http_code}'' localhost:15020/healthz/ready)\\\" != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; start-app-cmd\"] 这里直接在deployment加上启动命令，如果探测envoy服务的15020端口返回200，则启动业务进程，这样可以保证sidecar先比业务容器启动 © vishon all right reserved，powered by GitbookUpdated at 2021-06-01 13:01:03 "},"istio/istio-traffic-shift.html":{"url":"istio/istio-traffic-shift.html","title":"【istio】istio流量转移实践","keywords":"","body":"有时候我们需要将服务的部分流量转移到另外一个服务，灰度测试的时候，为了测试新版本是否有问题，我们需要将部分流量打到新版本上，本次任务，我们实现将请求流量20%打到v1版本，80%的流量打到v2版本。 部署nginx的v1和v2版本 apiVersion: v1 data: index.html: I am v1 version kind: ConfigMap metadata: name: index-v1 namespace: mesh --- apiVersion: v1 data: index.html: I am v2 version kind: ConfigMap metadata: name: index-v2 namespace: mesh --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx version: v1 name: nginx-v1 namespace: mesh spec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: containers: - image: nginx imagePullPolicy: Always name: nginx-v1 resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: vol subPath: index.html dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - configMap: defaultMode: 420 name: index-v1 name: vol --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx version: v2 name: nginx-v2 namespace: mesh spec: replicas: 1 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: containers: - image: nginx imagePullPolicy: Always name: nginx-v2 resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi volumeMounts: - mountPath: /usr/share/nginx/html/index.html name: vol subPath: index.html dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always volumes: - configMap: defaultMode: 420 name: index-v2 name: vol --- apiVersion: v1 kind: Service metadata: name: nginx-v1 labels: app: nginx service: nginx spec: ports: - port: 80 name: http selector: app: nginx 这里部署了一个统一的入口service关联到后端的v1和v2版本pod里面 创建DestinationRule apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: nginx namespace: mesh spec: host: nginx subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 创建gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: mesh spec: servers: - port: number: 88 name: HTTP-88-ebsi protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway istio: ingressgateway 挂载ingressgateway作为统一的访问入口 创建VirtualService apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx namespace: mesh spec: hosts: - '*' gateways: - mesh/nginx-gw http: - route: - destination: host: nginx subset: v1 weight: 20 - destination: host: nginx subset: v2 weight: 80 VirtualService里面的weight字段代表的是流量转发的比例，这里将流量20%转发到v1，流量80%转到v2 访问测试 我们直接用ingress的LoadBalancer的vip和88端口访问10次看下 [root@VM-0-13-centos ~]# for i in {1..10};do curl 106.xx.xx.93:88 ; echo \"\" ;done I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v2 version I am v1 version I am v1 version 从上面的测试结果看，\"I am v1 version\"出现了2次，\"I am v2 version\"出现了8次，说明请求的流量是符合VirtualService的分配比例的 © vishon all right reserved，powered by GitbookUpdated at 2021-06-04 10:59:22 "},"istio/istio-multi-cluster-nearby-access.html":{"url":"istio/istio-multi-cluster-nearby-access.html","title":"【istio】istio多集群就近接入","keywords":"","body":"随着业务的扩展，有时候希望客户能就近访问我们的网站，减少网络延迟，但是很多时候会在多地域部署相同的服务，其实有了istio后，我们可以利用就近接入来解决这个问题，这样无需在另一地域集群部署整套业务，只需在网格管理的另一个集群中部署边缘代理网关并配置好监听规则，即可以另一集群为入口访问电商网站业务 腾讯云上的服务网格如果不同地域通过云联网打通了可以通过同一个网格管理，下面我们通过同一个vpc下的2个集群模拟多个地域来配置下就近访问 首先我们在网格主集群A中部署一个nginx服务，通过gateway提供访问 apiVersion: apps/v1 kind: Deployment metadata: generation: 5 labels: k8s-app: nginx qcloud-app: nginx name: nginx namespace: mesh spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: nginx qcloud-app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: nginx qcloud-app: nginx spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: limits: cpu: 500m memory: 1Gi requests: cpu: 250m memory: 256Mi securityContext: privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: qcloudregistrykey restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: nginx-test namespace: mesh spec: ports: - name: 80-80-tcp port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx qcloud-app: nginx sessionAffinity: None type: ClusterIP 主集群A部署gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: nginx-gw namespace: mesh spec: servers: - port: number: 88 name: HTTP-88-ebsi protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway istio: ingressgateway 我们将子集群B加入网格内，也会部署一个istio-ingressgateway作为访问的入口，然后在B集群部署一个gateway来访问我们A集群部署的服务 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: child-gw namespace: mesh spec: servers: - port: number: 80 name: HTTP-80-6pnq protocol: HTTP hosts: - '*' selector: app: istio-ingressgateway-2 istio: ingressgateway 然后我们创建一个VirtualService来关联这2个gateway apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nearest-access-vs namespace: mesh spec: hosts: - '*' gateways: - mesh/child-gw - mesh/nginx-gw http: - route: - destination: host: nginx-test.mesh.svc.cluster.local port: number: 80 现在我们在B集群没有部署nginx服务，然后我们用B集群的gateway来访问我们服务看是否能访问到 [root@VM-17-4-centos ~]# kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway-2 LoadBalancer 172.22.253.209 10.0.0.31 80:31199/TCP,15021:30299/TCP 4h5m istiod-1-8-1 LoadBalancer 172.22.255.147 10.0.0.141 15012:32190/TCP,443:31540/TCP 4h28m istiod-1-8-1-injector ClusterIP None 443/TCP 4h28m kube-mesh LoadBalancer 172.22.254.32 10.0.0.144 443:30431/TCP 4h28m zipkin ClusterIP 172.22.255.45 9411/TCP 4h27m [root@VM-17-4-centos ~]# kubectl get pod -n mesh No resources found in mesh namespace. [root@VM-17-4-centos ~]# kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway-2 LoadBalancer 172.22.253.209 10.0.0.31 80:31199/TCP,15021:30299/TCP 4h5m istiod-1-8-1 LoadBalancer 172.22.255.147 10.0.0.141 15012:32190/TCP,443:31540/TCP 4h28m istiod-1-8-1-injector ClusterIP None 443/TCP 4h28m kube-mesh LoadBalancer 172.22.254.32 10.0.0.144 443:30431/TCP 4h28m zipkin ClusterIP 172.22.255.45 9411/TCP 4h27m [root@VM-17-4-centos ~]# kubectl get pod -n mesh No resources found in mesh namespace. [root@VM-17-4-centos ~]# curl 10.0.0.31:80 Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 从上面测试结果看，我们的ingressgateway的service是一个内网lb，并且我们在mesh命名空间下没有部署服务，我们通过lb的vip和80端口是可以访问到nginx服务的，这里说明我们访问B集群的流量被路由到了主集群A中。 © vishon all right reserved，powered by GitbookUpdated at 2021-06-04 19:06:27 "},"istio/VirtualService-host-configuration-and-use.html":{"url":"istio/VirtualService-host-configuration-and-use.html","title":"【istio】VirtualService中hosts字段的配置使用","keywords":"","body":"VirtualService中hosts字段的配置使用 VirtualService配置示例 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx namespace: mesh spec: hosts: - 172.16.85.174 - nginx-test.mesh.svc.cluster.local - nginx-test gateways: - mesh/vpc-gw http: - route: - destination: host: nginx subset: v1 weight: 20 - destination: host: nginx subset: v2 weight: 80 使用hosts字段列举虚拟服务的主机——即用户指定的目标或是路由规则设定的目标。这是客户端向服务发送请求时使用的一个或多个地址。只有访问客户端的Host字段为hosts配置的地址才能路由到后端服务。 hosts: 172.16.85.174 nginx-test.mesh.svc.cluster.local nginx-test 虚拟服务主机名可以是IP 地址、DNS 名称，或者依赖于平台的一个简称（例如 Kubernetes 服务的短名称），隐式或显式地指向一个完全限定域名（FQDN）。您也可以使用通配符（“*”）前缀，让您创建一组匹配所有服务的路由规则。虚拟服务的 hosts 字段实际上不必是 Istio 服务注册的一部分，它只是虚拟的目标地址。这让您可以为没有路由到网格内部的虚拟主机建模。 上面的VirtualService配置了多个hosts，并且挂载了一个gateways，客户端直接访问后端的service是可以通的，但是我们通过域名访问后端服务时候就需要指定host了。 首先我们直接访问下gateway域名，是无法访问通的，因为VirtualService流量规则指定了hosts，我们的请求Host没在配置列表中。 bash-4.4# curl nginx.istio.niewx.top 如果你希望能通过域名直接访问，可以将域名配置到hosts下，默认发起请求的Host就是域名本身 spec: hosts: - 172.16.85.174 - nginx-test.mesh.svc.cluster.local - nginx-test - nginx.istio.niewx.top 直接访问域名 bash-4.4# curl -Iv nginx.istio.niewx.top * Rebuilt URL to: nginx.istio.niewx.top/ * Trying 10.0.0.183... * TCP_NODELAY set * Connected to nginx.istio.niewx.top (10.0.0.183) port 80 (#0) > HEAD / HTTP/1.1 > Host: nginx.istio.niewx.top > User-Agent: curl/7.61.1 > Accept: */* > 下面我们在请求中指定Host为172.16.85.174看下 bash-4.4# curl --silent -H \"Host: 172.16.85.174\" \"nginx.istio.niewx.top\" Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 指定Host为nginx-test.mesh.svc.cluster.local域名 bash-4.4# curl --silent -H \"Host: nginx-test.mesh.svc.cluster.local\" \"nginx.istio.niewx.top\" Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 当我们在请求中指定配置的hosts列表中Host时候是可以访问成功的。 © vishon all right reserved，powered by GitbookUpdated at 2021-09-13 18:39:30 "},"istio/Gateway-configuration-http-forced-to-https.html":{"url":"istio/Gateway-configuration-http-forced-to-https.html","title":"【istio】gateway配置http强转https","keywords":"","body":"gateway配置http强转https 使用istio的过程中，有时候不想让用户可以http访问，这时候就需要在gateway配置http强转为https访问，下面我们来说明下如何在gateway配置http强转https。 首先我们测试下正常配置http和https，看下是否分别通过http和https访问到后端的服务。 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: vpc-gw namespace: mesh spec: selector: app: istio-ingressgateway-vpc istio: ingressgateway servers: - hosts: - nginx.istio.niewx.top port: name: HTTP-80-iy2r number: 80 protocol: HTTP - hosts: - nginx.istio.niewx.top port: name: HTTPS-443-1krv number: 443 protocol: HTTPS tls: credentialName: vpc-gw-https-443-1krv mode: SIMPLE 分别通过http和https都可以成功的访问到后端。 [root@VM-0-13-centos ~]# curl -I http://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:41:56 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 26 [root@VM-0-13-centos ~]# curl -I https://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:42:16 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 1 gateway配置http强转https，只需要在gateway的http的配置中加上如下配置即可。 tls: httpsRedirect: true 下面我们在gateway中加上强制跳转的配置，再来通过http访问下。 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: vpc-gw namespace: mesh spec: selector: app: istio-ingressgateway-vpc istio: ingressgateway servers: - hosts: - nginx.istio.niewx.top port: name: HTTP-80-iy2r number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - nginx.istio.niewx.top port: name: HTTPS-443-1krv number: 443 protocol: HTTPS tls: credentialName: vpc-gw-https-443-1krv mode: SIMPLE 从下面的测试结果可以发现，访问http的时候会出现301，说明我们配置的永久重定向成功了。 [root@VM-0-13-centos ~]# curl -I http://nginx.istio.niewx.top HTTP/1.1 301 Moved Permanently location: https://nginx.istio.niewx.top/ date: Tue, 21 Sep 2021 15:45:12 GMT server: istio-envoy transfer-encoding: chunked [root@VM-0-13-centos ~]# curl -I https://nginx.istio.niewx.top HTTP/1.1 200 OK server: istio-envoy date: Tue, 21 Sep 2021 15:45:14 GMT content-type: text/html content-length: 15 last-modified: Sat, 18 Sep 2021 18:32:54 GMT etag: \"614630d6-f\" accept-ranges: bytes x-envoy-upstream-service-time: 1 © vishon all right reserved，powered by GitbookUpdated at 2021-09-21 23:47:31 "}}